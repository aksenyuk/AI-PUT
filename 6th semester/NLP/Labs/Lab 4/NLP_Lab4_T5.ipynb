{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention -- implementing attention between a given decoder state and the encoder states\n",
        "\n",
        "Below, there are two matrices, `encoder_states` and `decoder_states` representing the state of the hidden layer after processing each word by the encoder and the static embedding related to a given input of the decoder. A single hidden layer state contains an embedding of length = 3, which is equal to the size of the embedding in the decoder. In the encoder, we have 4 hidden layer states, because we are processing a sequence consisting of 4 tokens.\n",
        "\n",
        "There are 5 tokens in the decoder, which are generated based on the sequence processed by the encoder.\n",
        "\n",
        "The task is to: a) Calculate the similarity of all embeddings from the decoder (queries) to all embeddings of subsequent states of the encoder (keys) (remember that matrices can be transposed. In NumPy, we transpose a matrix using `matrix_name.T`)\n",
        "\n",
        "b) Softmax (imported from scipy) should be performed on the created similarity matrix. Note: remember to apply softmax in the right dimension. All hidden states of the encoder should be softmaxed in the context of a given decoder state. In scipy, the softmax function includes an `axis` argument which may help.\n",
        "\n",
        "c) Combine the attention matrix from step b) and `encoder_states` to generate a matrix containing context vectors for each token from the decoder."
      ],
      "metadata": {
        "id": "mooJ5vGuciH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# scipy.special.softmax(x, axis=None)\n",
        "\n",
        "encoder_states = np.array(\n",
        "    [[1.2, 3.4, 5.6],   # encoder's hidden layer output at the step 1, related to a given input token, e.g., I\n",
        "    [-2.3, 0.2, 7.2],   # encoder's hidden layer output at the step 2, related to a given token, e.g., like\n",
        "    [10.2, 0.2, 0.3],   # encoder's hidden layer output at the step 3, related to a given token, e.g., NLP\n",
        "    [0.4, 0.7, 1.2]]    # encoder's hidden layer output at the step 4, related to a given token, e.g., \".\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "decoder_states = np.array(\n",
        "    [[0.74, 0.23, 0.56],  # decoder's static word embedding at the step 1, related to a given token, e.g., <BOS>\n",
        "    [7.23, 0.12, 0.55],  # decoder's static word embedding at the step 2, related to a given token, e.g., Ja\n",
        "    [9.12, 4.23, 0.44], # decoder's static word embedding at the step 3, related to a given token, e.g., lubię\n",
        "    [4.1, 3.23, 0.5],    # decoder's static word embedding at the step 4, related to a given token, e.g., przetwarzanie\n",
        "    [5.2, 3.1, 8.5]]     # decoder's static word embedding at the step 5, related to a given token, e.g., języka\n",
        ")"
      ],
      "metadata": {
        "id": "Vbq0QW2td41r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected outputs:\n",
        "\n",
        "a) [[ 4.806 2.376 7.762 1.129] [ 12.164 -12.645 73.935 3.636] [ 27.79 -16.962 94.002 7.137] [ 18.702 -5.184 42.616 4.501] [ 64.38 49.86 56.21 14.45 ]]\n",
        "\n",
        "b) [[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03] [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31] [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38] [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17] [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
        "\n",
        "c) [[ 9.69108631 0.35799187 0.59163688] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [ 1.20254471 3.39909302 5.59850122]]\n"
      ],
      "metadata": {
        "id": "N8bfd5X4fAJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "## Using transformer-based T5 model to solve various NLP tasks in a sequence-to-sequence manner\n",
        "\n",
        "Today we're going to learn a new library -- the HuggingFace **transformers** library (https://huggingface.co/docs/transformers/index) and use it to solve several non-obvious NLP-related problems using the **T5** model\n",
        "\n",
        "\n",
        "HuggingFace transformers is one of the most popular libraries that provide us with a high-level API for using neural networks to solve tasks related to natural language processing, audio processing, computer vision, or even multimodal scenarios in which we have to utilize multiple modalities at once (e.g., answering questions about pictures, information extraction from invoices).\n",
        "\n",
        "First, let's install the dependencies, the `transformers` library itself and the `sentencepiece` module, which helps us tokenize documents and transform tokens into one-hot encodings (we will discuss the idea of sentencepiece later in detail).\n",
        "\n",
        "**Warning**: if you notice some weird exceptions like `cannot call from_pretrained on a None object` somewhere in your code, restart the environment using: Runtime -> restart. Then run the cells with code (without re-installing the libraries) one more time."
      ],
      "metadata": {
        "id": "5sPUDVw7fKHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers   # install HuggingFace transformers library\n",
        "!pip install sentencepiece  # install sentencepiece"
      ],
      "metadata": {
        "id": "jmRzPimhfRja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The API provided by the `transformer` library is a high-level one. We can download a given model and generate an output using 4 lines of code!\n",
        "\n",
        "Read the docs on the T5 model provided here: https://huggingface.co/docs/transformers/model_doc/t5\n",
        "\n",
        "In the `inference` section, you can find a description showing how we can download a pretrained model, and use it to solve a given task. Simply use the code provided to translate some sentence from English into German!"
      ],
      "metadata": {
        "id": "wSAg0LOEgGj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYOfsrkeJ6GF"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# TODO: instantiate T5 and use it to solve some task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various tasks\n",
        "\n",
        "Experiment with some other inputs, e.g., those provided in Figure 1 presented in the paper introducing the T5 model or even a wider list of use cases from  Appendix D provided with the paper. You can find the paper here: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "Note: there are some abbreviations used among the inputs provided, some of them are:\n",
        "-  `stsb`: it stands for the semantic textual similarity benchmark. Given two sentences, we can calculate their semantic similarities, which can help us determine whether one sentence is a paraphrase of the other one.\n",
        "-  `cola`: it stands for the Corpus of Linguistic Acceptability and helps us determine whether a given sentence is grammatical or ungrammatical.\n",
        "\n",
        "If you look at Appendix D, there are more abbreviations, these are related to the names of tasks presented in the GLUE benchmark (available here: https://gluebenchmark.com/tasks) and the SUPERGLUE benchmark (available here: https://super.gluebenchmark.com/tasks). The idea of GLUE and SUPERGLUE is to collect a set of challenging tasks that may be used to evaluate the systems requiring natural language understanding. \n",
        "\n",
        "**Paste some 3 examples of tasks and the inputs you processed in the cell below** "
      ],
      "metadata": {
        "id": "z322IklnhQnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: paste 3 calls returning the outputs generated by T5 for some selected inputs"
      ],
      "metadata": {
        "id": "OfxmQwV4lIXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various model types\n",
        "\n",
        "There are several T5 models available, which differ in size (and quality). The bigger the model is, the better output it should generate. Experiment with some models from the following set: \n",
        "- t5-small\n",
        "- t5-base\n",
        "- t5-large\n",
        "- t5-3b\n",
        "- t5-11b\n",
        "\n",
        "and check whether you can observe any difference in the quality of outputs.\n",
        "\n",
        "Also, compare the size of the models, you can use the `model.num_parameters()` function to obtain the parameter number related to each model. For each model you are able to load, provide the size in the cell below (if you can't load a given model because it is too big, no worries, just type 'too big to load')."
      ],
      "metadata": {
        "id": "xQvUkC-BlW3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t5-small params number:\n",
        "# t5-base params number:\n",
        "# t5-large params number:\n",
        "# t5-3b params number:\n",
        "# t5-11b params number:"
      ],
      "metadata": {
        "id": "1MTMhyyeR3ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language-specific T5s (OPTIONAL ASSIGNMENT -- you are not required to provide code here)\n",
        "\n",
        "There are even some alternatives to the original T5 models. As the T5 model was trained on English, there are some models available that are specific to other languages, e.g., Polish (for example plT5 proposed by Allegro - https://huggingface.co/allegro/plt5-small). The Polish model was trained to solve a set of tasks collected in the KLEJ benchmark, which represents the Polish analogy to the GLUE benchmark: https://klejbenchmark.com.\n",
        "\n",
        "You can find more details on plT5 in the research paper: https://arxiv.org/pdf/2205.08808.pdf. Table 2 presents some examples of prompts that can be used to solve some of the tasks listed in KLEJ.\n",
        "\n",
        "You can search for an alternative to the original T5, for example, the one related to your language, and experiment with it (**this task is not mandatory**)."
      ],
      "metadata": {
        "id": "yDud66l6msUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (OPTIONAL): If you want, experiment with some alternative models (like language-related, e.g., plT5 related to Polish)"
      ],
      "metadata": {
        "id": "KJZZUkduoEhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flan-T5\n",
        "\n",
        "At the end of 2022, an evolution of T5 was proposed called Flan-T5. This model is also provided by the HuggingFace transformer library. Please visit this website: https://huggingface.co/docs/transformers/model_doc/flan-t5 to see how you can use this model (simply change the name of the model to download!). \n",
        "\n",
        "Flan-T5 is much more powerful than T5. You can take a look at Appendix D included in the paper describing Flan T5 to familiarize yourself with some input formats (prompts) and the generated values. The paper is here: https://arxiv.org/pdf/1910.10683.pdf. You should focus on `processed input` fields as they are the representations that the model consumes. Experiment with some selected tasks and see if you can obtain the same results! In the code below, paste some code loading the Flan-T5 model and using it to solve some selected tasks."
      ],
      "metadata": {
        "id": "e1hsTnxboIe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Here paste some code using Flan-T5 model to solve some task"
      ],
      "metadata": {
        "id": "ppbmGGqVqERf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Fine-tuning\n",
        "\n",
        "You can even fine-tune the T5/Flan-T5 model to solve a task you want. You may load an existing T5/Flan-T5 model, which is already trained to solve some tasks, and use the power of transfer learning to learn it to solve some different tasks. This is much better than training a network from scratch and should require fewer training examples. \n",
        "\n",
        "The fine-tuning phase is quite complex. However, you can find the step-by-step description here: https://www.philschmid.de/fine-tune-flan-t5\n",
        "\n",
        "You can try to fine-tune some selected model."
      ],
      "metadata": {
        "id": "vwyRmBx6q0iT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frt5p4E_rjGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}