{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUxZr25plTgt"
   },
   "source": [
    "# Word Embeddings (static word embeddings)\n",
    "---\n",
    "The bag of words (BOW) representation is very useful when it comes to representing documents as vectors of numbers, but it is not without its drawbacks. It is usually memory-hungry (the length of the vector is the size of of the entire vocabulary /Vocabulary=list of all seen tokens: words, numbers, interpunction/). While it is true that we can reduce memory consumption by limiting the dictionary to the most important tokens from the corpus, we lose the information carried by the deleted tokens, because they are ignored. <br/>\n",
    "Another problem with this representation is its inability to encode word similarity information (as will be shown in subsequent sections). <br/> There is therefore a need to look for alternative representations to overcome the above-mentioned problems. One of them is Word Embeddings.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "We will need external resources (embeddings) to complete the tasks. Go to: http://nlp.stanford.edu/data/glove.6B.zip, unpack the package, and then move the file: \"glove.6B.50d.txt\" to the folder where this notebook is located.</span>\n",
    "\n",
    "# Similarity between documents\n",
    "It is quite a common need to evaluate the similarity of two documents. When we represent documents as vectors of equal length (and using BOW we have equal length vectors), we can use a cosine similarity to measure the similarity of the vectors.\n",
    "\n",
    "$similarity = cos(\\vec{a}, \\vec{b}) = \\frac{\\sum_{i=1}^na_i b_i}{\\sqrt{\\sum_{i=1}^{n}a_i^2}\\sqrt{\\sum_{i=1}^{n}b_i^2}}$\n",
    "<br/>\n",
    "Vector $\\vec{a}$ represents the first, and vector $\\vec{b}$ the second document.\n",
    "<br/>\n",
    "Below you can find an implementation of cosine similarity using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Done by:** Sofya Aksenyuk, 150284.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lN6cUXEtlTgy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7977198918178166\n",
      "0.23409628697705598\n",
      "-0.48434715333575534\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   \n",
    "\n",
    "def cosine(v1, v2):\n",
    "    v1, v2 = np.array(v1), np.array(v2)\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "print(cosine([1.0, 2.0, 3.0], [1.5, -0.7, -20]))\n",
    "print(cosine([-10.0, 17.0, 2.0], [5.3, 12.0, -20]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, -3000, 184]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md09VaHmlThV"
   },
   "source": [
    "We can generate Bag of Words representation using `CountVectorizer` object provided in `sklearn`. Let's use it to transform each document into a vector counting how many times a given word occurs in the document. Then, we can measure the similarity between vectors by using `cosine` similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZQW0jxaZlTha"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents represented using BOW. Documents represent rows, columns represent tokens (there are 5 distinct tokens (words) in these documents)\n",
      "Cell at row x and col y represents how many times a token assigned to position y is observed in document x\n",
      "[[1 0 1 0 1]\n",
      " [1 1 1 1 1]]\n",
      "\n",
      "\n",
      "Document similarity:\n",
      "0.7745966692414834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "doc1 = \"Ala has a cat\"\n",
    "doc2 = \"Ala has a beautiful fluffy cat\"\n",
    "\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(docs).todense()\n",
    "\n",
    "print(\"Documents represented using BOW. Documents represent rows, columns represent tokens (there are 5 distinct tokens (words) in these documents)\")\n",
    "print(\"Cell at row x and col y represents how many times a token assigned to position y is observed in document x\")\n",
    "print(X_train_counts)\n",
    "print(\"\\n\\nDocument similarity:\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0])) # tolist()[0] transforms a 1xn matrix into a list of 1x elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EWfaLX5lTho"
   },
   "source": [
    "When documents share words, the cosine similarity is non-zero. However, what happens when we have two very similar documents expressed using synonymous words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uJ-0hBdJlThs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents represented using BOW. Documents represent rows, columns represent tokens (there are 5 distinct tokens (words) in these documents)\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "\n",
      "Document similarity:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "doc1 = \"cat\"\n",
    "doc2 = \"kitten\"\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "\n",
    "print(\"Documents represented using BOW. Documents represent rows, columns represent tokens (there are 5 distinct tokens (words) in these documents)\")\n",
    "X_train_counts = count_vect.fit_transform(docs).todense()\n",
    "print(X_train_counts)\n",
    "\n",
    "print(\"\\n\\nDocument similarity:\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "F2x8XQAslTh4"
   },
   "source": [
    "Using BOW there is no way of spotting that a `kitten` is semantically related to a `cat` (at least more related than a `chair` or a `car`)!\n",
    "\n",
    "---\n",
    "Embeddings are nothing more than vector representation of the meaning of words (tokens) in an n-dimensional space so that similar words appear closely in this n-dimensional space. We can create them ourselves from a large body of text (which can be time-consuming), using packages such as: gensim (https://radimrehurek.com/gensim/) but we can also use \"pretrained\" vectors already created on some corpus, available e.g. at: (https://nlp.stanford.edu/projects/glove/). We will choose the second option - using existing vectors. <br/>\n",
    "\n",
    "Embeddings provided by the Stanford team are text files represented as a set of lines: <br/>\n",
    "word [SPACE] vector_of_numbers_separated_by_spaces_representing_words_meaning <br/>\n",
    "\n",
    "The function to load embeddings has already been created. <br/> **Run the following code so that we can use this function in subsequent cells and evaluate the similarity of words.** <span style = \"color: red\"> Note: the `mapping` variable will be used in subsequent cells, so we have to run that code to make it visible for subsequent cells. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ueRQZF8MlTh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between a cat and a kitten:\n",
      "0.6386305647068642\n",
      "Similarity between a cat and a chair\n",
      "0.29425297716624566\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(path):\n",
    "    mapping = dict()\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            splitted = line.split(\" \")\n",
    "            mapping[splitted[0]] = np.array(splitted[1:], dtype=float)\n",
    "    return mapping\n",
    "\n",
    "mapping = load_embeddings('glove.6B.50d.txt') # load embeddings into a dict mapping words into vectors\n",
    "\n",
    "cat = mapping['cat']\n",
    "kitten = mapping['kitten']\n",
    "chair = mapping['chair']\n",
    "\n",
    "print(\"Similarity between a cat and a kitten:\")\n",
    "print(cosine(cat, kitten))\n",
    "\n",
    "print(\"Similarity between a cat and a chair\")\n",
    "print(cosine(cat, chair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh6yBWbFlTiH"
   },
   "source": [
    "As can be seen, we can represent WORDS (TOKENS) as vectors instead of whole documents. As a result, we can measure the similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKOKHf8VlTiI"
   },
   "source": [
    "# Embedding space\n",
    "Embeddings are representations of the meaning of words in n-dimensional space. The website: http://projector.tensorflow.org tries to visualize this space by projecting the pretrained vectors onto 3-dimensional space.\n",
    "\n",
    "To complete the task, open the above page and follow steps a) and b) <br/>\n",
    "**Task**: List 5 closest words to the word \"data\" analyzing embeddings loaded by default by the website (We can type the word whose neighbors we want to locate in the \"Search\" field in the upper right part of the screen).\n",
    "Let's use cosine distance to measure distance between embeddings.\n",
    "<br/>\n",
    "Note - this page uses cosine distance instead of similarity. The relationship between the two measures is very simple: distance = 1 - similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "I6CLUpvmlTiM"
   },
   "outputs": [],
   "source": [
    "# 1. Słowo: information        Distance: 0.435\n",
    "# 2. Słowo: instructions       Distance: 0.506\n",
    "# 3. Słowo: files              Distance: 0.522\n",
    "# 4. Słowo: file               Distance: 0.542\n",
    "# 5. Słowo: register           Distance: 0.547"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKDvakKZlTic"
   },
   "source": [
    "As we can see, the words that come closest to the word \"date\" are synonymous in this case. <br/>\n",
    "**Task**: Enter 5 closest words to the word \"red\". Are we still dealing with synonyms?\n",
    "Answer the question: how can you interpret the most similar words (in what aspect are they similar), since, as you can see, they are not synonyms (recall the principle of embedding)? Share your answers in the comments below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i7slgdzAlTif"
   },
   "outputs": [],
   "source": [
    "# 1. Słowo: blue       Distance: 0.333\n",
    "# 2. Słowo: yellow     Distance: 0.380\n",
    "# 3. Słowo: white      Distance: 0.391\n",
    "# 4. Słowo: green      Distance: 0.396\n",
    "# 5. Słowo: black      Distance: 0.489\n",
    "\n",
    "# Interpretation: They all belong to one group - colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YinOeLHNlTio"
   },
   "source": [
    "# Embeddings model relations between words\n",
    "\n",
    "Embeddings contain information about the meaning. What's more - they are vectors, so we can perform operations on them (addition, subtraction, ...). Let's check what effects we get by performing operations on these vectors.\n",
    "<br/>\n",
    "We will be interested in the effect of the operation: $\\vec{italy} - \\vec {rome} + \\vec{warsaw}$. What will the vector defined in this way indicate? <br/>\n",
    "Since the result of this operation will be a new vector, let's write a function that checks which existing word is closest to this vector.\n",
    "\n",
    "**Task:** Fill the `get_most_similar(...)` function so that for the given vector `vec1`, it returns the word whose vector (embedding) is the most similar to the vector `vec1` (to evaluate the similarity use the cosine function created at the beggining of this notebook). The embeddings parameter is a dictionary that maps a word to the corresponding vector. <br/>\n",
    "What word was determined closest to the calculated point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "sS75OtnLlTiq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poland\n"
     ]
    }
   ],
   "source": [
    "def get_most_similar(vec1, embeddings):\n",
    "    all_similarities = sorted(embeddings.items(), key=lambda x: cosine(vec1, x[1]), reverse=True)\n",
    "    \n",
    "    return all_similarities[0][0]\n",
    "\n",
    "new_point = mapping['italy'] - mapping['rome'] + mapping['warsaw']\n",
    "print(get_most_similar(new_point, mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fskt2SGClTi4"
   },
   "source": [
    "Therefore, we can see that performing operations on embedding allows for very interesting results. If we subtract the capital from the \"Italy\" object and add the Polish capital, we will get the \"Poland\" object. In other words - we answer the question: what is the word in the same relation to Poland as the relation between Italy and Rome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07h6J9plTi5"
   },
   "source": [
    "# Words embeddings for document classification, simple but powerful heuristic\n",
    "\n",
    "It also turns out that embeddings are useful in classification, effectively reducing the number of features and solving the problem of sparse representation introduced by BOW (most BOW vectors have >50% of elements set to 0). Let's imagine a spam classification task. In order to decide whether we are dealing with spam or ham, we would like to use the SVC classifier that adopts embedding as features. <br/>\n",
    "\n",
    "However, since embedding describes individual words as n-dimensional vectors, and in the problem of classifying e-mails, we have to represent entire documents as vectors - we need to aggregate information about all words in one feature vector.\n",
    "<br/>\n",
    "\n",
    "One method that works surprisingly well is to represent the entire document as a vector, which is the \"center of gravity\" of the words it is made of. The result vector is an n-dimensional vector (as is the vector of each of the \"component\" words), where the $i$-th position of the vector has a value that is the arithmetic mean of the $i$-th positions of the word vectors from the given document. NOTE: it may turn out that in pre-trained embeddings the word from the document that we want to represent as a vector is not present. In such a situation, let us ignore this word completely (let us assume that it does not exist).\n",
    "<br/>\n",
    "<br/>\n",
    "**TASK**: Implement the function `documents_to_ave_embeddings(docs, embeddings)` taking two parameters:\n",
    "<ol>\n",
    "    <li> docs - list of documents in text form (list of strings) to be transformed into vectors </li>\n",
    "    <li> embeddings - word mapping -> vector (embedding) from an existing model </li>\n",
    "</ol>\n",
    "The function should return one variable, a list of document vectors, where 1 document is the \"average vector\" of the vectors of the words it is made of (as in the paragraph above the content of the task). Use a tokenizer with NLTK (`word_tokenize`) and before tokenizing - convert document texts to all lowercase letters.\n",
    "\n",
    "Consider using `numpy.mean` with the appropriate axis parameter value to calculate the mean (it will be easier to use a ready-made function than to implement it manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9VIwtTljlTi8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.81446111869032%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       517\n",
      "           1       0.88      0.84      0.86       216\n",
      "\n",
      "    accuracy                           0.92       733\n",
      "   macro avg       0.91      0.89      0.90       733\n",
      "weighted avg       0.92      0.92      0.92       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "\n",
    "full_dataset = pandas.read_csv('spam_emails.csv', encoding='utf-8')      # read the CSV file\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham':0, 'spam':1})  # map string labels \"ham\"/\"spam\", into numbers so that they can be processed by sklearn\n",
    "\n",
    "np.random.seed(0)                                       # set seed = 0 to ensure reproducibility of experiments\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7 # choose random 70% of rows as a trainset\n",
    "train = full_dataset[train_indices] # create trainset (70%)\n",
    "test = full_dataset[~train_indices] # create testset (remaining - 30%)\n",
    "\n",
    "\n",
    "def documents_to_ave_embeddings(docs, embeddings):\n",
    "    def avg_document(doc):\n",
    "        to_avg = [embeddings[x] for x in word_tokenize(doc.lower()) if x in embeddings]\n",
    "        \n",
    "        return np.mean(to_avg, axis=0)\n",
    "    \n",
    "    return [avg_document(doc) for doc in docs]\n",
    "        \n",
    "\n",
    "# ------------------- VECTORIZE -----------\n",
    " \n",
    "classifier = SVC(C=1.0)\n",
    "\n",
    "train_transformed = documents_to_ave_embeddings(train['text'], mapping)\n",
    "test_transformed = documents_to_ave_embeddings(test['text'], mapping)\n",
    "\n",
    "# ------------------- TRAIN CLASSIFIER -----------\n",
    "\n",
    "classifier.fit(train_transformed, train['label_num']) \n",
    "\n",
    "# ------------------- EVALUATE -----------\n",
    "accuracy = classifier.score(test_transformed, test['label_num'])\n",
    "print(\"Accuracy: {n}%\".format(n=100.*accuracy))\n",
    "print(classification_report(test['label_num'], classifier.predict(test_transformed))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "IQOivOVBlTjX"
   },
   "source": [
    "# Training your own vectors\n",
    "Since training vectors on a large body can be time-consuming and requires a large body to catch the right contexts of words, we did not do it in laboratories. If you are interested in creating your own vectors, I recommend the article: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/, which describes how to train embeddings using python and the `gensim` package."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
