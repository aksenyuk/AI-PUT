{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MB7EDV0-Rym"
      },
      "source": [
        "#Computational linguistics\n",
        "\n",
        "We are going to use [spaCy](https://spacy.io), a popular library for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Done by:** Sofya Aksenyuk, 150284\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bkUoL6ad2J4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "L-_2GtQ7-Ryq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e94e82-1a5d-4201-951f-31723e8d02b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-21 15:06:36.279960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-21 15:06:37.497347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md') # load a medium sized model for English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s8AzU0o-Ryt"
      },
      "source": [
        "## Relationship extraction\n",
        "\n",
        "Understanding a text requires both understanding individual words and the relationship between those words. While we have already talked about the meaning of individual words (embeddings and similarity assessment with their use, as well as the problem of POS-tagging, which reveals what part of speech a given word represents), we did not talk much about the relationships between words.\n",
        "\n",
        "The relations between the words in a sentence are governed by grammar, thanks to which we can understand how the ideas mentioned in the sentences are related to each other. Previous research in the field of natural language processing has proposed the so-called dependency trees (dependency tree or dependency parse tree), as a visualization of grammatical dependencies between words in the form of a tree. The root of this tree is the most often the most important verb in the sentence. Connections between nodes in a dependency tree are labeled with relationship names.\n",
        "\n",
        "Visualizations of generated dependency trees for given sentences can be generated at: https://explosion.ai/demos/displacy\n",
        "\n",
        "The labels on the edges of the tree are described at: https://nlp.stanford.edu/software/dependencies_manual.pdf in Chapter 2.\n",
        "\n",
        "We can visualize the dependency tree (no labels on node connections) using spaCy and NLTK. Run the following code to observe the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h1KwOcul-Ryt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0608b259-2362-4bb9-bf9f-63bc2aaf958e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the lazy dog.\n",
            "-----------------------------------\n",
            "        jumps                    \n",
            "  ________|______________         \n",
            " |        |             over     \n",
            " |        |              |        \n",
            " |       fox            dog      \n",
            " |    ____|_____      ___|____    \n",
            " .  The quick brown the      lazy\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Mary met Mike.\n",
            "-----------------------------------\n",
            "     met     \n",
            "  ____|____   \n",
            "Mary Mike  . \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk import Tree # Helper object to visualize the tree\n",
        "\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog. Mary met Mike.\") # Sample sentences to process\n",
        "\n",
        "def to_nltk_tree(node): # tworzymy drzewo\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.text, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.text\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)\n",
        "    print(\"-----------------------------------\")\n",
        "    to_nltk_tree(sent.root).pretty_print() # create a tree and provide a beautiful visualizatiion ;)\n",
        "    print(\"\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjtJKHrR-Ryt"
      },
      "source": [
        "What can a dependency tree be useful for?\n",
        "We can use such a tree, for example, to simplify sentences, discover relationships between sentence elements, or, for example, to discover what part of the text an emotionally charged phrase refers to (\"I really like these grandma's country dumplings, but I don't despise a good kebab either\" = > I like dumplings, I don't despise kebab)\n",
        "\n",
        "Let's use the dependency tree to create a simplified representation of a sentence containing a relation (verb) and the arguments of this relation in the form `relation(argument1, argument2,...)`\n",
        "\n",
        "**Task: Simple Relationship Extraction Using Dependency Tree**\n",
        "\n",
        "Using the attributes of the tokens created by spaCy after running the nlp() function (https://spacy.io/api/token#attributes) - create a CSV-like (space separated lists) representation with the following attributes (columns):\n",
        "<ol>\n",
        "<li>identifier of the word in the document</li>\n",
        "<li>word text</li>\n",
        "<li>dependency tree label on \"parent\" connection</li>\n",
        "<li>parent text from dependency tree</li>\n",
        "<li>a list of children from the dependency tree</li>\n",
        "</ol>\n",
        "\n",
        "The expected result:\n",
        "\n",
        "<pre>\n",
        "0 The det fox []\n",
        "1 quick amod fox []\n",
        "2 brown amod fox []\n",
        "3 fox nsubj jumps [The, quick, brown]\n",
        "4 jumps ROOT jumps [fox, over, .]\n",
        "5 over prep jumps [dog]\n",
        "6 the det dog []\n",
        "7 lazy amod dog []\n",
        "8 dog pobj over [the, lazy]\n",
        "9 . punct jumps []\n",
        "\n",
        "\n",
        "10 Mary nsubj met []\n",
        "11 met ROOT met [Mary, Mike, .]\n",
        "12 Mike dobj met []\n",
        "13 . punct met []\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6TIjpBUQ-Ryu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aef7652-a832-4c9c-e371-4f440ff90a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 The det fox []\n",
            "1 quick amod fox []\n",
            "2 brown amod fox []\n",
            "3 fox nsubj jumps [The, quick, brown]\n",
            "4 jumps ROOT jumps [fox, over, .]\n",
            "5 over prep jumps [dog]\n",
            "6 the det dog []\n",
            "7 lazy amod dog []\n",
            "8 dog pobj over [the, lazy]\n",
            "9 . punct jumps []\n",
            "\n",
            "\n",
            "\n",
            "10 Mary nsubj met []\n",
            "11 met ROOT met [Mary, Mike, .]\n",
            "12 Mike npadvmod met []\n",
            "13 . punct met []\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk import Tree # Helper for tree generation\n",
        "\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog. Mary met Mike.\") # Sample sentences\n",
        "\n",
        "#TODO - visualize the output\n",
        "idx = 0\n",
        "for sent in doc.sents:\n",
        "    for word in sent:\n",
        "        print(idx, word, word.dep_, word.head, list(word.children))\n",
        "        idx += 1\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Note:** It can be noticed that `Mike` is shown to be of other type of dependency (npadvmod, not dobj), which is also considered in the next task.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ywu0KMXk68jR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvJhRxAo-Ryu"
      },
      "source": [
        "We see that the most important word is the verb \"jumps\" (root of the dependency tree (ROOT)).\n",
        "We also see that the words are grouped accordingly. Children of the word 'fox' are ['The', 'quick', 'brown'] - so the terms that define what this fox is like! (Similar case for the word dog)\n",
        "\n",
        "\n",
        "**Task: Relationship extraction**\n",
        "\n",
        "Knowing how to retrieve information about the dependency tree from Token objects in spaCy, write a parsing function that for each sentence (sentence processed by spaCy) will extract the most important relation name (a verb marked as a ROOT), as well as arguments of this relation (subject and object) based on generated dependency tree.\n",
        "\n",
        "<ol>\n",
        "<li>The relation should be stored in the predicate variable</li>\n",
        "<li>The subject, let's define it as a token from the sentence, which is connected to the ROOT by the relation 'nsubj', should be stored in the variable subj.</li>\n",
        "<li>and the object can be defined, for example, as: an element connected with the ROOT by the relation 'dobj', or, if the ROOT has no connection 'dobj', but is connected with the element by the relation 'prep' (preposition in relation to the verb), then the predicate is a token that is linked to this preposition by the relation 'pobj'. If the second situation occurs, i.e. the preposition is connected directly with the ROOT - and only this preposition with the term, the preposition should be added to the string stored in the predicate variable (For simplicity, let's assume that the preposition always follows the verb). Store the object in the 'obj' variable.</li>\n",
        "</ol>\n",
        "To understand how the object relation works, look at the expected output of this task and the dependency tree generated in the first snippet of this section.\n",
        "\n",
        "The expected result:\n",
        "<pre>\n",
        "jumps over(fox, dog)\n",
        "meth(Mary, Mike)\n",
        "</pre>\n",
        "\n",
        "While the second example of met(Mary, Mike) is obvious, the first should identify the word 'jumps' as a relation, note that there is no direct object (no 'dobj' for root), instead we have the preposition `over`, which in turn is combined with the expected object ('dog'). Therefore, we add the preposition to the name of the predicate, replacing the previous jumps with jumps over, and the complement is the element connected with the preposition by the relation 'pobj': dog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HAMuLx-U-Ryu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479b37a0-03e7-4b6e-9937-28a0b166ba70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jumps over(fox, dog)\n",
            "met(Mary, Mike)\n"
          ]
        }
      ],
      "source": [
        "from nltk import Tree # Helper function used to print a tree\n",
        "from spacy.symbols import pobj, dobj, prep, nsubj, npadvmod\n",
        "doc = nlp(\"The quick brown fox jumps over the lazy dog. Mary met Mike.\") # przykładowe zdania do przetworzenia\n",
        "\n",
        "def parse(sent):\n",
        "    # predicate = None\n",
        "    subj = None\n",
        "    obj = None\n",
        "    predicate = [sent.root.text]\n",
        "    # TODO: Detect subject, object and main predicate\n",
        "    for word in sent:\n",
        "        if word.head.text in predicate:\n",
        "            if word.dep in [pobj, dobj, npadvmod]:\n",
        "                obj = word.text\n",
        "            if word.dep == prep:\n",
        "                predicate.append(word.text)\n",
        "            if word.dep == nsubj:\n",
        "                subj = word.text\n",
        "\n",
        "    predicate = ' '.join(predicate)\n",
        "    print(\"{pred}({subj}, {obj})\".format(pred=predicate, subj=subj, obj=obj))\n",
        "    \n",
        "for sent in doc.sents:\n",
        "    parse(sent)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional:\n",
        "Download BRAT annotation tool from GitHub: https://github.com/nlplab/brat and try to run the annotation server locally. This server may be used to mark spans of text and relations between them. You need to download the package, unpack it, and install via `sudo ./install.sh`. Then, after creating an account, you should be able to run the server via `python standalone.py`. Please remember that you need to log in after running the server (there is a button in the top-right corner).\n",
        "\n",
        "Datasets are stored in folders, files to annotate are simple textual files. Annotations are also serialized as textual names sharing the same filename, but anding with `*.ann` extension.\n",
        "\n",
        "The whole configuration of our tasks is stored in config files, out of which `annotation.conf` file is the most important one (that should be placed in the same folder as our dataset (e.g., https://github.com/nlplab/brat/blob/master/example-data/tutorials/bio/annotation.conf). We have also config files for visuals (visual.conf, e.g., https://github.com/nlplab/brat/blob/master/example-data/tutorials/bio/visual.conf), where we can define colors for our annotations and keyboard shortcuts (kb_shortcuts.conf, e.g., https://github.com/nlplab/brat/blob/master/example-data/tutorials/bio/kb_shortcuts.conf).\n",
        "\n",
        "More information about the tool can be found on the website: https://brat.nlplab.org\n"
      ],
      "metadata": {
        "id": "ki1kIcyNHtrn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5C9DtaNLu5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}