{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45UD3DuQ_P-"
      },
      "source": [
        "#CompIntel-Lab6: Active Learning: Simulated-user Active Regression with pool-based sampling for numerical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ONWsRL6SDF4"
      },
      "source": [
        "Based on: \n",
        "\n",
        "* https://scikit-learn.org/stable/_downloads/21b82d82985712b5de6347f382c77c86/plot_partial_dependence.ipynb\n",
        "* https://modal-python.readthedocs.io/en/latest/content/examples/active_regression.html\n",
        "* https://modal-python.readthedocs.io/en/latest/content/examples/ensemble_regression.html\n",
        "* https://modal-python.readthedocs.io/en/latest/content/examples/query_by_committee.html\n",
        "* https://modal-python.readthedocs.io/en/latest/content/examples/bootstrapping_and_bagging.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8EINShGSkgl"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijs7XjaYbtNd"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3feT8ztSkgy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cal_housing = fetch_california_housing()\n",
        "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
        "y = cal_housing.target\n",
        "\n",
        "#y -= y.mean()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADVlo3WkTED1"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "equ6it_qf_JK"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBsfIIE6gF0H"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gakpZtN6Skg1"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "print(\"Training MLPRegressor...\")\n",
        "tic = time()\n",
        "est = make_pipeline(\n",
        "    QuantileTransformer(),\n",
        "    MLPRegressor(\n",
        "        hidden_layer_sizes=(30, 15),\n",
        "        learning_rate_init=0.01,\n",
        "        early_stopping=True,\n",
        "        random_state=0,\n",
        "    ),\n",
        ")\n",
        "est.fit(X_train, y_train)\n",
        "print(f\"done in {time() - tic:.3f}s\")\n",
        "print(f\"Test R2 score: {est.score(X_test, y_test):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sNDs3A5td6w"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01sfymJSfA_V"
      },
      "outputs": [],
      "source": [
        "print([np.random.randint(10) for _ in range(10)])\n",
        "print([np.random.randint(10) for _ in range(10)])\n",
        "# Szwabe: What is it for?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: We need real randomness for real expolation. Therefore, we make sure that fixed seed is turned off."
      ],
      "metadata": {
        "id": "_j4MmzT2lzHs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJva5guVIOi"
      },
      "source": [
        "#One-dimensional active regression toy example\n",
        "\n",
        "In this example, we are going to demonstrate how can the ActiveLearner be used for active regression using Gaussian processes. Since Gaussian processes provide a way to quantify uncertainty of the predictions as the covariance function of the process, they can be used in an active learning setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y2wAwUIU8z0"
      },
      "outputs": [],
      "source": [
        "!pip install modAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rrkmfe_UCXF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
        "from modAL.models import ActiveLearner\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoAsNi_fUg_P"
      },
      "outputs": [],
      "source": [
        "X = np.random.choice(np.linspace(0, 20, 10000), size=200, replace=False).reshape(-1, 1)\n",
        "y = np.sin(X) + np.random.normal(scale=0.3, size=X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__kshZ3RUhBf"
      },
      "outputs": [],
      "source": [
        "with plt.style.context('seaborn-white'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(X, y, c='k', s=20)\n",
        "    plt.title('sin(x) + noise')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B11B3NQUqOf"
      },
      "outputs": [],
      "source": [
        "def GP_regression_std(regressor, X):\n",
        "    _, std = regressor.predict(X, return_std=True)\n",
        "    query_idx = np.argmax(std)\n",
        "    return query_idx, X[query_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLptAaSrUtaw"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH_afjYFUv6F"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NDKz5C9UygO"
      },
      "outputs": [],
      "source": [
        "with plt.style.context('seaborn-white'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(X_grid, y_pred)\n",
        "    plt.fill_between(X_grid, y_pred - y_std, y_pred + y_std, alpha=0.2)\n",
        "    plt.scatter(X, y, c='k', s=20)\n",
        "    plt.title('Initial prediction')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j41aSpSZU07f"
      },
      "outputs": [],
      "source": [
        "n_queries = 10\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHesjxYKU26h"
      },
      "outputs": [],
      "source": [
        "y_pred_final, y_std_final = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred_final, y_std_final = y_pred_final.ravel(), y_std_final.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcGqKEKeU5qW"
      },
      "outputs": [],
      "source": [
        "with plt.style.context('seaborn-white'):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(X_grid, y_pred_final)\n",
        "    plt.fill_between(X_grid, y_pred_final - y_std_final, y_pred_final + y_std_final, alpha=0.2)\n",
        "    plt.scatter(X, y, c='k', s=20)\n",
        "    plt.title('Prediction after active learning')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEfnAeqnQGSB"
      },
      "source": [
        "### Despite variance underestimation (light blue area), GPR with a 'standard' kernel function parameters space enables both: accurate expected value prediction (dark blue curve) and sufficient exploration:   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNDZ6DFGaAuY"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMqBKXXmaAud"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWU_vd1CQMkv"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj3F8fdeV9cl"
      },
      "outputs": [],
      "source": [
        "regressor.estimator.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsQpDuO6d5zG"
      },
      "source": [
        "#The impact of the Gaussian process regression kernel function parameters optimization on the variance prediction quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFjrjDp-Rh6L"
      },
      "source": [
        "## GRP kernel parameters are not fixed - their ranges are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vgn-1vbhorV"
      },
      "source": [
        "\"kernelkernel instance, default=None\n",
        "\n",
        "The kernel specifying the covariance function of the GP. If None is passed, the kernel ConstantKernel(1.0, constant_value_bounds=\"fixed\" * RBF(1.0, length_scale_bounds=\"fixed\") is used as default. Note that the kernel hyperparameters are optimized during fitting unless the bounds are marked as “fixed”.\n",
        "alphafloat or ndarray of shape (n_samples,), default=1e-10\n",
        "\n",
        "Value added to the diagonal of the kernel matrix during fitting. This can prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. It can also be interpreted as the variance of additional Gaussian measurement noise on the training observations. Note that this is different from using a WhiteKernel. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge.\n",
        "optimizer“fmin_l_bfgs_b” or callable, default=”fmin_l_bfgs_b”\n",
        "\n",
        "Can either be one of the internally supported optimizers for optimizing the kernel’s parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the signature:\n",
        "\n",
        "    def optimizer(obj_func, initial_theta, bounds):\n",
        "        # * 'obj_func': the objective function to be minimized, which\n",
        "        #   takes the hyperparameters theta as a parameter and an\n",
        "        #   optional flag eval_gradient, which determines if the\n",
        "        #   gradient is returned additionally to the function value\n",
        "        # * 'initial_theta': the initial value for theta, which can be\n",
        "        #   used by local optimizers\n",
        "        # * 'bounds': the bounds on the values of theta\n",
        "        ....\n",
        "        # Returned are the best found hyperparameters theta and\n",
        "        # the corresponding value of the target function.\n",
        "        return theta_opt, func_min\n",
        "\n",
        "Per default, the L-BFGS-B algorithm from scipy.optimize.minimize is used. If None is passed, the kernel’s parameters are kept fixed. Available internal optimizers are: {'fmin_l_bfgs_b'}.\n",
        "n_restarts_optimizerint, default=0\n",
        "\n",
        "The number of restarts of the optimizer for finding the kernel’s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel’s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.\"\n",
        "\n",
        "[https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW43ZQwLg8J6"
      },
      "source": [
        "###Szwabe: What may be the impact of setting the GPR kernel parameters to fixed values (i.e. turning off the default optimization function) to the AR performance? Does variance overestimation inavoidably lead to poor exploration? \n",
        "\n",
        "150284: The fixed noise level = 1 is not that reasonable choice in our case, since it seems to be too big, i.e., it brings overestimation, which leads to noise. However, noise from overestimation actually can be used for better exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSzU7-Jpea1s"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1.0,length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=1, noise_level_bounds=\"fixed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kD94Lk6d3Rh"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZtxcKabd3Rl"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0HKqEDpd3Ro"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEq6IKX_TYFk"
      },
      "source": [
        "###Szwabe: Can a radical (10 orders of magnitude) increase of the lower range of the homoskedastic ('white') factor of the GPR kernel make both mean and variance predictions more robust? Is the improvement possible for very few training samples? \n",
        "\n",
        "150284: Adding white component to the kernel when using GPR even not expecting big noise may be beneficial, since it increases the stability of self-optimization, providing better robustness. You could also do it by increasing the lower range of the white factor. GPR is the best regressor in terms of being able to provide reasonable prediction for a very few data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsJ_TGVZeqq3"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-1, 1e+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWobGex3eqq6"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oSGVdfeqq8"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20B-eVgveqq-"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fog68agIojvK"
      },
      "source": [
        "## Szwabe: What if we knew the optimal kernel parameters... Wait! In this particular toy example we actually know them! Let's fix just the most important one. Can it improve the robustness of the predictions for a very low number of training samples? Is the exploration capability compromised by fixing the parameter of the homoskedastic 'ingredient' of the GPR kernel?  \n",
        "\n",
        "150284: Making white kernel component work by setting non-zero variance at known data points. By fixing the white component, we indeed can improve robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nVBobbiol1p"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1.0,length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=0.3, noise_level_bounds=\"fixed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbOg2NQ3ol1r"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2YiHcSxol1u"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T-H-DULol1u"
      },
      "outputs": [],
      "source": [
        "n_queries = 21\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NnqI-QFpYNn"
      },
      "outputs": [],
      "source": [
        "regressor.estimator.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9Hf45Fd1Z5T"
      },
      "outputs": [],
      "source": [
        "np.pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQufObhRpd51"
      },
      "source": [
        "##Szwabe: What if we fix the kernel length scale as well? To what value?\n",
        "\n",
        "150284: Width of the kernel to be of a similar shape as the given waveland of sinus function (see second plot below). If we are able to estimate the width of RBF kernel well, it may be good enough to represent the data, so that the white component may be not needed and have low amplitude therefore. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J_iMt2ZqYrt"
      },
      "outputs": [],
      "source": [
        "with plt.style.context('seaborn-white'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(X, np.sin(X), c='k', s=20)\n",
        "    plt.title('sin(x)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW8yX07XtNIt"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HxhbHaltVd8"
      },
      "outputs": [],
      "source": [
        "#temp_length_scale = math.sqrt(np.pi/2)\n",
        "temp_length_scale = 1.0\n",
        "temp_kernel = RBF(length_scale=temp_length_scale)\n",
        "temp_estimator = GaussianProcessRegressor(kernel=temp_kernel)\n",
        "temp_estimator.fit([[np.pi/2]],[1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp3CgAaOqtTx"
      },
      "outputs": [],
      "source": [
        "X_aux = np.linspace(-np.pi/2, np.pi*1.5, 100)\n",
        "mu = np.pi/2\n",
        "variance = 1\n",
        "sigma = math.sqrt(variance)\n",
        "sigma2 = sigma\n",
        "with plt.style.context('seaborn-white'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(X_aux, np.sin(X_aux), label=\"sin(x)\")\n",
        "    plt.plot(X_aux, 2*stats.norm.pdf(X_aux, mu, sigma2)/stats.norm.pdf(mu, mu, sigma2)-1.0, label=\"scaled N(pi/2,\"+str(sigma2*2)+\"1)(x)-1.0\")\n",
        "    plt.plot(X_aux, 0.01 + 2*temp_estimator.predict(X_aux.reshape(-1, 1))-1.0, label=\"2*RBF(length_scale=\"+str(temp_length_scale)+\")-1.0\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I-BUezpJ4ln"
      },
      "outputs": [],
      "source": [
        "temp_estimator.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7CMzeFhpcKz"
      },
      "outputs": [],
      "source": [
        "#kernel = RBF(length_scale=np.pi,length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.3, noise_level_bounds=\"fixed\")\n",
        "kernel = RBF(length_scale=1.0,length_scale_bounds=\"fixed\") + WhiteKernel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgX2VlYzpcK1"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYtOSOIpcK3"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlTR0ukHpcK4"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: Variance is assumed to be zero in intersection points (green crosses). Variance is used to estimate uncertainty. In some cases, makes exploration faster. To get variance estimation, GPR/ETR algorithms may be used. However, ETR is much less computationally expensive."
      ],
      "metadata": {
        "id": "RXrNulssjxd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO6KCwuZHTFA"
      },
      "outputs": [],
      "source": [
        "regressor.estimator.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zjagQh8Kd4y"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
        "\n",
        "seasonal_kernel = (RBF(length_scale=1.0, length_scale_bounds=\"fixed\") * ExpSineSquared(length_scale=np.pi, periodicity=1.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azU-KO0uLKWa"
      },
      "outputs": [],
      "source": [
        "#kernel = RBF(length_scale=np.pi,length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=0.3, noise_level_bounds=\"fixed\")\n",
        "#kernel = ExpSineSquared(length_scale=1.0, periodicity=np.pi/2, periodicity_bounds=\"fixed\") \n",
        "kernel = seasonal_kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXcNrOWNLKWb"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD-GSjHyLKWd"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP_JvwfHLKWd"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppUwiUWyLKWf"
      },
      "outputs": [],
      "source": [
        "regressor.estimator.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvs8I3vTnQaz"
      },
      "source": [
        "#Szwabe: Can using an tree-based variance-aware regressor (computationally much less complex) instead of GPR help? What is the difference between the default configurations of ET regressors for the generic use of Scikit-Learn and for the case of using ETR for BO? Why? Why are the exploration capabilities of AR based on ETR of BO-like parameters limited? \n",
        "\n",
        "150284: ETR is not kernel-based, it's a modification of RandomForest with an additional feature of predicting variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF2lVUBGihMC"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMyHP8Wckpmr"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsWpazG-lupP"
      },
      "outputs": [],
      "source": [
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzawUBVafZRV"
      },
      "outputs": [],
      "source": [
        "SklearnExtraTreesRegressor = sklearn.ensemble.ExtraTreesRegressor()\n",
        "SklearnExtraTreesRegressor.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUhGrtNHi1RV"
      },
      "outputs": [],
      "source": [
        "from skopt.learning import ExtraTreesRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSq4y112Y-V2"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor = ExtraTreesRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tnY4OSvZCco"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znhvwfBSi9B4"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor = ExtraTreesRegressor(n_estimators=10, min_samples_split=2, min_samples_leaf=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwOdY0AYbUJJ"
      },
      "outputs": [],
      "source": [
        "#!pip install -U scikit-learn==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtlOyRZZihpx"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=TreeBasedRegressor,\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktu9Dlz2ihpz"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NALf1fb-ihp0"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Ox84bAjbSw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSiusyAwnpBL"
      },
      "source": [
        "##Szwabe: The impact of the tree-based regressor... hyperparameters (!) (kind-of chicken and egg or infinite number of optimizers of other optimizers)\n",
        "### What makes the AR exploration problem different from analogical BO exploration problem?\n",
        "### Poor exploration capablities of AR based on ETR of 'genereric' hyperparameters' configuration: \n",
        "\n",
        "150284: Selecting hyperparameters plays a big role in ETR, because of the lack of built-in self-optimization (as opposite to GPR). Typical values of good GPR hyperparameters may not be typical to Random Forest. Active Regression nature of ETR makes it better use around 20 trees (in opposite to default 100 of Random Forest).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu8DL40Bjbon"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor = ExtraTreesRegressor(n_estimators=100, min_samples_split=2, min_samples_leaf=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIwNrtLAjbop"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=TreeBasedRegressor,\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTbx1TTUjboq"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuwlrC7wjbos"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tVp9AaVrVgM"
      },
      "source": [
        "### Can changing values of ETR hyperparameters other than the most important one help? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: Note: In Active Learning selecting next new sample from the pool using variance estimate (most unknown sample)"
      ],
      "metadata": {
        "id": "i3Bz7hwlqn0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQhKxhCxjzCP"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor = ExtraTreesRegressor(n_estimators=10, min_samples_split=2*2, min_samples_leaf=1*1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVM-85-hjzCR"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "#kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "#         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "\n",
        "regressor = ActiveLearner(\n",
        "    estimator=TreeBasedRegressor,\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training.reshape(-1, 1), y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cxSU9B6jzCS"
      },
      "outputs": [],
      "source": [
        "X_grid = np.linspace(0, 20, 1000)\n",
        "y_pred, y_std = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "y_pred, y_std = y_pred.ravel(), y_std.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWNNHArXjzCU"
      },
      "outputs": [],
      "source": [
        "n_queries = 11\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    y_pred_temp, y_std_temp = regressor.predict(X_grid.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp, y_std_temp = y_pred_temp.ravel(), y_std_temp.ravel()\n",
        "    y_pred_temp_aux, _ = regressor.predict(X.reshape(-1, 1), return_std=True)\n",
        "    y_pred_temp_aux = y_pred_temp_aux.ravel()\n",
        "    temp_X_train = regressor.X_training\n",
        "    temp_y_train = regressor.y_training\n",
        "    if idx > 0:\n",
        "      with plt.style.context('seaborn-white'):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.plot(X_grid_old, y_pred_temp_old)\n",
        "        plt.fill_between(X_grid_old, y_pred_temp_old - y_std_temp_old, y_pred_temp_old + y_std_temp_old, alpha=0.2)\n",
        "        plt.scatter(X, y, c='k', s=20)\n",
        "        #plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='r', s=100, marker=\"X\")\n",
        "        plt.scatter(temp_X_train, temp_y_train, c='green', s=100, marker=\"X\")\n",
        "        plt.scatter(X[query_idx], y_pred_temp_aux_old[query_idx], c='orange', s=200, marker=\"*\")\n",
        "        plt.title('Prediction after active learning' + \" - step \" + str(idx))\n",
        "        plt.show()\n",
        "    y_pred_temp_old = y_pred_temp\n",
        "    y_std_temp_old = y_std_temp\n",
        "    X_grid_old = X_grid\n",
        "    query_idx_old = query_idx\n",
        "    y_pred_temp_aux_old = y_pred_temp_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p7ZqRE-bwWt"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names).values\n",
        "y = cal_housing.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j05bzAKtbHQZ"
      },
      "outputs": [],
      "source": [
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "X_training, y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkHzuOomUYiu"
      },
      "source": [
        "#The not-so-correct methodology of AR evaluation presented in ModAL\n",
        "\n",
        "Based on https://modal-python.readthedocs.io/en/latest/content/examples/pool-based_sampling.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQDcIr_-U1ui"
      },
      "outputs": [],
      "source": [
        "learner = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmHOqMwjd1NO"
      },
      "source": [
        "Mind the score method in https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor.score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FG0-cUcVqBP"
      },
      "outputs": [],
      "source": [
        "X_raw = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
        "y_raw = cal_housing.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNb0-ymOgRb8"
      },
      "outputs": [],
      "source": [
        "X_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEY72jxsgW7b"
      },
      "outputs": [],
      "source": [
        "y_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32M-vzYcUPsI"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names).values\n",
        "y = cal_housing.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeiRPa9t1Lb2"
      },
      "outputs": [],
      "source": [
        "initial_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAKmbpC3d6N1"
      },
      "outputs": [],
      "source": [
        "X_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU8gpt4yecbB"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpdOSemrep2-"
      },
      "outputs": [],
      "source": [
        "X_training.reshape(-1, 1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyETj_Gkdy9I"
      },
      "outputs": [],
      "source": [
        "regressor = ActiveLearner(\n",
        "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "    query_strategy=GP_regression_std,\n",
        "    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KbrXuWUVxpk"
      },
      "outputs": [],
      "source": [
        "n_queries = 10\n",
        "for idx in range(n_queries):\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8847PwiRze"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faW0B-1Te3kh"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "#n_queries = 1000\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = ActiveLearner(estimator=GaussianProcessRegressor(kernel=kernel), query_strategy=GP_regression_std, X_training=X_train, y_training=y_train.reshape(-1, 1))\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRZ9PCToZYEz"
      },
      "source": [
        "###The order of samples in the raw/original dataset should be random, so one should be suspicious about any order of samples selected by the query strategy: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUxTlIzTBxPB"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekO3FfbHDxYQ"
      },
      "outputs": [],
      "source": [
        "8510 in unknown_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siPFhhpnHj_9"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQFYOEFJIhy0"
      },
      "outputs": [],
      "source": [
        "X_query.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXzW6dJeIsC3"
      },
      "outputs": [],
      "source": [
        "np.arange(len(X)) not in np.array(known_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsxE0dch_q3j"
      },
      "outputs": [],
      "source": [
        "set(known_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrVEXT-4k85L"
      },
      "outputs": [],
      "source": [
        "regressor.X_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlyaE7Dm-Tq1"
      },
      "outputs": [],
      "source": [
        "known_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrmmYiAAkvS-"
      },
      "outputs": [],
      "source": [
        "regressor.estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeZj53Q0r7tT"
      },
      "source": [
        "#Sanity check based on Scikit-Optimize - with additional randomization of the order of samples in the dataset ('just in case'). One should remain suspicious about any order of samples selected by the query strategy: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NER231tsEGb"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "#n_queries = 1000\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vrnsSNa1ceD"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rORJO7MURyTk"
      },
      "outputs": [],
      "source": [
        "cal_housing = fetch_california_housing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EQehNAkR77F"
      },
      "outputs": [],
      "source": [
        "cal_housing.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9P574wsTbI-"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOMhIlI2SLVS"
      },
      "outputs": [],
      "source": [
        "cal_housing_df = pd.concat([pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names), pd.DataFrame(cal_housing.target, columns=[\"target\"])], axis=1)\n",
        "cal_housing_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68yvj3ecwRbm"
      },
      "outputs": [],
      "source": [
        "print(cal_housing.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlA8n54EayQ4"
      },
      "source": [
        "### Quick exploratory data analysis (EDA) of the dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCiB9uDkUx5f"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DobAndTvV_E7"
      },
      "outputs": [],
      "source": [
        "#sns.pairplot(cal_housing_df, hue='target', palette=\"tab10\")\n",
        "sns.pairplot(cal_housing_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI0bQnfxQ521"
      },
      "outputs": [],
      "source": [
        "#cal_housing_df = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJbxYTuIRE1k"
      },
      "outputs": [],
      "source": [
        "cal_housing_df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niCwVm1mB9jC"
      },
      "outputs": [],
      "source": [
        "X = cal_housing_df[cal_housing.feature_names].values\n",
        "y = cal_housing_df.target.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8L-7rl_JJyp"
      },
      "outputs": [],
      "source": [
        "#X, _X, y, _y = train_test_split(X, y, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82NxVZGEJKLd"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "#n_queries = 1000\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = ActiveLearner(estimator=GaussianProcessRegressor(kernel=kernel), query_strategy=GP_regression_std, X_training=X_train, y_training=y_train.reshape(-1, 1))\n",
        "    query_idx, query_instance = regressor.query(X)\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtiSf0HyJKLh"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEGWoBrDJKLj"
      },
      "outputs": [],
      "source": [
        "8510 in unknown_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRaa0FmQJKLl"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVd4tU9yJKLn"
      },
      "outputs": [],
      "source": [
        "X_query.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EpbORKWJKLp"
      },
      "outputs": [],
      "source": [
        "np.arange(len(X)) not in np.array(known_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlSunWaLJKLr"
      },
      "outputs": [],
      "source": [
        "set(known_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mefki9QFJKLt"
      },
      "outputs": [],
      "source": [
        "regressor.X_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIGIH7HsJKLv"
      },
      "outputs": [],
      "source": [
        "known_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A92ms-rFJKLw"
      },
      "outputs": [],
      "source": [
        "regressor.estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cCJWcGpJKLy"
      },
      "source": [
        "#Sanity check based on Scikit-Optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LY3Mah_JKLz"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "#n_queries = 1000\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3BS77n9JKL2"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP1d8asm9txd"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1m_O_ED9uRO"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1.0,) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-2, 1e+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkOzt-bTp20F"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR1H2-mmp20H"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLZ-DQ3bp20I"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7Q1sj8C1Hlq"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1.0,) + WhiteKernel(noise_level=1, noise_level_bounds=(10, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TomvVDcP1Hls"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lfmjj8n1Hlv"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnTGnQmH1Hlw"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: Note: y-axis is ID of a sample, x-axis is iteration number of Active Learner."
      ],
      "metadata": {
        "id": "eAefZtqCwro1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMLXlwtD-Z8T"
      },
      "source": [
        "#Szwabe: Are the GPR kernel parameters really well \"optimized\"?\n",
        "\n",
        "150284: In the predicted standard deviation is depicted above, huge number of samples has the same std value, therefore, it is unacceptable behavior of regresion, because we are selecting samples to the pool based on its std value, but there's plenty of the same ones, in this case. Therefore, something is wrong with variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlkHs6Ri-93D"
      },
      "source": [
        "\"kernelkernel instance, default=None\n",
        "\n",
        "The kernel specifying the covariance function of the GP. \n",
        "        \n",
        "If None is passed, the kernel ConstantKernel(1.0, constant_value_bounds=\"fixed\" * RBF(1.0, length_scale_bounds=\"fixed\") is used as default. \n",
        "        \n",
        "**Note that the kernel hyperparameters are optimized during fitting unless the bounds are marked as “fixed”.**\"\n",
        "\n",
        "[https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVrXliuW-yf9"
      },
      "outputs": [],
      "source": [
        "kernel = RBF(length_scale=1.0,length_scale_bounds=\"fixed\") + WhiteKernel(noise_level=1, noise_level_bounds=\"fixed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwW8cgyg-yf_"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_GaussianProcessRegressor = []\n",
        "MAE_scores_of_GaussianProcessRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_GaussianProcessRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnTR7rdV-ygD"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEl_DVCd-ygE"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIWO-2tsSeF"
      },
      "source": [
        "#Szwabe: What do the above-seen results indicate?\n",
        "\n",
        "150284: Huge number of samples has the same std value => bad optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mjG0K-a31UB"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcI_kfRf2b66"
      },
      "outputs": [],
      "source": [
        "from skopt.learning import ExtraTreesRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gO17rYV1sli"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor = ExtraTreesRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbZLTb6o1s4l"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_TreeBasedRegressor = []\n",
        "MAE_scores_of_TreeBasedRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    #regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    regressor = TreeBasedRegressor()\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp_V-kgJ1s4o"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r71hpkN1s4q"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QbQx1HX0y2"
      },
      "source": [
        "#Szwabe: Can using a extra trees regressor (instead of GPR) help to diversify target variable predictions (i.e. to actually enable 'real AR')? To what extent? \n",
        "\n",
        "150284: Using ETR, the situation significantly changed, which means ETR can be used in such cases, instead of GPR. However, some fraction of samples still have same variance (0.0) (see plot above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lyyqfXXZQQq"
      },
      "source": [
        "#Szwabe: Can ETR hyperparameters tuning help to deal with the problem of some predictions equal to zero?\n",
        "\n",
        "150284: As it is stated above, ETR provides much better results, eliminating major number of same-valued samples (but not all of them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWB1ExfL4hFd"
      },
      "outputs": [],
      "source": [
        "TreeBasedRegressor = ExtraTreesRegressor(n_estimators=10, min_samples_split=2, min_samples_leaf=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d7kZHHF4hFf"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_TreeBasedRegressor = []\n",
        "MAE_scores_of_TreeBasedRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    #regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    #regressor = TreeBasedRegressor(n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
        "    regressor = TreeBasedRegressor\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls-_tm7r4hFh"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsO6g-hs4hFi"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv_3JnvxaICQ"
      },
      "source": [
        "#Szwabe: Which ETR hyperparameter requires especially careful tuning? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJgg4Fe-5U7L"
      },
      "outputs": [],
      "source": [
        "#TreeBasedRegressor = ExtraTreesRegressor(n_estimators=20, min_samples_split=2*2, min_samples_leaf=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZadiV1M5U7N"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_TreeBasedRegressor = []\n",
        "MAE_scores_of_TreeBasedRegressor = []\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    #regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    #regressor = TreeBasedRegressor(n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
        "    #regressor = TreeBasedRegressor\n",
        "    regressor = ExtraTreesRegressor(n_estimators=20, min_samples_split=2*2, min_samples_leaf=1)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    #temp_predictions = \n",
        "    #temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    #MSE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    #temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    #MAE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQcVtNw5U7Q"
      },
      "outputs": [],
      "source": [
        "plt.plot(known_idx[20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvTkprJq5U7R"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: Default hyperparameter values of ETR lead to the best results. As well as no monotonic relation between x and y in the first plot above \"disappeared\"."
      ],
      "metadata": {
        "id": "nKK2CHttuPZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ARZNVWg65kU"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn39E32lVhWT"
      },
      "source": [
        "#Szwabe: Can a piggybacked gradient boosting regressor enable to achieve a higher performance? Are these results deterministic?\n",
        "\n",
        "150284: It can be beneficial or even essential to be used in some cases. Results are not deterministic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EBX8IwN657L"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_TreeBasedRegressor = []\n",
        "MAE_scores_of_TreeBasedRegressor = []\n",
        "\n",
        "MSE_scores_of_piggybackedRegressor = []\n",
        "MAE_scores_of_piggybackedRegressor = []\n",
        "\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    #regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    regressor = ExtraTreesRegressor(n_estimators=20, min_samples_split=2*2, min_samples_leaf=1)\n",
        "    #regressor = TreeBasedRegressor\n",
        "    #piggybackedRegressor = MLPRegressor()\n",
        "    piggybackedRegressor = GradientBoostingRegressor(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_depth=4)\n",
        "    #piggybackedRegressor = LinearRegression()\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    temp_predictions = y_pred\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    piggybackedRegressor.fit(X_training, y_training)\n",
        "    temp_predictions = piggybackedRegressor.predict(X_query)\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_piggybackedRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_piggybackedRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fFNPsx3657O"
      },
      "outputs": [],
      "source": [
        "#plt.plot(known_idx[20:])\n",
        "plt.plot(known_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1piDZbej657P"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D50fj1Ap7OT1"
      },
      "outputs": [],
      "source": [
        "plt.plot(MSE_scores_of_TreeBasedRegressor, label=\"ARRegressor\")\n",
        "plt.plot(MSE_scores_of_piggybackedRegressor, label=\"piggybackedRegressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiulWEV37QLU"
      },
      "outputs": [],
      "source": [
        "plt.plot(MAE_scores_of_TreeBasedRegressor, label=\"ARRegressor\")\n",
        "plt.plot(MAE_scores_of_piggybackedRegressor, label=\"piggybackedRegressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: When evaluating random algorithms, repeat experiments several times."
      ],
      "metadata": {
        "id": "d2tPsREyvQ05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIoETzs0Vdgz"
      },
      "outputs": [],
      "source": [
        "#regressor = ActiveLearner(\n",
        "#    estimator=GaussianProcessRegressor(kernel=kernel),\n",
        "#    query_strategy=GP_regression_std,\n",
        "#    X_training=X_training, y_training=y_training.reshape(-1, 1)\n",
        "#)\n",
        "\n",
        "\n",
        "n_initial = 5\n",
        "initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "#initial_idx = np.array([18234, 3571, 18557, 13201, 12983])\n",
        "\n",
        "#old_X_training, old_y_training = X[initial_idx], y[initial_idx]\n",
        "\n",
        "MSE_scores_of_TreeBasedRegressor = []\n",
        "MAE_scores_of_TreeBasedRegressor = []\n",
        "\n",
        "MSE_scores_of_piggybackedRegressor = []\n",
        "MAE_scores_of_piggybackedRegressor = []\n",
        "\n",
        "\n",
        "known_idx = initial_idx.tolist()\n",
        "print(\"known_idx: \", known_idx)\n",
        "\n",
        "X_train = X[known_idx]\n",
        "y_train = y[known_idx]\n",
        "\n",
        "\n",
        "unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "X_query = X[list(unknown_idx)]\n",
        "y_query = y[list(unknown_idx)]\n",
        "\n",
        "n_queries = 100\n",
        "for idx in range(n_queries):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    #regressor = GaussianProcessRegressor(kernel=kernel)\n",
        "    regressor = ExtraTreesRegressor(n_estimators=20, min_samples_split=2*2, min_samples_leaf=1)\n",
        "    #regressor = TreeBasedRegressor\n",
        "    #piggybackedRegressor = MLPRegressor()\n",
        "    piggybackedRegressor = GradientBoostingRegressor(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_depth=4)\n",
        "    #piggybackedRegressor = LinearRegression()\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    #query_idx, query_instance = regressor.query(X)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "    query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    #print(\"query_instance: \", query_instance)\n",
        "\n",
        "    #regressor.teach(X[query_idx].reshape(1, -1), y[query_idx].reshape(1, -1))\n",
        "    temp_shape = X_train.shape\n",
        "    print(\"X_train_shape: \", temp_shape)\n",
        "    temp_shape = X_query.shape\n",
        "    print(\"X_query_shape: \", temp_shape)\n",
        "    #y_query = \n",
        "    temp_predictions = y_pred\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_TreeBasedRegressor.append(temp_score)\n",
        "    piggybackedRegressor.fit(X_training, y_training)\n",
        "    temp_predictions = piggybackedRegressor.predict(X_query)\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_piggybackedRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_piggybackedRegressor.append(temp_score)\n",
        "    print(\"known_idx: \", known_idx)\n",
        "    #X_query = X_query[np.arange(len(X)) not in known_idx]\n",
        "    #y_query = y_query[np.arange(len(y)) not in known_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2fFP2yAVdg0"
      },
      "outputs": [],
      "source": [
        "#plt.plot(known_idx[20:])\n",
        "plt.plot(known_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Y95d4fVdg2"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_pred_STD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ZFbWJ7Vdg3"
      },
      "outputs": [],
      "source": [
        "plt.plot(MSE_scores_of_TreeBasedRegressor, label=\"ARRegressor\")\n",
        "plt.plot(MSE_scores_of_piggybackedRegressor, label=\"piggybackedRegressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xstQdni1Vdg4"
      },
      "outputs": [],
      "source": [
        "plt.plot(MAE_scores_of_TreeBasedRegressor, label=\"ARRegressor\")\n",
        "plt.plot(MAE_scores_of_piggybackedRegressor, label=\"piggybackedRegressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgtvgdyHRwEG"
      },
      "outputs": [],
      "source": [
        "from random import sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9863yIRMHXCI"
      },
      "outputs": [],
      "source": [
        "query_idx = sample(list(range(X_query.shape[0])), 1)[0]\n",
        "query_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ndCmV7XzeM"
      },
      "outputs": [],
      "source": [
        "def test_comparing_with_baseline(annealing_factor, number_of_cycles):\n",
        "  #shuffled_indices = np.arange(y.shape[0])\n",
        "  #np.random.shuffle(shuffled_indices)\n",
        "  #y = y[shuffled_indices]\n",
        "  #X = X[shuffled_indices]\n",
        "\n",
        "  cal_housing_df.sample(frac=1).reset_index(drop=True)\n",
        "  X = cal_housing_df[cal_housing.feature_names].values\n",
        "  y = cal_housing_df.target.values\n",
        "\n",
        "  #committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n",
        "\n",
        "  #number_of_initial_samples = 2 * number_of_regressors_in_commitee #temporary\n",
        "  #initial_X_samples = X[:number_of_initial_samples]\n",
        "  #initial_y_samples = y[:number_of_initial_samples]\n",
        "  #X_query = X[number_of_initial_samples:]\n",
        "  #y_query = y[number_of_initial_samples:]\n",
        "  #for temp_initial_sample_idx in range(number_of_initial_samples):\n",
        "  #  X_train_sample = initial_X_samples[temp_initial_sample_idx]\n",
        "  #  y_train_sample = initial_y_samples[temp_initial_sample_idx]\n",
        "  #  committee.teach(X_train_sample, y_train_sample)\n",
        "\n",
        "  n_initial = 5\n",
        "  initial_idx = np.random.choice(range(len(X)), size=n_initial, replace=False)\n",
        "\n",
        "\n",
        "  #query_result = committee.query(X_query)\n",
        "  #MSE_scores_of_AlternativeCommitteeRegressor = []\n",
        "  #MAE_scores_of_AlternativeCommitteeRegressor = []\n",
        "\n",
        "  MSE_scores_of_ARRegressor = []\n",
        "  MAE_scores_of_ARRegressor = []\n",
        "\n",
        "  MSE_scores_of_piggybackedRegressor = []\n",
        "  MAE_scores_of_piggybackedRegressor = []\n",
        "\n",
        "  known_idx = initial_idx.tolist()\n",
        "  print(\"known_idx: \", known_idx)\n",
        "\n",
        "  X_train = X[known_idx]\n",
        "  y_train = y[known_idx]\n",
        "\n",
        "\n",
        "  unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "  print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "  X_query = X[list(unknown_idx)]\n",
        "  y_query = y[list(unknown_idx)]\n",
        "\n",
        "\n",
        "  temp_probas = []\n",
        "  for i in range(number_of_cycles):\n",
        "    #query_result = committee.query(X_query)\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = ExtraTreesRegressor(n_estimators=20, min_samples_split=2*2, min_samples_leaf=1)\n",
        "    piggybackedRegressor = GradientBoostingRegressor(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_depth=4)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    temp_prob = np.random.rand()\n",
        "    temp_prob = np.log(temp_prob)\n",
        "    temp_prob = temp_prob / (annealing_factor * i + 1)\n",
        "    temp_prob = int(round(temp_prob)) % 2\n",
        "    temp_probas.append(temp_prob)\n",
        "    print(\"temp_prob: \", temp_prob)\n",
        "    temp_probas.append(temp_prob)\n",
        "    if temp_prob > 0.5:\n",
        "        query_idx = sample(list(range(len(unknown_idx))), 1)[0]\n",
        "        print(\"this time it's random\")\n",
        "    else:\n",
        "        highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "        query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    temp_predictions = y_pred\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_ARRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_ARRegressor.append(temp_score)\n",
        "    piggybackedRegressor.fit(X_training, y_training)\n",
        "    temp_predictions = piggybackedRegressor.predict(X_query)\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_piggybackedRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_piggybackedRegressor.append(temp_score)\n",
        "\n",
        "\n",
        "  #initial_X_samples = X[:number_of_initial_samples]\n",
        "  #initial_y_samples = y[:number_of_initial_samples]\n",
        "  #X_query = X[number_of_initial_samples:]\n",
        "  #y_query = y[number_of_initial_samples:]\n",
        "  known_idx = initial_idx.tolist()\n",
        "  print(\"known_idx: \", known_idx)\n",
        "\n",
        "  X_train = X[known_idx]\n",
        "  y_train = y[known_idx]\n",
        "\n",
        "\n",
        "  unknown_idx = set(range(len(X))).difference(set(known_idx))\n",
        "  print(\"unknown_idx: \", unknown_idx)\n",
        "\n",
        "  X_query = X[list(unknown_idx)]\n",
        "  y_query = y[list(unknown_idx)]\n",
        "\n",
        "\n",
        "  temp_probas = []\n",
        "\n",
        "\n",
        "\n",
        "#  for temp_initial_sample_idx in range(number_of_initial_samples):\n",
        "#      X_train_sample = initial_X_samples[temp_initial_sample_idx]\n",
        "#      y_train_sample = initial_y_samples[temp_initial_sample_idx]\n",
        "#      committee.teach(X_train_sample, y_train_sample)\n",
        "#  query_result = committee.query(X_query)\n",
        "  MSE_scores_of_random_baseline = []\n",
        "  MAE_scores_of_random_baseline = []\n",
        "  MSE_scores_of_random_baseline_piggybackedRegressor = []\n",
        "  MAE_scores_of_random_baseline_piggybackedRegressor = []\n",
        "\n",
        "  for i in range(number_of_cycles):\n",
        "    X_train = X[known_idx]\n",
        "    y_train = y[known_idx]\n",
        "    regressor = ExtraTreesRegressor(n_estimators=20, min_samples_split=2*2, min_samples_leaf=1)\n",
        "    piggybackedRegressor = GradientBoostingRegressor(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_depth=4)\n",
        "    X_training = X_train\n",
        "    y_training = y_train.reshape(-1, 1)\n",
        "    regressor.fit(X_training, y_training)\n",
        "    X_query = X[list(unknown_idx)]\n",
        "    y_query = y[list(unknown_idx)]\n",
        "    y_pred, y_pred_STD = regressor.predict(X_query, return_std=True)\n",
        "    temp_prob = np.random.rand()\n",
        "    temp_prob = np.log(temp_prob)\n",
        "    temp_prob = temp_prob / (annealing_factor * i + 1)\n",
        "    temp_prob = int(round(temp_prob)) % 2\n",
        "    temp_probas.append(temp_prob)\n",
        "    print(\"temp_prob: \", temp_prob)\n",
        "    temp_probas.append(temp_prob)\n",
        "    #if temp_prob > 0.5:\n",
        "    if True:\n",
        "        query_idx = sample(list(range(len(unknown_idx))), 1)[0]\n",
        "        print(\"this time it's random\")\n",
        "    else:\n",
        "        highest_STD_unknown_idx = np.argmax(y_pred_STD)\n",
        "        query_idx = list(unknown_idx)[highest_STD_unknown_idx]\n",
        "    known_idx.append(query_idx)\n",
        "    temp_predictions = y_pred\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_random_baseline.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_random_baseline.append(temp_score)\n",
        "    piggybackedRegressor.fit(X_training, y_training)\n",
        "    temp_predictions = piggybackedRegressor.predict(X_query)\n",
        "    temp_score = mean_squared_error(y_query, temp_predictions)\n",
        "    MSE_scores_of_random_baseline_piggybackedRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_predictions)\n",
        "    MAE_scores_of_random_baseline_piggybackedRegressor.append(temp_score)\n",
        "\n",
        "  return MSE_scores_of_ARRegressor, MSE_scores_of_piggybackedRegressor, MSE_scores_of_random_baseline, MSE_scores_of_random_baseline_piggybackedRegressor, MAE_scores_of_ARRegressor, MAE_scores_of_piggybackedRegressor, MAE_scores_of_random_baseline, MAE_scores_of_random_baseline_piggybackedRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLsmgCZZkjTa"
      },
      "outputs": [],
      "source": [
        "#number_of_tests = 10\n",
        "annealing_factor = 0.03\n",
        "number_of_cycles = 200\n",
        "MSE_scores_of_ARRegressor, MSE_scores_of_piggybackedRegressor, MSE_scores_of_random_baseline, MSE_scores_of_random_baseline_piggybackedRegressor, MAE_scores_of_ARRegressor, MAE_scores_of_piggybackedRegressor, MAE_scores_of_random_baseline, MAE_scores_of_random_baseline_piggybackedRegressor = test_comparing_with_baseline(annealing_factor, number_of_cycles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH6sEJIFlOCi"
      },
      "outputs": [],
      "source": [
        "plt.plot(MSE_scores_of_ARRegressor, label=\"ARRegressor\")\n",
        "plt.plot(MSE_scores_of_piggybackedRegressor, label=\"piggybackedRegressor\")\n",
        "plt.plot(MSE_scores_of_random_baseline, label=\"random_baseline\")\n",
        "plt.plot(MSE_scores_of_random_baseline_piggybackedRegressor, label=\"random_baseline_piggybackedRegressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbr00wb-olbM"
      },
      "outputs": [],
      "source": [
        "plt.plot(MAE_scores_of_ARRegressor, label=\"ARRegressor\") \n",
        "plt.plot(MAE_scores_of_piggybackedRegressor, label=\"piggybackedRegressor\") \n",
        "plt.plot(MAE_scores_of_random_baseline, label=\"random_baseline\")\n",
        "plt.plot(MAE_scores_of_random_baseline_piggybackedRegressor, label=\"random_baseline_piggybackedRegressor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2wMHWa1cnGv"
      },
      "source": [
        "#Szwabe: How many test repetitions ar needed to observe any significant value of any AR algorithm based on a non-random query strategy? How does this value depend on the sample target variable attribution/assignment budget (the number of AR cycles)?  \n",
        "\n",
        "150284: In our case, the changing parameters does not really influence the results. It can be seen on the plots that at some points random sampling performs better, but still it is not that much reliable. As for the dependency between test repetitions and AR cycles, unfortunately, it also was not that noticable, at least during the class time, since it is better to run the experiments many times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vFmfmRkpX4e"
      },
      "outputs": [],
      "source": [
        "def multiple_tests_comparing_with_baseline(number_of_tests, annealing_factor, number_of_cycles):\n",
        "  means_and_stds = {\"MAE\": {}, \"MSE\": {}}\n",
        "  MSE_scores_of_ARRegressor_list = []\n",
        "  MSE_scores_of_piggybackedRegressor_list = []\n",
        "  MSE_scores_of_random_baseline_list = []\n",
        "  MAE_scores_of_ARRegressor_list = []\n",
        "  MAE_scores_of_piggybackedRegressor_list = []\n",
        "  MAE_scores_of_random_baseline_list = []\n",
        "  for test_number in range(number_of_tests):\n",
        "    #MSE_scores_of_ARRegressor, MSE_scores_of_random_baseline, MAE_scores_of_ARRegressor, MAE_scores_of_random_baseline = test_comparing_with_baseline(annealing_factor, number_of_cycles)\n",
        "    MSE_scores_of_ARRegressor, MSE_scores_of_piggybackedRegressor, MSE_scores_of_random_baseline, MSE_scores_of_random_baseline_piggybackedRegressor, MAE_scores_of_ARRegressor, MAE_scores_of_piggybackedRegressor, MAE_scores_of_random_baseline, MAE_scores_of_random_baseline_piggybackedRegressor = test_comparing_with_baseline(annealing_factor, number_of_cycles)\n",
        "    MSE_scores_of_ARRegressor_list.append(MSE_scores_of_ARRegressor)\n",
        "    MSE_scores_of_piggybackedRegressor_list.append(MSE_scores_of_piggybackedRegressor)\n",
        "    MSE_scores_of_random_baseline_list.append(MSE_scores_of_random_baseline)\n",
        "    MAE_scores_of_ARRegressor_list.append(MAE_scores_of_ARRegressor)\n",
        "    MAE_scores_of_piggybackedRegressor_list.append(MAE_scores_of_piggybackedRegressor)\n",
        "    MAE_scores_of_random_baseline_list.append(MAE_scores_of_random_baseline)\n",
        "  temp_mean = np.mean(np.array(MSE_scores_of_ARRegressor_list), axis=0)\n",
        "  temp_std = np.std(np.array(MSE_scores_of_ARRegressor_list), axis=0)\n",
        "  means_and_stds[\"MSE\"][\"ARRegressor\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MSE_scores_of_piggybackedRegressor_list), axis=0)\n",
        "  temp_std = np.std(np.array(MSE_scores_of_piggybackedRegressor_list), axis=0)\n",
        "  means_and_stds[\"MSE\"][\"piggybackedRegressor\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MAE_scores_of_ARRegressor_list), axis=0)\n",
        "  temp_std = np.std(np.array(MAE_scores_of_ARRegressor_list), axis=0)\n",
        "  means_and_stds[\"MAE\"][\"ARRegressor\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MAE_scores_of_piggybackedRegressor_list), axis=0)\n",
        "  temp_std = np.std(np.array(MAE_scores_of_piggybackedRegressor_list), axis=0)\n",
        "  means_and_stds[\"MAE\"][\"piggybackedRegressor\"] = {\"mean\": temp_mean, \"std\": temp_std}  \n",
        "  temp_mean = np.mean(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  temp_std = np.std(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  means_and_stds[\"MSE\"][\"random_baseline\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MAE_scores_of_random_baseline_list), axis=0)\n",
        "  temp_std = np.std(np.array(MAE_scores_of_random_baseline_list), axis=0)\n",
        "  means_and_stds[\"MAE\"][\"random_baseline\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  MSE_scores_of_random_baseline_mean = np.mean(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  MSE_scores_of_random_baseline_std = np.std(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  return means_and_stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWS3IC2KtNt_"
      },
      "outputs": [],
      "source": [
        "def plot_means_and_stds(means_and_stds, temp_measure):\n",
        "  plt.clf\n",
        "  plt.rcParams[\"figure.figsize\"] = (20,14)\n",
        "  temp_color_letters = [\"b\", \"y\", \"g\"]\n",
        "  #temp_measure = \"MAE\"\n",
        "  temp_measure_data = means_and_stds[temp_measure]\n",
        "  temp_algorithms_labes = list(temp_measure_data)\n",
        "  for temp_algorithm_label_idx, temp_algorithm_label in enumerate(temp_algorithms_labes):\n",
        "    temp_color_letter = temp_color_letters[temp_algorithm_label_idx]\n",
        "    temp_mean = means_and_stds[temp_measure][temp_algorithm_label][\"mean\"]\n",
        "    temp_std = means_and_stds[temp_measure][temp_algorithm_label][\"std\"]\n",
        "    x = np.arange(len(temp_mean))\n",
        "    plt.plot(x, temp_mean, temp_color_letter+\"-\", label=temp_algorithm_label)\n",
        "    plt.fill_between(x, temp_mean - temp_std, temp_mean + temp_std, color=temp_color_letter, alpha=0.2)\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vYl-2_CatUpk"
      },
      "outputs": [],
      "source": [
        "#number_of_tests = 20\n",
        "number_of_tests = 10\n",
        "annealing_factor = 0.03\n",
        "number_of_cycles = 200\n",
        "\n",
        "means_and_stds = multiple_tests_comparing_with_baseline(number_of_tests, annealing_factor, number_of_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hzM5UJa4ylRz"
      },
      "outputs": [],
      "source": [
        "temp_measure = \"MSE\"\n",
        "plot_means_and_stds(means_and_stds, temp_measure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zHvqcoQ_t4-K"
      },
      "outputs": [],
      "source": [
        "temp_measure = \"MAE\"\n",
        "plot_means_and_stds(means_and_stds, temp_measure)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsX6V_6xfOAr"
      },
      "source": [
        "#Szwabe: Set an appropriate number of AR cycles and test repetitions (below) in order to compare practical value of the three sampling strategies in the most reliable way that is still feasible considering the amount of available time (of the classwork). Compare and comment on the practical value of the strategies.\n",
        "\n",
        "150284: Variance overlay each other. Even after increasing number of AR cycles and test repetitions (e.g., number_of_tests = 200), we are still not sure about reliability of the results, due to lack of time. However, random sampling seems to work the best in that case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "61AEUBCP6TQm"
      },
      "outputs": [],
      "source": [
        "number_of_tests = 20\n",
        "annealing_factor = 0.0\n",
        "number_of_cycles = 200\n",
        "\n",
        "means_and_stds = multiple_tests_comparing_with_baseline(number_of_tests, annealing_factor, number_of_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mip7k5VD69F5"
      },
      "outputs": [],
      "source": [
        "temp_measure = \"MSE\"\n",
        "plot_means_and_stds(means_and_stds, temp_measure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "01uLOAW_69F6"
      },
      "outputs": [],
      "source": [
        "temp_measure = \"MAE\"\n",
        "plot_means_and_stds(means_and_stds, temp_measure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Ui0GUyPj6Y"
      },
      "outputs": [],
      "source": [
        "number_of_tests = 20\n",
        "annealing_factor = 1.0\n",
        "number_of_cycles = 200\n",
        "\n",
        "means_and_stds = multiple_tests_comparing_with_baseline(number_of_tests, annealing_factor, number_of_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fAWZxbP8Pj6d"
      },
      "outputs": [],
      "source": [
        "temp_measure = \"MSE\"\n",
        "plot_means_and_stds(means_and_stds, temp_measure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kq7am80VPj6f"
      },
      "outputs": [],
      "source": [
        "temp_measure = \"MAE\"\n",
        "plot_means_and_stds(means_and_stds, temp_measure)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "150284: It can be noticed on the plot above, that most of the time totally random sample selection works better than using regression."
      ],
      "metadata": {
        "id": "QFZzXE0-vsAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q24I1kZ3PZn1"
      },
      "outputs": [],
      "source": [
        "!pip freeze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xrFuC8cuBVg"
      },
      "source": [
        "absl-py==1.0.0\n",
        "alabaster==0.7.12\n",
        "albumentations==0.1.12\n",
        "altair==4.2.0\n",
        "appdirs==1.4.4\n",
        "argon2-cffi==21.3.0\n",
        "argon2-cffi-bindings==21.2.0\n",
        "arviz==0.11.4\n",
        "astor==0.8.1\n",
        "astropy==4.3.1\n",
        "astunparse==1.6.3\n",
        "atari-py==0.2.9\n",
        "atomicwrites==1.4.0\n",
        "attrs==21.4.0\n",
        "audioread==2.1.9\n",
        "autograd==1.3\n",
        "Babel==2.9.1\n",
        "backcall==0.2.0\n",
        "beautifulsoup4==4.6.3\n",
        "bleach==4.1.0\n",
        "blis==0.4.1\n",
        "bokeh==2.3.3\n",
        "Bottleneck==1.3.4\n",
        "branca==0.4.2\n",
        "bs4==0.0.1\n",
        "CacheControl==0.12.10\n",
        "cached-property==1.5.2\n",
        "cachetools==4.2.4\n",
        "catalogue==1.0.0\n",
        "certifi==2021.10.8\n",
        "cffi==1.15.0\n",
        "cftime==1.5.2\n",
        "chardet==3.0.4\n",
        "charset-normalizer==2.0.12\n",
        "click==7.1.2\n",
        "cloudpickle==1.3.0\n",
        "cmake==3.12.0\n",
        "cmdstanpy==0.9.5\n",
        "colorcet==3.0.0\n",
        "colorlover==0.3.0\n",
        "community==1.0.0b1\n",
        "contextlib2==0.5.5\n",
        "convertdate==2.4.0\n",
        "coverage==3.7.1\n",
        "coveralls==0.5\n",
        "crcmod==1.7\n",
        "cufflinks==0.17.3\n",
        "cvxopt==1.2.7\n",
        "cvxpy==1.0.31\n",
        "cycler==0.11.0\n",
        "cymem==2.0.6\n",
        "Cython==0.29.28\n",
        "daft==0.0.4\n",
        "dask==2.12.0\n",
        "datascience==0.10.6\n",
        "debugpy==1.0.0\n",
        "decorator==4.4.2\n",
        "defusedxml==0.7.1\n",
        "descartes==1.1.0\n",
        "dill==0.3.4\n",
        "distributed==1.25.3\n",
        "dlib @ file:///dlib-19.18.0-cp37-cp37m-linux_x86_64.whl\n",
        "dm-tree==0.1.6\n",
        "docopt==0.6.2\n",
        "docutils==0.17.1\n",
        "dopamine-rl==1.0.5\n",
        "earthengine-api==0.1.300\n",
        "easydict==1.9\n",
        "ecos==2.0.10\n",
        "editdistance==0.5.3\n",
        "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
        "entrypoints==0.4\n",
        "ephem==4.1.3\n",
        "et-xmlfile==1.1.0\n",
        "fa2==0.3.5\n",
        "fastai==1.0.61\n",
        "fastdtw==0.3.4\n",
        "fastprogress==1.0.2\n",
        "fastrlock==0.8\n",
        "fbprophet==0.7.1\n",
        "feather-format==0.4.1\n",
        "filelock==3.6.0\n",
        "firebase-admin==4.4.0\n",
        "fix-yahoo-finance==0.0.22\n",
        "Flask==1.1.4\n",
        "flatbuffers==2.0\n",
        "folium==0.8.3\n",
        "future==0.16.0\n",
        "gast==0.5.3\n",
        "GDAL==2.2.2\n",
        "gdown==4.2.2\n",
        "gensim==3.6.0\n",
        "geographiclib==1.52\n",
        "geopy==1.17.0\n",
        "gin-config==0.5.0\n",
        "glob2==0.7\n",
        "google==2.0.3\n",
        "google-api-core==1.26.3\n",
        "google-api-python-client==1.12.10\n",
        "google-auth==1.35.0\n",
        "google-auth-httplib2==0.0.4\n",
        "google-auth-oauthlib==0.4.6\n",
        "google-cloud-bigquery==1.21.0\n",
        "google-cloud-bigquery-storage==1.1.0\n",
        "google-cloud-core==1.0.3\n",
        "google-cloud-datastore==1.8.0\n",
        "google-cloud-firestore==1.7.0\n",
        "google-cloud-language==1.2.0\n",
        "google-cloud-storage==1.18.1\n",
        "google-cloud-translate==1.5.0\n",
        "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz\n",
        "google-pasta==0.2.0\n",
        "google-resumable-media==0.4.1\n",
        "googleapis-common-protos==1.55.0\n",
        "googledrivedownloader==0.4\n",
        "graphviz==0.10.1\n",
        "greenlet==1.1.2\n",
        "grpcio==1.44.0\n",
        "gspread==3.4.2\n",
        "gspread-dataframe==3.0.8\n",
        "gym==0.17.3\n",
        "h5py==3.1.0\n",
        "HeapDict==1.0.1\n",
        "hijri-converter==2.2.3\n",
        "holidays==0.10.5.2\n",
        "holoviews==1.14.8\n",
        "html5lib==1.0.1\n",
        "httpimport==0.5.18\n",
        "httplib2==0.17.4\n",
        "httplib2shim==0.0.3\n",
        "humanize==0.5.1\n",
        "hyperopt==0.1.2\n",
        "ideep4py==2.0.0.post3\n",
        "idna==2.10\n",
        "imageio==2.4.1\n",
        "imagesize==1.3.0\n",
        "imbalanced-learn==0.8.1\n",
        "imblearn==0.0\n",
        "imgaug==0.2.9\n",
        "importlib-metadata==4.11.2\n",
        "importlib-resources==5.4.0\n",
        "imutils==0.5.4\n",
        "inflect==2.1.0\n",
        "iniconfig==1.1.1\n",
        "intel-openmp==2022.0.2\n",
        "intervaltree==2.1.0\n",
        "ipykernel==4.10.1\n",
        "ipython==5.5.0\n",
        "ipython-genutils==0.2.0\n",
        "ipython-sql==0.3.9\n",
        "ipywidgets==7.6.5\n",
        "itsdangerous==1.1.0\n",
        "jax==0.3.1\n",
        "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.0+cuda11.cudnn805-cp37-none-manylinux2010_x86_64.whl\n",
        "jedi==0.18.1\n",
        "jieba==0.42.1\n",
        "Jinja2==2.11.3\n",
        "joblib==1.1.0\n",
        "jpeg4py==0.1.4\n",
        "jsonschema==4.3.3\n",
        "jupyter==1.0.0\n",
        "jupyter-client==5.3.5\n",
        "jupyter-console==5.2.0\n",
        "jupyter-core==4.9.2\n",
        "jupyterlab-pygments==0.1.2\n",
        "jupyterlab-widgets==1.0.2\n",
        "kaggle==1.5.12\n",
        "kapre==0.3.7\n",
        "keras==2.8.0\n",
        "Keras-Preprocessing==1.1.2\n",
        "keras-vis==0.4.1\n",
        "kiwisolver==1.3.2\n",
        "korean-lunar-calendar==0.2.1\n",
        "libclang==13.0.0\n",
        "librosa==0.8.1\n",
        "lightgbm==2.2.3\n",
        "llvmlite==0.34.0\n",
        "lmdb==0.99\n",
        "LunarCalendar==0.0.9\n",
        "lxml==4.2.6\n",
        "Markdown==3.3.6\n",
        "MarkupSafe==2.0.1\n",
        "matplotlib==3.2.2\n",
        "matplotlib-inline==0.1.3\n",
        "matplotlib-venn==0.11.6\n",
        "missingno==0.5.1\n",
        "mistune==0.8.4\n",
        "mizani==0.6.0\n",
        "mkl==2019.0\n",
        "mlxtend==0.14.0\n",
        "modAL==0.4.1\n",
        "more-itertools==8.12.0\n",
        "moviepy==0.2.3.5\n",
        "mpmath==1.2.1\n",
        "msgpack==1.0.3\n",
        "multiprocess==0.70.12.2\n",
        "multitasking==0.0.10\n",
        "murmurhash==1.0.6\n",
        "music21==5.5.0\n",
        "natsort==5.5.0\n",
        "nbclient==0.5.11\n",
        "nbconvert==5.6.1\n",
        "nbformat==5.1.3\n",
        "nest-asyncio==1.5.4\n",
        "netCDF4==1.5.8\n",
        "networkx==2.6.3\n",
        "nibabel==3.0.2\n",
        "nltk==3.2.5\n",
        "notebook==5.3.1\n",
        "numba==0.51.2\n",
        "numexpr==2.8.1\n",
        "numpy==1.21.5\n",
        "nvidia-ml-py3==7.352.0\n",
        "oauth2client==4.1.3\n",
        "oauthlib==3.2.0\n",
        "okgrade==0.4.3\n",
        "opencv-contrib-python==4.1.2.30\n",
        "opencv-python==4.1.2.30\n",
        "openpyxl==3.0.9\n",
        "opt-einsum==3.3.0\n",
        "osqp==0.6.2.post0\n",
        "packaging==21.3\n",
        "palettable==3.3.0\n",
        "pandas==1.3.5\n",
        "pandas-datareader==0.9.0\n",
        "pandas-gbq==0.13.3\n",
        "pandas-profiling==1.4.1\n",
        "pandocfilters==1.5.0\n",
        "panel==0.12.1\n",
        "param==1.12.0\n",
        "parso==0.8.3\n",
        "pathlib==1.0.1\n",
        "patsy==0.5.2\n",
        "pep517==0.12.0\n",
        "pexpect==4.8.0\n",
        "pickleshare==0.7.5\n",
        "Pillow==7.1.2\n",
        "pip-tools==6.2.0\n",
        "plac==1.1.3\n",
        "plotly==5.5.0\n",
        "plotnine==0.6.0\n",
        "pluggy==0.7.1\n",
        "pooch==1.6.0\n",
        "portpicker==1.3.9\n",
        "prefetch-generator==1.0.1\n",
        "preshed==3.0.6\n",
        "prettytable==3.1.1\n",
        "progressbar2==3.38.0\n",
        "prometheus-client==0.13.1\n",
        "promise==2.3\n",
        "prompt-toolkit==1.0.18\n",
        "protobuf==3.17.3\n",
        "psutil==5.4.8\n",
        "psycopg2==2.7.6.1\n",
        "ptyprocess==0.7.0\n",
        "py==1.11.0\n",
        "pyaml==21.10.1\n",
        "pyarrow==6.0.1\n",
        "pyasn1==0.4.8\n",
        "pyasn1-modules==0.2.8\n",
        "pycocotools==2.0.4\n",
        "pycparser==2.21\n",
        "pyct==0.4.8\n",
        "pydata-google-auth==1.3.0\n",
        "pydot==1.3.0\n",
        "pydot-ng==2.0.0\n",
        "pydotplus==2.0.2\n",
        "PyDrive==1.3.1\n",
        "pyemd==0.5.1\n",
        "pyerfa==2.0.0.1\n",
        "pyglet==1.5.0\n",
        "Pygments==2.6.1\n",
        "pygobject==3.26.1\n",
        "pymc3==3.11.4\n",
        "PyMeeus==0.5.11\n",
        "pymongo==4.0.1\n",
        "pymystem3==0.2.0\n",
        "PyOpenGL==3.1.6\n",
        "pyparsing==3.0.7\n",
        "pyrsistent==0.18.1\n",
        "pysndfile==1.3.8\n",
        "PySocks==1.7.1\n",
        "pystan==2.19.1.1\n",
        "pytest==3.6.4\n",
        "python-apt==0.0.0\n",
        "python-chess==0.23.11\n",
        "python-dateutil==2.8.2\n",
        "python-louvain==0.16\n",
        "python-slugify==6.1.1\n",
        "python-utils==3.1.0\n",
        "pytz==2018.9\n",
        "pyviz-comms==2.1.0\n",
        "PyWavelets==1.2.0\n",
        "PyYAML==3.13\n",
        "pyzmq==22.3.0\n",
        "qdldl==0.1.5.post0\n",
        "qtconsole==5.2.2\n",
        "QtPy==2.0.1\n",
        "regex==2019.12.20\n",
        "requests==2.23.0\n",
        "requests-oauthlib==1.3.1\n",
        "resampy==0.2.2\n",
        "rpy2==3.4.5\n",
        "rsa==4.8\n",
        "scikit-image==0.18.3\n",
        "scikit-learn==1.0.2\n",
        "scikit-optimize==0.9.0\n",
        "scipy==1.4.1\n",
        "screen-resolution-extra==0.0.0\n",
        "scs==3.2.0\n",
        "seaborn==0.11.2\n",
        "semver==2.13.0\n",
        "Send2Trash==1.8.0\n",
        "setuptools-git==1.2\n",
        "Shapely==1.8.1.post1\n",
        "simplegeneric==0.8.1\n",
        "six==1.15.0\n",
        "sklearn==0.0\n",
        "sklearn-pandas==1.8.0\n",
        "smart-open==5.2.1\n",
        "snowballstemmer==2.2.0\n",
        "sortedcontainers==2.4.0\n",
        "SoundFile==0.10.3.post1\n",
        "spacy==2.2.4\n",
        "Sphinx==1.8.6\n",
        "sphinxcontrib-serializinghtml==1.1.5\n",
        "sphinxcontrib-websupport==1.2.4\n",
        "SQLAlchemy==1.4.31\n",
        "sqlparse==0.4.2\n",
        "srsly==1.0.5\n",
        "statsmodels==0.10.2\n",
        "sympy==1.7.1\n",
        "tables==3.7.0\n",
        "tabulate==0.8.9\n",
        "tblib==1.7.0\n",
        "tenacity==8.0.1\n",
        "tensorboard==2.8.0\n",
        "tensorboard-data-server==0.6.1\n",
        "tensorboard-plugin-wit==1.8.1\n",
        "tensorflow @ file:///tensorflow-2.8.0-cp37-cp37m-linux_x86_64.whl\n",
        "tensorflow-datasets==4.0.1\n",
        "tensorflow-estimator==2.8.0\n",
        "tensorflow-gcs-config==2.8.0\n",
        "tensorflow-hub==0.12.0\n",
        "tensorflow-io-gcs-filesystem==0.24.0\n",
        "tensorflow-metadata==1.7.0\n",
        "tensorflow-probability==0.16.0\n",
        "termcolor==1.1.0\n",
        "terminado==0.13.1\n",
        "testpath==0.6.0\n",
        "text-unidecode==1.3\n",
        "textblob==0.15.3\n",
        "Theano-PyMC==1.1.2\n",
        "thinc==7.4.0\n",
        "threadpoolctl==3.1.0\n",
        "tifffile==2021.11.2\n",
        "tomli==2.0.1\n",
        "toolz==0.11.2\n",
        "torch @ https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "torchaudio @ https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "torchsummary==1.5.1\n",
        "torchtext==0.11.0\n",
        "torchvision @ https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "tornado==5.1.1\n",
        "tqdm==4.63.0\n",
        "traitlets==5.1.1\n",
        "tweepy==3.10.0\n",
        "typeguard==2.7.1\n",
        "typing-extensions==3.10.0.2\n",
        "tzlocal==1.5.1\n",
        "uritemplate==3.0.1\n",
        "urllib3==1.24.3\n",
        "vega-datasets==0.9.0\n",
        "wasabi==0.9.0\n",
        "wcwidth==0.2.5\n",
        "webencodings==0.5.1\n",
        "Werkzeug==1.0.1\n",
        "widgetsnbextension==3.5.2\n",
        "wordcloud==1.5.0\n",
        "wrapt==1.13.3\n",
        "xarray==0.18.2\n",
        "xgboost==0.90\n",
        "xkit==0.0.0\n",
        "xlrd==1.1.0\n",
        "xlwt==1.3.0\n",
        "yellowbrick==1.4\n",
        "zict==2.1.0\n",
        "zipp==3.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLonYh4QPbze"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}