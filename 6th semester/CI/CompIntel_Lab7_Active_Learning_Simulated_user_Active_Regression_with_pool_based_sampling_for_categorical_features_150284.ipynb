{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dppOI4yU45v_"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install sklearn\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed98Zw9s5ECk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKwpypervcye"
      },
      "outputs": [],
      "source": [
        "##\"https://docs.google.com/spreadsheets/d/1_PNPgD9gfckqJDMPH7Rcn8wyY5zHrQKE9I1KoLrZbZE/edit?usp=sharing\"\n",
        "#sheet_id = \"1_PNPgD9gfckqJDMPH7Rcn8wyY5zHrQKE9I1KoLrZbZE\"\n",
        "#sheet_name = \"CriteoSearchData_partner_C0F515F0A2D0A5D9F854008BA76EB537\"\n",
        "#url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0dH_W88vcyn"
      },
      "outputs": [],
      "source": [
        "#https://docs.google.com/spreadsheets/d/1OKjhawrM8ZAmmpN18yOI0SRhlrhBdl667E30mm9_xb8/edit?usp=sharing\n",
        "sheet_id = \"1OKjhawrM8ZAmmpN18yOI0SRhlrhBdl667E30mm9_xb8\"\n",
        "sheet_name = \"CriteoSearchData_partner_C0F515F0A2D0A5D9F854008BA76EB537_new\"\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRplWbQCz9W1"
      },
      "outputs": [],
      "source": [
        "pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY8KZl177YvR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgLlt0blt4pe"
      },
      "outputs": [],
      "source": [
        "def read_CPSD_per_partner_data_group_CSV_file(partner_ID, nrows=None):\n",
        "\n",
        "    raw_Header_Information = \"<Sale> ,<SalesAmountInEuro> ,<time_delay_for_conversion>, <click_timestamp> ,<nb_clicks_1week> ,<product_price> ,<product_age_group> ,<device_type>,<audience_id> ,<product_gender> ,<product_brand> ,<product_category(1-7)> ,<product_country>, <product_id> ,<product_title> ,<partner_id> ,<user_id>\"\n",
        "    product_category_list = []\n",
        "    for product_category_index in range(7):\n",
        "        temp_str = \"<product_category_\" + str(product_category_index + 1) + \">\"\n",
        "        product_category_list.append(temp_str)\n",
        "    raw_Header_Information = raw_Header_Information.replace(\" \", \"\")\n",
        "    raw_Header_Information = raw_Header_Information.replace(\" \", \"\")\n",
        "    raw_Header_Information = raw_Header_Information.replace(\"<\", \"\")\n",
        "    raw_Header_Information = raw_Header_Information.replace(\">\", \"\")\n",
        "    Header_Information = raw_Header_Information.split(\",\")\n",
        "    header_info = Header_Information[:11] + product_category_list + Header_Information[12:]\n",
        "\n",
        "    dtypes = {'Sale': 'int64', 'SalesAmountInEuro': 'string', 'time_delay_for_conversion': 'string',\n",
        "                   'click_timestamp': 'int64', 'nb_clicks_1week': 'string', 'product_price': 'string',\n",
        "                   'product_age_group': 'string', 'device_type': 'string', 'audience_id': 'string',\n",
        "                   'product_gender': 'string', 'product_brand': 'string', '<product_category_1>': 'string',\n",
        "                   '<product_category_2>': 'string', '<product_category_3>': 'string', '<product_category_4>': 'string',\n",
        "                   '<product_category_5>': 'string', '<product_category_6>': 'string', '<product_category_7>': 'string',\n",
        "                   'product_country': 'string', 'product_id': 'string', 'product_title': 'string',\n",
        "                   'partner_id': 'string', 'user_id': 'string'}\n",
        "\n",
        "    #temp_file_path = \"CPSD_per-partner_data_groups\"\n",
        "    temp_file_path = \"\"\n",
        "    temp_file_path += \"CriteoSearchData_partner_\" + partner_ID + \"_new.csv\"\n",
        "    #CPSD_per_partner_data_df = pd.read_csv(temp_file_path, sep=\"\\t\", nrows=nrows, header=0, names=header_info, dtype=dtypes)\n",
        "    CPSD_per_partner_data_df = pd.read_csv(url, nrows=nrows, dtype=dtypes)\n",
        "    #CPSD_per_partner_data_df = pd.read_csv(url, nrows=nrows)\n",
        "    return CPSD_per_partner_data_df\n",
        "\n",
        "def make_product_df(partner_ID=\"C0F515F0A2D0A5D9F854008BA76EB537\"):\n",
        "    CPSD_per_partner_data_df = read_CPSD_per_partner_data_group_CSV_file(partner_ID)\n",
        "    #CPSD_per_partner_data_df['product_price'] = CPSD_per_partner_data_df['product_price'].fillna(0.0)\n",
        "    useful_rows_mask = CPSD_per_partner_data_df[\"product_price\"] != \"0.0\"\n",
        "    temp_product_df = CPSD_per_partner_data_df[useful_rows_mask]\n",
        "    print(\"CPSD_per_partner_data_df.shape: \", CPSD_per_partner_data_df.shape)\n",
        "    print(\"temp_product_df.shape: \", temp_product_df.shape)\n",
        "    #temp_product_df[\"product_price\"] = temp_product_df[\"product_price\"].astype(\"float\")\n",
        "    temp_product_df[\"product_price\"] = pd.to_numeric(temp_product_df[\"product_price\"], errors='coerce')\n",
        "    useful_rows_mask = temp_product_df[\"product_price\"].isna()\n",
        "    temp_product_df = temp_product_df[~useful_rows_mask]\n",
        "    print(\"temp_product_df.shape: \", temp_product_df.shape)\n",
        "    all_raw_columns = list(CPSD_per_partner_data_df)\n",
        "    print(\"all_raw_columns: \", all_raw_columns)\n",
        "    useful_columns = [\"product_price\", \"product_age_group\", \"product_gender\", \"product_brand\", \"product_id\"]\n",
        "    # useful_columns_prefixes = [\"<product_category_\"]\n",
        "    additional_useful_columns = []\n",
        "    for raw_column in all_raw_columns:\n",
        "        if raw_column.find(\"<product_category_\") != -1:\n",
        "            if raw_column not in [\"<product_category_6\", \"<product_category_7\"]:\n",
        "              additional_useful_columns.append(raw_column)\n",
        "    useful_columns += additional_useful_columns\n",
        "    print(\"useful_columns: \", useful_columns)\n",
        "    product_title_df = temp_product_df[\"product_title\"].str.split(\" \")\n",
        "    product_title_expl_df = product_title_df.apply(pd.Series)\n",
        "    temp_col_rename_dict = {}\n",
        "    product_title_features = []\n",
        "    for df_expl_col_name in list(product_title_expl_df):\n",
        "        temp_new_product_feature_name = \"product_title_part_\" + str(df_expl_col_name + 1)\n",
        "        temp_col_rename_dict[df_expl_col_name] = temp_new_product_feature_name\n",
        "        # product_features.append(temp_new_product_feature_name)\n",
        "        product_title_features.append(temp_new_product_feature_name)\n",
        "    product_title_expl_df.rename(columns=temp_col_rename_dict, inplace=True)\n",
        "    temp_product_df.drop(columns=[\"product_title\"], inplace=True)\n",
        "    print(\"about to reindex...\")\n",
        "    product_df = pd.concat([temp_product_df, product_title_expl_df.reindex(product_title_expl_df.index)], axis=1)\n",
        "    print(list(product_df))\n",
        "    print(\"useful_columns: \", useful_columns)\n",
        "    product_df = product_df[useful_columns]\n",
        "    return product_df\n",
        "\n",
        "def make_X_and_y(partner_ID=\"C0F515F0A2D0A5D9F854008BA76EB537\"):\n",
        "    product_df = make_product_df(partner_ID)\n",
        "    list_of_column_names = list(product_df)\n",
        "    list_of_feature_names = list(set(list_of_column_names).difference(set([\"product_price\"])))\n",
        "    X_df = product_df[list_of_feature_names]\n",
        "    y_series = product_df[\"product_price\"]\n",
        "    X_df = X_df.astype(\"str\")\n",
        "    #X_array = np.array(X_df)\n",
        "    X_array = X_df.to_numpy()\n",
        "    X_array = np.nan_to_num(X_array)\n",
        "    OH_encoder = OneHotEncoder(sparse=False)\n",
        "    OH_encoder.fit(X_array)\n",
        "    OH_encoded_X_dense_matrix = OH_encoder.transform(X_array)\n",
        "    y_array = np.array(y_series)\n",
        "    return OH_encoded_X_dense_matrix, y_array\n",
        "\n",
        "\n",
        "OH_encoded_X, y = make_X_and_y()\n",
        "test_Regressor = XGBRegressor()\n",
        "test_Regressor.fit(X=OH_encoded_X, y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCiB9uDkUx5f"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZTMsEF1h1tZ"
      },
      "outputs": [],
      "source": [
        "product_df = make_product_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DobAndTvV_E7"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(product_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bec-52qiSGv"
      },
      "outputs": [],
      "source": [
        "product_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzaKgwQzTZQw"
      },
      "outputs": [],
      "source": [
        "OH_encoded_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCSczkwfwc0i"
      },
      "outputs": [],
      "source": [
        "# Loosely inspired by sample bootstraping [https://en.wikipedia.org/wiki/Bootstrap_aggregating]\n",
        "\n",
        "\n",
        "class IndiciesBootstrapper:\n",
        "\n",
        "    def __init__(self, number_of_bootstraps, min_bootstrap_size=2):\n",
        "        self.number_of_bootstraps = number_of_bootstraps\n",
        "        self.min_bootstrap_size = min_bootstrap_size\n",
        "        self.bootstraps = []\n",
        "        for i in range(number_of_bootstraps):\n",
        "            self.bootstraps.append([])\n",
        "\n",
        "    def bootstraps_diversity_first_indices_list(self, indices_list):\n",
        "        indices_list_size = len(indices_list)\n",
        "        bootstrap_size = indices_list_size - 1\n",
        "        temp_unique_combinations = [comb for comb in combinations(indices_list, bootstrap_size)]\n",
        "        temp_unique_combinations = np.array(temp_unique_combinations)\n",
        "        np.random.shuffle(temp_unique_combinations)\n",
        "        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n",
        "            self.bootstraps[temp_bootstrap_idx] = temp_unique_combinations[temp_bootstrap_idx].tolist()\n",
        "        return self.bootstraps\n",
        "\n",
        "    def classical_independent_bootstraps_indices_list(self, indices_list):\n",
        "        # temp_buffer = np.array(indices_list)\n",
        "        # np.random.shuffle(temp_buffer)\n",
        "        indices_list_size = len(indices_list)\n",
        "        potential_bootstrap_size = int(indices_list_size/self.number_of_bootstraps)\n",
        "        bootstrap_size = max(potential_bootstrap_size, self.min_bootstrap_size)\n",
        "        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n",
        "            self.bootstraps[temp_bootstrap_idx] = list(np.random.choice(indices_list, bootstrap_size, replace=False))\n",
        "        return self.bootstraps\n",
        "\n",
        "    def enhanced_classical_independent_bootstraps_indices_list(self, indices_list):\n",
        "        # temp_buffer = np.array(indices_list)\n",
        "        # np.random.shuffle(temp_buffer)\n",
        "        indices_list_size = len(indices_list)\n",
        "        no_repetitions_bootstrap_size = int(indices_list_size/self.number_of_bootstraps)\n",
        "        bootstrap_size = max(no_repetitions_bootstrap_size, self.min_bootstrap_size)\n",
        "        additional_samples_size = bootstrap_size - no_repetitions_bootstrap_size\n",
        "        #print(\"min_bootstrap_size: \", self.min_bootstrap_size)\n",
        "        #print(\"bootstrap_size: \", bootstrap_size)\n",
        "        #print(\"no_repetitions_bootstrap_size: \", no_repetitions_bootstrap_size)\n",
        "        #print(\"additional_samples_size: \", additional_samples_size)\n",
        "        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n",
        "            self.bootstraps[temp_bootstrap_idx] = list(np.random.choice(indices_list, no_repetitions_bootstrap_size, replace=False))\n",
        "            self.bootstraps[temp_bootstrap_idx] += list(np.random.choice(indices_list, additional_samples_size, replace=False))\n",
        "        return self.bootstraps\n",
        "\n",
        "\n",
        "#test_indices_list = list(range(3))\n",
        "test_indices_list = list(range(6))\n",
        "print(\"test_indices_list: \", test_indices_list)\n",
        "number_of_bootstraps = 3\n",
        "min_bootstrap_size = 4\n",
        "test_IndiciesBootstrapper = IndiciesBootstrapper(number_of_bootstraps,min_bootstrap_size)\n",
        "#test_bootstraps = test_IndiciesBootstrapper.bootstraps_diversity_first_indices_list(test_indices_list)\n",
        "#test_bootstraps = test_IndiciesBootstrapper.classical_independent_bootstraps_indices_list(test_indices_list)\n",
        "test_bootstraps = test_IndiciesBootstrapper.enhanced_classical_independent_bootstraps_indices_list(test_indices_list)\n",
        "print()\n",
        "print(\"test_bootstraps: \")\n",
        "print(test_bootstraps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDG5UbaAfcaY"
      },
      "source": [
        "#Szwabe: What are the differences between the three IndiciesBootstrapper's bootstraping functions/modes? To what extent is the bootstraps independence assumption followed in each of these bootstraping functions/modes? \n",
        "\n",
        "150284: IndiciesBootstrapper is used to estimate value functions at so-called indices (states) to bootstrap the estimates in the indices near. Full mode assumes the indices are independent, uses each index' estimate for indices near; Block mode: same schema, but indices are considered correlated; Window mode: indices are correlated, but it uses only indices near. Since independence is connected with blocks/windows used by Block and Window modes (see Note), these two modes follow the bootstraps independence assumption (however, still not fully), whereas in Full mode it is not preserved at all.\n",
        "\n",
        "**Note:** Larger bootstrapping blocks/windows lead to less independence between estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfu4VqcBIrql"
      },
      "outputs": [],
      "source": [
        "class AlternativeSamplesBootstrapper:\n",
        "\n",
        "    def __init__(self, number_of_bootstraps, min_bootstrap_size):\n",
        "        self.number_of_bootstraps = number_of_bootstraps\n",
        "        self.min_bootstrap_size = min_bootstrap_size\n",
        "        self.bootstraps = {\"X\":[], \"y\":[]}\n",
        "        self.indicies_bootstrapper = IndiciesBootstrapper(self.number_of_bootstraps, self.min_bootstrap_size)\n",
        "\n",
        "    def make_bootstraps(self, X, y):\n",
        "        indices_list = list(range(len(X)))\n",
        "        self.indices_bootstraps = self.indicies_bootstrapper.bootstraps_diversity_first_indices_list(indices_list)\n",
        "        #print(\"indices_bootstraps: \", self.indices_bootstraps)\n",
        "        X_as_array = np.array(X)\n",
        "        y_as_array = np.array(y)\n",
        "        self.bootstraps = {\"X\":[], \"y\":[]}\n",
        "        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n",
        "            #print(\"self.indices_bootstraps[temp_bootstrap_idx]: \", self.indices_bootstraps[temp_bootstrap_idx)\n",
        "            #print(\"np.array(self.indices_bootstraps[temp_bootstrap_idx]): \", np.array(self.indices_bootstraps[temp_bootstrap_idx]))\n",
        "            #print(\"X[[7,8,3], :]: \", X[[7,8,3], :])\n",
        "            self.bootstraps[\"X\"].append(X_as_array[self.indices_bootstraps[temp_bootstrap_idx], :])\n",
        "            self.bootstraps[\"y\"].append(y_as_array[self.indices_bootstraps[temp_bootstrap_idx]])\n",
        "            #self.bootstraps[temp_bootstrap_idx] = X[np.array(self.indices_bootstraps[temp_bootstrap_idx]), :]\n",
        "        return self.bootstraps\n",
        "\n",
        "\n",
        "test_num_of_samples = 10\n",
        "number_of_bootstraps = 3\n",
        "min_bootstrap_size = 2\n",
        "X, y = make_X_and_y()\n",
        "test_AlternativeSamplesBootstrapper = AlternativeSamplesBootstrapper(number_of_bootstraps, min_bootstrap_size)\n",
        "test_AlternativeSamplesBootstrapper.make_bootstraps(X[:test_num_of_samples], y[:test_num_of_samples])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8eLI5fydAmm"
      },
      "source": [
        "#Szwabe: How is the query strategy implemented?\n",
        "\n",
        "#What is the purpose of the temp_proba? What does temp_probas indicate? What is the impact of annealing_factor on temp_probas?\n",
        "\n",
        "150284: temp_proba is a parameter that is used to control the level of exploration during the committee-based learning. Higher temp_proba value leads to more exploratory policy, whereas a lower temp_proba value leads to more greedy policy. Greedy policy makes it more likely to sample from the best performing estimate, whereas exploratory one - from all the estimates in the committee. So that temp_proba to some extent is in charge of exploration-exploitation trade-off during commitee-based learning. The annealing_factor parameter is used to reduce the value of temp_proba during the learning. It shifts the learning from exploration to exploitation over time.\n",
        "\n",
        "**Note:** Committee-based learning involves sampling from the different estimates to select an action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uci6BzeJJgZ"
      },
      "outputs": [],
      "source": [
        "class AlternativeCommitteeRegressor:\n",
        "\n",
        "    def __init__(self, variance_unaware_regressor_class, number_of_regressors_in_commitee):\n",
        "        self.number_of_regressors_in_committee = number_of_regressors_in_commitee\n",
        "        self.min_bootstrap_size = 2 # temporary\n",
        "        self.committee = []\n",
        "        for regressor_idx in range(self.number_of_regressors_in_committee):\n",
        "            #self.committee.append(variance_unaware_regressor_class())\n",
        "            self.committee.append(variance_unaware_regressor_class(verbosity = 0))\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.samples_bootstraper = AlternativeSamplesBootstrapper(self.number_of_regressors_in_committee, self.min_bootstrap_size)\n",
        "\n",
        "    def _distribute_X_and_y(self):\n",
        "        pass\n",
        "\n",
        "    def teach(self, X, y):\n",
        "        self.X.append(X)\n",
        "        self.y.append(y)\n",
        "\n",
        "    def _make_bootstraps(self):\n",
        "        self.bootstraps = self.samples_bootstraper.make_bootstraps(self.X, self.y)\n",
        "\n",
        "    def _committee_train(self):\n",
        "        for regressor_idx in range(self.number_of_regressors_in_committee):\n",
        "            self.committee[regressor_idx].fit(X=self.bootstraps[\"X\"][regressor_idx], y=self.bootstraps[\"y\"][regressor_idx])\n",
        "\n",
        "    def predict(self, X_query):\n",
        "        self.committee_predictions = []\n",
        "        for regressor_idx in range(self.number_of_regressors_in_committee):\n",
        "            self.committee_predictions.append(self.committee[regressor_idx].predict(X_query))\n",
        "\n",
        "    def _compute_means_and_STDs_for_query(self, X_query):\n",
        "        self._make_bootstraps()\n",
        "        self._committee_train()\n",
        "        self.predict(X_query)\n",
        "\n",
        "        self.committee_predictions_array = np.array(self.committee_predictions)\n",
        "\n",
        "        self._committee_means = np.mean(self.committee_predictions_array, axis=0)\n",
        "        # self.committee_differences = self.committee_predictions_array - self.committee_means\n",
        "        # self.committee_differences_squared = self.committee_differences**2\n",
        "        # self.committee_differences_squared_sums = np.sum(self.committee_differences_squared, axis=0)\n",
        "        # self.committee_differences_squared_means = self.committee_differences_squared_sums/self.committee_differences.shape[0]\n",
        "        # self.committee_differences_squared_means_roots = np.sqrt(self.committee_differences_squared_means)\n",
        "        # Szwabe: What would the 5 lines above do if they were not commented?\n",
        "        '''\n",
        "        150284: \n",
        "        It would estimate variance of the estimates, so that\n",
        "        it would reduce less certain estimates' weights.\n",
        "        This could improve the stability.\n",
        "        '''\n",
        "        self._committee_STDs = np.std(self.committee_predictions_array, axis=0)\n",
        "        #print(self.committee_STDs == self.committee_differences_squared_means_roots)\n",
        "        #return self.committee_means, self.committee_STDs\n",
        "\n",
        "    def query(self, X_query):\n",
        "        self._compute_means_and_STDs_for_query(X_query)\n",
        "        self.query_result = np.argmax(self._committee_STDs)\n",
        "        return self.query_result\n",
        "\n",
        "\n",
        "X, y = make_X_and_y()\n",
        "shuffled_indices = np.arange(y.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "y = y[shuffled_indices]\n",
        "X = X[shuffled_indices]\n",
        "\n",
        "number_of_regressors_in_commitee = 2\n",
        "committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n",
        "number_of_initial_samples = 2 * number_of_regressors_in_commitee #temporary\n",
        "initial_X_samples = X[:number_of_initial_samples]\n",
        "initial_y_samples = y[:number_of_initial_samples]\n",
        "X_query = X[number_of_initial_samples:]\n",
        "y_query = y[number_of_initial_samples:]\n",
        "for temp_initial_sample_idx in range(number_of_initial_samples):\n",
        "  X_train_sample = initial_X_samples[temp_initial_sample_idx]\n",
        "  y_train_sample = initial_y_samples[temp_initial_sample_idx]\n",
        "  committee.teach(X_train_sample, y_train_sample)\n",
        "query_result = committee.query(X_query)\n",
        "MSE_scores_of_AlternativeCommitteeRegressor = []\n",
        "MAE_scores_of_AlternativeCommitteeRegressor = []\n",
        "temp_probas = []\n",
        "annealing_factor = 0.04\n",
        "for i in range(200):\n",
        "  query_result = committee.query(X_query)\n",
        "  temp_prob = np.random.rand()\n",
        "  temp_prob = np.log(temp_prob)\n",
        "  temp_prob = temp_prob / (annealing_factor * i + 1)\n",
        "  temp_prob = int(round(temp_prob)) % 2\n",
        "  temp_probas.append(temp_prob)\n",
        "  print(\"temp_prob: \", temp_prob)\n",
        "  if temp_prob > 0.5:\n",
        "      query_result = sample(list(range(X_query.shape[0])), 1)[0]\n",
        "      print(\"this time it's random\")\n",
        "  print(\"query_result: \", query_result)\n",
        "  temp_committee_predictions = committee._committee_means\n",
        "  temp_score = mean_squared_error(y_query, temp_committee_predictions)\n",
        "  MSE_scores_of_AlternativeCommitteeRegressor.append(temp_score)\n",
        "  temp_score = mean_absolute_error(y_query, temp_committee_predictions)\n",
        "  MAE_scores_of_AlternativeCommitteeRegressor.append(temp_score)\n",
        "  another_teaching_sample_X = X_query[query_result]\n",
        "  another_teaching_sample_y = y_query[query_result]\n",
        "  committee.teach(another_teaching_sample_X, another_teaching_sample_y)\n",
        "  X_query = X_query[np.arange(len(X_query))!=query_result]\n",
        "  y_query = y_query[np.arange(len(y_query))!=query_result]\n",
        "\n",
        "committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n",
        "number_of_initial_samples = 2 * number_of_regressors_in_commitee #temporary\n",
        "initial_X_samples = X[:number_of_initial_samples]\n",
        "initial_y_samples = y[:number_of_initial_samples]\n",
        "X_query = X[number_of_initial_samples:]\n",
        "y_query = y[number_of_initial_samples:]\n",
        "for temp_initial_sample_idx in range(number_of_initial_samples):\n",
        "    X_train_sample = initial_X_samples[temp_initial_sample_idx]\n",
        "    y_train_sample = initial_y_samples[temp_initial_sample_idx]\n",
        "    committee.teach(X_train_sample, y_train_sample)\n",
        "query_result = committee.query(X_query)\n",
        "\n",
        "MSE_scores_of_random_baseline = []\n",
        "MAE_scores_of_random_baseline = []\n",
        "for i in range(200):\n",
        "    query_result = committee.query(X_query)\n",
        "    query_result = sample(list(range(X_query.shape[0])), 1)[0]\n",
        "    print(\"query_result: \", query_result)\n",
        "    temp_committee_predictions = committee._committee_means\n",
        "    temp_score = mean_squared_error(y_query, temp_committee_predictions)\n",
        "    MSE_scores_of_random_baseline.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_committee_predictions)\n",
        "    MAE_scores_of_random_baseline.append(temp_score)\n",
        "    another_teaching_sample_X = X_query[query_result]\n",
        "    another_teaching_sample_y = y_query[query_result]\n",
        "    committee.teach(another_teaching_sample_X, another_teaching_sample_y)\n",
        "    X_query = X_query[np.arange(len(X_query))!=query_result]\n",
        "    y_query = y_query[np.arange(len(y_query))!=query_result]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnTaSLT_6_Rd"
      },
      "outputs": [],
      "source": [
        "plt.plot(MSE_scores_of_AlternativeCommitteeRegressor, label=\"AlternativeCommitteeRegressor\")\n",
        "plt.plot(MSE_scores_of_random_baseline, label=\"Random\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKqIKPcM7D9C"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "plt.plot(MAE_scores_of_AlternativeCommitteeRegressor, label=\"AlternativeCommitteeRegressor\")\n",
        "plt.plot(MAE_scores_of_random_baseline, label=\"Random\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCmgxQ5v8GBR"
      },
      "outputs": [],
      "source": [
        "    plt.clf()\n",
        "    plt.plot(temp_probas, label=\"temp_probas\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ndCmV7XzeM"
      },
      "outputs": [],
      "source": [
        "def test_comparing_with_baseline(X, y, annealing_factor, number_of_regressors_in_commitee, number_of_cycles):\n",
        "  shuffled_indices = np.arange(y.shape[0])\n",
        "  np.random.shuffle(shuffled_indices)\n",
        "  y = y[shuffled_indices]\n",
        "  X = X[shuffled_indices]\n",
        "\n",
        "  committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n",
        "  number_of_initial_samples = 2 * number_of_regressors_in_commitee #temporary\n",
        "  initial_X_samples = X[:number_of_initial_samples]\n",
        "  initial_y_samples = y[:number_of_initial_samples]\n",
        "  X_query = X[number_of_initial_samples:]\n",
        "  y_query = y[number_of_initial_samples:]\n",
        "  for temp_initial_sample_idx in range(number_of_initial_samples):\n",
        "    X_train_sample = initial_X_samples[temp_initial_sample_idx]\n",
        "    y_train_sample = initial_y_samples[temp_initial_sample_idx]\n",
        "    committee.teach(X_train_sample, y_train_sample)\n",
        "  query_result = committee.query(X_query)\n",
        "  MSE_scores_of_AlternativeCommitteeRegressor = []\n",
        "  MAE_scores_of_AlternativeCommitteeRegressor = []\n",
        "  temp_probas = []\n",
        "  for i in range(number_of_cycles):\n",
        "    query_result = committee.query(X_query)\n",
        "    temp_prob = np.random.rand()\n",
        "    temp_prob = np.log(temp_prob)\n",
        "    temp_prob = temp_prob / (annealing_factor * i + 1)\n",
        "    temp_prob = int(round(temp_prob)) % 2\n",
        "    temp_probas.append(temp_prob)\n",
        "    print(\"temp_prob: \", temp_prob)\n",
        "    if temp_prob > 0.5:\n",
        "        query_result = sample(list(range(X_query.shape[0])), 1)[0]\n",
        "        print(\"this time it's random\")\n",
        "    print(\"query_result: \", query_result)\n",
        "    temp_committee_predictions = committee._committee_means\n",
        "    temp_score = mean_squared_error(y_query, temp_committee_predictions)\n",
        "    MSE_scores_of_AlternativeCommitteeRegressor.append(temp_score)\n",
        "    temp_score = mean_absolute_error(y_query, temp_committee_predictions)\n",
        "    MAE_scores_of_AlternativeCommitteeRegressor.append(temp_score)\n",
        "    another_teaching_sample_X = X_query[query_result]\n",
        "    another_teaching_sample_y = y_query[query_result]\n",
        "    committee.teach(another_teaching_sample_X, another_teaching_sample_y)\n",
        "    X_query = X_query[np.arange(len(X_query))!=query_result]\n",
        "    y_query = y_query[np.arange(len(y_query))!=query_result]\n",
        "  committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n",
        "  number_of_initial_samples = 2 * number_of_regressors_in_commitee #temporary\n",
        "  initial_X_samples = X[:number_of_initial_samples]\n",
        "  initial_y_samples = y[:number_of_initial_samples]\n",
        "  X_query = X[number_of_initial_samples:]\n",
        "  y_query = y[number_of_initial_samples:]\n",
        "  for temp_initial_sample_idx in range(number_of_initial_samples):\n",
        "      X_train_sample = initial_X_samples[temp_initial_sample_idx]\n",
        "      y_train_sample = initial_y_samples[temp_initial_sample_idx]\n",
        "      committee.teach(X_train_sample, y_train_sample)\n",
        "  query_result = committee.query(X_query)\n",
        "  MSE_scores_of_random_baseline = []\n",
        "  MAE_scores_of_random_baseline = []\n",
        "  for i in range(number_of_cycles):\n",
        "      query_result = committee.query(X_query)\n",
        "      query_result = sample(list(range(X_query.shape[0])), 1)[0]\n",
        "      print(\"query_result: \", query_result)\n",
        "      temp_committee_predictions = committee._committee_means\n",
        "      temp_score = mean_squared_error(y_query, temp_committee_predictions)\n",
        "      MSE_scores_of_random_baseline.append(temp_score)\n",
        "      temp_score = mean_absolute_error(y_query, temp_committee_predictions)\n",
        "      MAE_scores_of_random_baseline.append(temp_score)\n",
        "      another_teaching_sample_X = X_query[query_result]\n",
        "      another_teaching_sample_y = y_query[query_result]\n",
        "      committee.teach(another_teaching_sample_X, another_teaching_sample_y)\n",
        "      X_query = X_query[np.arange(len(X_query))!=query_result]\n",
        "      y_query = y_query[np.arange(len(y_query))!=query_result]\n",
        "  return MSE_scores_of_AlternativeCommitteeRegressor, MSE_scores_of_random_baseline, MAE_scores_of_AlternativeCommitteeRegressor, MAE_scores_of_random_baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZBjG_RhXzgm"
      },
      "outputs": [],
      "source": [
        "def multiple_tests_comparing_with_baseline(X, y, number_of_tests, annealing_factor, number_of_regressors_in_commitee, number_of_cycles):\n",
        "  means_and_stds = {\"MAE\": {}, \"MSE\": {}}\n",
        "  MSE_scores_of_AlternativeCommitteeRegressor_list = []\n",
        "  MSE_scores_of_random_baseline_list = []\n",
        "  MAE_scores_of_AlternativeCommitteeRegressor_list = []\n",
        "  MAE_scores_of_random_baseline_list = []\n",
        "  for test_number in range(number_of_tests):\n",
        "    MSE_scores_of_AlternativeCommitteeRegressor, MSE_scores_of_random_baseline, MAE_scores_of_AlternativeCommitteeRegressor, MAE_scores_of_random_baseline = test_comparing_with_baseline(X, y, annealing_factor, number_of_regressors_in_commitee, number_of_cycles)\n",
        "    MSE_scores_of_AlternativeCommitteeRegressor_list.append(MSE_scores_of_AlternativeCommitteeRegressor)\n",
        "    MSE_scores_of_random_baseline_list.append(MSE_scores_of_random_baseline)\n",
        "    MAE_scores_of_AlternativeCommitteeRegressor_list.append(MAE_scores_of_AlternativeCommitteeRegressor)\n",
        "    MAE_scores_of_random_baseline_list.append(MAE_scores_of_random_baseline)\n",
        "  temp_mean = np.mean(np.array(MSE_scores_of_AlternativeCommitteeRegressor_list), axis=0)\n",
        "  temp_std = np.std(np.array(MSE_scores_of_AlternativeCommitteeRegressor_list), axis=0)\n",
        "  means_and_stds[\"MSE\"][\"AlternativeCommitteeRegressor\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MAE_scores_of_AlternativeCommitteeRegressor_list), axis=0)\n",
        "  temp_std = np.std(np.array(MAE_scores_of_AlternativeCommitteeRegressor_list), axis=0)\n",
        "  means_and_stds[\"MAE\"][\"AlternativeCommitteeRegressor\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  temp_std = np.std(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  means_and_stds[\"MSE\"][\"random_baseline\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  temp_mean = np.mean(np.array(MAE_scores_of_random_baseline_list), axis=0)\n",
        "  temp_std = np.std(np.array(MAE_scores_of_random_baseline_list), axis=0)\n",
        "  means_and_stds[\"MAE\"][\"random_baseline\"] = {\"mean\": temp_mean, \"std\": temp_std}\n",
        "  MSE_scores_of_random_baseline_mean = np.mean(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  MSE_scores_of_random_baseline_std = np.std(np.array(MSE_scores_of_random_baseline_list), axis=0)\n",
        "  return means_and_stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr_YH6G2XzjZ"
      },
      "outputs": [],
      "source": [
        "def plot_means_and_stds(means_and_stds, temp_measure):\n",
        "  plt.clf\n",
        "  plt.rcParams[\"figure.figsize\"] = (20,14)\n",
        "  temp_color_letters = [\"b\", \"y\"]\n",
        "  #temp_measure = \"MAE\"\n",
        "  temp_measure_data = means_and_stds[temp_measure]\n",
        "  temp_algorithms_labes = list(temp_measure_data)\n",
        "  for temp_algorithm_label_idx, temp_algorithm_label in enumerate(temp_algorithms_labes):\n",
        "    temp_color_letter = temp_color_letters[temp_algorithm_label_idx]\n",
        "    temp_mean = means_and_stds[temp_measure][temp_algorithm_label][\"mean\"]\n",
        "    temp_std = means_and_stds[temp_measure][temp_algorithm_label][\"std\"]\n",
        "    x = np.arange(len(temp_mean))\n",
        "    plt.plot(x, temp_mean, temp_color_letter+\"-\", label=temp_algorithm_label)\n",
        "    plt.fill_between(x, temp_mean - temp_std, temp_mean + temp_std, color=temp_color_letter, alpha=0.2)\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a1smxoqBUXR"
      },
      "source": [
        "#Szwabe: Set an appropriate number of AR cycles and test repetitions (below) in order to compare practical value of the sampling strategies in the most reliable way that is still feasible considering the amount of available time (of the classwork). Compare and comment on the practical value of the AR solution. What is the true practical value of the temp_proba for different values of annealing_factor? Can you make the active regressor significantly (and reliably) more effective than random sampling by using some specific combination of bootstraping function/mode and annealing_factor value? Note all the observations you make.\n",
        "\n",
        "150284: We need bigger number_of_tests using small annealing factor (such as 0.03, e.g.). However, we could also lower the number_of_tests to around 20 using annealing_factor of 0.1 at the time. It gives approximately the same results, since higher annealing_factor makes the exploitation faster. It is also possible to lower the number_of_cyces to 100 to achieve approximately the same results. It indeed is possible to make the active regressor significantly (and reliably) more effective than random sampling by using some specific combination of bootstraping function/mode and annealing_factor value. We could start with temp_proba of 1, making it firstly completely exploitational. And then we could periodically decrease annealing_factor value, so that it would reduce temp_proba and lead to more of the exploration. The number_of_cycles increases the running time a lot, so that I didn't manage to test it for the  value bigger than 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLVkl7-DYMWx"
      },
      "outputs": [],
      "source": [
        "number_of_tests = 25\n",
        "# number_of_tests = 4 # Warning: ridiculously low number\n",
        "annealing_factor = 0.06\n",
        "number_of_regressors_in_commitee = 2\n",
        "number_of_cycles = 100\n",
        "\n",
        "means_and_stds = multiple_tests_comparing_with_baseline(X, y, number_of_tests, annealing_factor, number_of_regressors_in_commitee, number_of_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLZwtjiVwPDQ"
      },
      "outputs": [],
      "source": [
        "plot_means_and_stds(means_and_stds, \"MSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVlFSUCVwMb9"
      },
      "outputs": [],
      "source": [
        "plot_means_and_stds(means_and_stds, \"MAE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deirs_3B1qrD"
      },
      "outputs": [],
      "source": [
        "number_of_tests = 25\n",
        "# number_of_tests = 4 # Warning: ridiculously low number\n",
        "annealing_factor = 0.08\n",
        "number_of_regressors_in_commitee = 2\n",
        "number_of_cycles = 100\n",
        "\n",
        "means_and_stds = multiple_tests_comparing_with_baseline(X, y, number_of_tests, annealing_factor, number_of_regressors_in_commitee, number_of_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmwQQmjvw8Ql"
      },
      "outputs": [],
      "source": [
        "plot_means_and_stds(means_and_stds, \"MSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkGu0i6tw8Qs"
      },
      "outputs": [],
      "source": [
        "plot_means_and_stds(means_and_stds, \"MAE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW0JNALd1uKn"
      },
      "outputs": [],
      "source": [
        "number_of_tests = 25\n",
        "# number_of_tests = 4 # Warning: ridiculously low number\n",
        "annealing_factor = 0.1\n",
        "number_of_regressors_in_commitee = 2\n",
        "number_of_cycles = 100\n",
        "\n",
        "means_and_stds = multiple_tests_comparing_with_baseline(X, y, number_of_tests, annealing_factor, number_of_regressors_in_commitee, number_of_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxZwkbIKwgcC"
      },
      "outputs": [],
      "source": [
        "plot_means_and_stds(means_and_stds, \"MSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8HIfzhrwgcG"
      },
      "outputs": [],
      "source": [
        "plot_means_and_stds(means_and_stds, \"MAE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fttzlMWjxfyg"
      },
      "outputs": [],
      "source": [
        "!pip freeze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXkxU3I2x4FO"
      },
      "source": [
        "absl-py==1.0.0\n",
        "alabaster==0.7.12\n",
        "albumentations==0.1.12\n",
        "altair==4.2.0\n",
        "appdirs==1.4.4\n",
        "argon2-cffi==21.3.0\n",
        "argon2-cffi-bindings==21.2.0\n",
        "arviz==0.12.0\n",
        "astor==0.8.1\n",
        "astropy==4.3.1\n",
        "astunparse==1.6.3\n",
        "atari-py==0.2.9\n",
        "atomicwrites==1.4.0\n",
        "attrs==21.4.0\n",
        "audioread==2.1.9\n",
        "autograd==1.4\n",
        "Babel==2.9.1\n",
        "backcall==0.2.0\n",
        "beautifulsoup4==4.6.3\n",
        "bleach==5.0.0\n",
        "blis==0.4.1\n",
        "bokeh==2.3.3\n",
        "Bottleneck==1.3.4\n",
        "branca==0.4.2\n",
        "bs4==0.0.1\n",
        "CacheControl==0.12.10\n",
        "cached-property==1.5.2\n",
        "cachetools==4.2.4\n",
        "catalogue==1.0.0\n",
        "certifi==2021.10.8\n",
        "cffi==1.15.0\n",
        "cftime==1.6.0\n",
        "chardet==3.0.4\n",
        "charset-normalizer==2.0.12\n",
        "click==7.1.2\n",
        "cloudpickle==1.3.0\n",
        "cmake==3.12.0\n",
        "cmdstanpy==0.9.5\n",
        "colorcet==3.0.0\n",
        "colorlover==0.3.0\n",
        "community==1.0.0b1\n",
        "contextlib2==0.5.5\n",
        "convertdate==2.4.0\n",
        "coverage==3.7.1\n",
        "coveralls==0.5\n",
        "crcmod==1.7\n",
        "cufflinks==0.17.3\n",
        "cvxopt==1.2.7\n",
        "cvxpy==1.0.31\n",
        "cycler==0.11.0\n",
        "cymem==2.0.6\n",
        "Cython==0.29.28\n",
        "daft==0.0.4\n",
        "dask==2.12.0\n",
        "datascience==0.10.6\n",
        "debugpy==1.0.0\n",
        "decorator==4.4.2\n",
        "defusedxml==0.7.1\n",
        "descartes==1.1.0\n",
        "dill==0.3.4\n",
        "distributed==1.25.3\n",
        "dlib @ file:///dlib-19.18.0-cp37-cp37m-linux_x86_64.whl\n",
        "dm-tree==0.1.7\n",
        "docopt==0.6.2\n",
        "docutils==0.17.1\n",
        "dopamine-rl==1.0.5\n",
        "earthengine-api==0.1.306\n",
        "easydict==1.9\n",
        "ecos==2.0.10\n",
        "editdistance==0.5.3\n",
        "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
        "entrypoints==0.4\n",
        "ephem==4.1.3\n",
        "et-xmlfile==1.1.0\n",
        "fa2==0.3.5\n",
        "fastai==1.0.61\n",
        "fastdtw==0.3.4\n",
        "fastjsonschema==2.15.3\n",
        "fastprogress==1.0.2\n",
        "fastrlock==0.8\n",
        "fbprophet==0.7.1\n",
        "feather-format==0.4.1\n",
        "filelock==3.6.0\n",
        "firebase-admin==4.4.0\n",
        "fix-yahoo-finance==0.0.22\n",
        "Flask==1.1.4\n",
        "flatbuffers==2.0\n",
        "folium==0.8.3\n",
        "future==0.16.0\n",
        "gast==0.5.3\n",
        "GDAL==2.2.2\n",
        "gdown==4.4.0\n",
        "gensim==3.6.0\n",
        "geographiclib==1.52\n",
        "geopy==1.17.0\n",
        "gin-config==0.5.0\n",
        "glob2==0.7\n",
        "google==2.0.3\n",
        "google-api-core==1.31.5\n",
        "google-api-python-client==1.12.11\n",
        "google-auth==1.35.0\n",
        "google-auth-httplib2==0.0.4\n",
        "google-auth-oauthlib==0.4.6\n",
        "google-cloud-bigquery==1.21.0\n",
        "google-cloud-bigquery-storage==1.1.1\n",
        "google-cloud-core==1.0.3\n",
        "google-cloud-datastore==1.8.0\n",
        "google-cloud-firestore==1.7.0\n",
        "google-cloud-language==1.2.0\n",
        "google-cloud-storage==1.18.1\n",
        "google-cloud-translate==1.5.0\n",
        "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz\n",
        "google-pasta==0.2.0\n",
        "google-resumable-media==0.4.1\n",
        "googleapis-common-protos==1.56.0\n",
        "googledrivedownloader==0.4\n",
        "graphviz==0.10.1\n",
        "greenlet==1.1.2\n",
        "grpcio==1.44.0\n",
        "gspread==3.4.2\n",
        "gspread-dataframe==3.0.8\n",
        "gym==0.17.3\n",
        "h5py==3.1.0\n",
        "HeapDict==1.0.1\n",
        "hijri-converter==2.2.3\n",
        "holidays==0.10.5.2\n",
        "holoviews==1.14.8\n",
        "html5lib==1.0.1\n",
        "httpimport==0.5.18\n",
        "httplib2==0.17.4\n",
        "httplib2shim==0.0.3\n",
        "humanize==0.5.1\n",
        "hyperopt==0.1.2\n",
        "ideep4py==2.0.0.post3\n",
        "idna==2.10\n",
        "imageio==2.4.1\n",
        "imagesize==1.3.0\n",
        "imbalanced-learn==0.8.1\n",
        "imblearn==0.0\n",
        "imgaug==0.2.9\n",
        "importlib-metadata==4.11.3\n",
        "importlib-resources==5.7.0\n",
        "imutils==0.5.4\n",
        "inflect==2.1.0\n",
        "iniconfig==1.1.1\n",
        "intel-openmp==2022.0.2\n",
        "intervaltree==2.1.0\n",
        "ipykernel==4.10.1\n",
        "ipython==5.5.0\n",
        "ipython-genutils==0.2.0\n",
        "ipython-sql==0.3.9\n",
        "ipywidgets==7.7.0\n",
        "itsdangerous==1.1.0\n",
        "jax==0.3.4\n",
        "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.2+cuda11.cudnn805-cp37-none-manylinux2010_x86_64.whl\n",
        "jedi==0.18.1\n",
        "jieba==0.42.1\n",
        "Jinja2==2.11.3\n",
        "joblib==1.1.0\n",
        "jpeg4py==0.1.4\n",
        "jsonschema==4.3.3\n",
        "jupyter==1.0.0\n",
        "jupyter-client==5.3.5\n",
        "jupyter-console==5.2.0\n",
        "jupyter-core==4.9.2\n",
        "jupyterlab-pygments==0.2.2\n",
        "jupyterlab-widgets==1.1.0\n",
        "kaggle==1.5.12\n",
        "kapre==0.3.7\n",
        "keras==2.8.0\n",
        "Keras-Preprocessing==1.1.2\n",
        "keras-vis==0.4.1\n",
        "kiwisolver==1.4.2\n",
        "korean-lunar-calendar==0.2.1\n",
        "libclang==13.0.0\n",
        "librosa==0.8.1\n",
        "lightgbm==2.2.3\n",
        "llvmlite==0.34.0\n",
        "lmdb==0.99\n",
        "LunarCalendar==0.0.9\n",
        "lxml==4.2.6\n",
        "Markdown==3.3.6\n",
        "MarkupSafe==2.0.1\n",
        "matplotlib==3.2.2\n",
        "matplotlib-inline==0.1.3\n",
        "matplotlib-venn==0.11.7\n",
        "missingno==0.5.1\n",
        "mistune==0.8.4\n",
        "mizani==0.6.0\n",
        "mkl==2019.0\n",
        "mlxtend==0.14.0\n",
        "more-itertools==8.12.0\n",
        "moviepy==0.2.3.5\n",
        "mpmath==1.2.1\n",
        "msgpack==1.0.3\n",
        "multiprocess==0.70.12.2\n",
        "multitasking==0.0.10\n",
        "murmurhash==1.0.6\n",
        "music21==5.5.0\n",
        "natsort==5.5.0\n",
        "nbclient==0.6.0\n",
        "nbconvert==5.6.1\n",
        "nbformat==5.3.0\n",
        "nest-asyncio==1.5.5\n",
        "netCDF4==1.5.8\n",
        "networkx==2.6.3\n",
        "nibabel==3.0.2\n",
        "nltk==3.2.5\n",
        "notebook==5.3.1\n",
        "numba==0.51.2\n",
        "numexpr==2.8.1\n",
        "numpy==1.21.6\n",
        "nvidia-ml-py3==7.352.0\n",
        "oauth2client==4.1.3\n",
        "oauthlib==3.2.0\n",
        "okgrade==0.4.3\n",
        "opencv-contrib-python==4.1.2.30\n",
        "opencv-python==4.1.2.30\n",
        "openpyxl==3.0.9\n",
        "opt-einsum==3.3.0\n",
        "osqp==0.6.2.post0\n",
        "packaging==21.3\n",
        "palettable==3.3.0\n",
        "pandas==1.3.5\n",
        "pandas-datareader==0.9.0\n",
        "pandas-gbq==0.13.3\n",
        "pandas-profiling==1.4.1\n",
        "pandocfilters==1.5.0\n",
        "panel==0.12.1\n",
        "param==1.12.1\n",
        "parso==0.8.3\n",
        "pathlib==1.0.1\n",
        "patsy==0.5.2\n",
        "pep517==0.12.0\n",
        "pexpect==4.8.0\n",
        "pickleshare==0.7.5\n",
        "Pillow==7.1.2\n",
        "pip-tools==6.2.0\n",
        "plac==1.1.3\n",
        "plotly==5.5.0\n",
        "plotnine==0.6.0\n",
        "pluggy==0.7.1\n",
        "pooch==1.6.0\n",
        "portpicker==1.3.9\n",
        "prefetch-generator==1.0.1\n",
        "preshed==3.0.6\n",
        "prettytable==3.2.0\n",
        "progressbar2==3.38.0\n",
        "prometheus-client==0.14.1\n",
        "promise==2.3\n",
        "prompt-toolkit==1.0.18\n",
        "protobuf==3.17.3\n",
        "psutil==5.4.8\n",
        "psycopg2==2.7.6.1\n",
        "ptyprocess==0.7.0\n",
        "py==1.11.0\n",
        "pyarrow==6.0.1\n",
        "pyasn1==0.4.8\n",
        "pyasn1-modules==0.2.8\n",
        "pycocotools==2.0.4\n",
        "pycparser==2.21\n",
        "pyct==0.4.8\n",
        "pydata-google-auth==1.4.0\n",
        "pydot==1.3.0\n",
        "pydot-ng==2.0.0\n",
        "pydotplus==2.0.2\n",
        "PyDrive==1.3.1\n",
        "pyemd==0.5.1\n",
        "pyerfa==2.0.0.1\n",
        "pyglet==1.5.0\n",
        "Pygments==2.6.1\n",
        "pygobject==3.26.1\n",
        "pymc3==3.11.4\n",
        "PyMeeus==0.5.11\n",
        "pymongo==4.1.1\n",
        "pymystem3==0.2.0\n",
        "PyOpenGL==3.1.6\n",
        "pyparsing==3.0.8\n",
        "pyrsistent==0.18.1\n",
        "pysndfile==1.3.8\n",
        "PySocks==1.7.1\n",
        "pystan==2.19.1.1\n",
        "pytest==3.6.4\n",
        "python-apt==0.0.0\n",
        "python-chess==0.23.11\n",
        "python-dateutil==2.8.2\n",
        "python-louvain==0.16\n",
        "python-slugify==6.1.1\n",
        "python-utils==3.1.0\n",
        "pytz==2022.1\n",
        "pyviz-comms==2.2.0\n",
        "PyWavelets==1.3.0\n",
        "PyYAML==3.13\n",
        "pyzmq==22.3.0\n",
        "qdldl==0.1.5.post2\n",
        "qtconsole==5.3.0\n",
        "QtPy==2.0.1\n",
        "regex==2019.12.20\n",
        "requests==2.23.0\n",
        "requests-oauthlib==1.3.1\n",
        "resampy==0.2.2\n",
        "rpy2==3.4.5\n",
        "rsa==4.8\n",
        "scikit-image==0.18.3\n",
        "scikit-learn==1.0.2\n",
        "scipy==1.4.1\n",
        "screen-resolution-extra==0.0.0\n",
        "scs==3.2.0\n",
        "seaborn==0.11.2\n",
        "semver==2.13.0\n",
        "Send2Trash==1.8.0\n",
        "setuptools-git==1.2\n",
        "Shapely==1.8.1.post1\n",
        "simplegeneric==0.8.1\n",
        "six==1.15.0\n",
        "sklearn==0.0\n",
        "sklearn-pandas==1.8.0\n",
        "smart-open==5.2.1\n",
        "snowballstemmer==2.2.0\n",
        "sortedcontainers==2.4.0\n",
        "SoundFile==0.10.3.post1\n",
        "soupsieve==2.3.2.post1\n",
        "spacy==2.2.4\n",
        "Sphinx==1.8.6\n",
        "sphinxcontrib-serializinghtml==1.1.5\n",
        "sphinxcontrib-websupport==1.2.4\n",
        "SQLAlchemy==1.4.35\n",
        "sqlparse==0.4.2\n",
        "srsly==1.0.5\n",
        "statsmodels==0.10.2\n",
        "sympy==1.7.1\n",
        "tables==3.7.0\n",
        "tabulate==0.8.9\n",
        "tblib==1.7.0\n",
        "tenacity==8.0.1\n",
        "tensorboard==2.8.0\n",
        "tensorboard-data-server==0.6.1\n",
        "tensorboard-plugin-wit==1.8.1\n",
        "tensorflow @ file:///tensorflow-2.8.0-cp37-cp37m-linux_x86_64.whl\n",
        "tensorflow-datasets==4.0.1\n",
        "tensorflow-estimator==2.8.0\n",
        "tensorflow-gcs-config==2.8.0\n",
        "tensorflow-hub==0.12.0\n",
        "tensorflow-io-gcs-filesystem==0.24.0\n",
        "tensorflow-metadata==1.7.0\n",
        "tensorflow-probability==0.16.0\n",
        "termcolor==1.1.0\n",
        "terminado==0.13.3\n",
        "testpath==0.6.0\n",
        "text-unidecode==1.3\n",
        "textblob==0.15.3\n",
        "Theano-PyMC==1.1.2\n",
        "thinc==7.4.0\n",
        "threadpoolctl==3.1.0\n",
        "tifffile==2021.11.2\n",
        "tinycss2==1.1.1\n",
        "tomli==2.0.1\n",
        "toolz==0.11.2\n",
        "torch @ https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "torchaudio @ https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "torchsummary==1.5.1\n",
        "torchtext==0.11.0\n",
        "torchvision @ https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "tornado==5.1.1\n",
        "tqdm==4.64.0\n",
        "traitlets==5.1.1\n",
        "tweepy==3.10.0\n",
        "typeguard==2.7.1\n",
        "typing-extensions==4.1.1\n",
        "tzlocal==1.5.1\n",
        "uritemplate==3.0.1\n",
        "urllib3==1.24.3\n",
        "vega-datasets==0.9.0\n",
        "wasabi==0.9.1\n",
        "wcwidth==0.2.5\n",
        "webencodings==0.5.1\n",
        "Werkzeug==1.0.1\n",
        "widgetsnbextension==3.6.0\n",
        "wordcloud==1.5.0\n",
        "wrapt==1.14.0\n",
        "xarray==0.18.2\n",
        "xgboost==0.90\n",
        "xkit==0.0.0\n",
        "xlrd==1.1.0\n",
        "xlwt==1.3.0\n",
        "yellowbrick==1.4\n",
        "zict==2.1.0\n",
        "zipp==3.8.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGmZtxGzD10Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}