{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional libraries\n",
    "\n",
    "Install the following:\n",
    "\n",
    "* PyTorch (CPU version is sufficient, you don't need GPU support) - Installation instruction on https://pytorch.org/\n",
    "* matplotlib and scikit-learn - these two should be available in your Python distribution (e.g., using pip: `pip install scikit-learn matplotlib`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "For this task we'll consider the problem of predicting houses prices in California. We begin by loading the dataset using `fetch_california_housing`. The returned object contains a few fields:\n",
    "\n",
    "* `data` - the matrix of features\n",
    "* `feature_names` - names of the features\n",
    "* `target` - the values to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are not very meaningful, but fortunately we can proceed without detailed understanding of the dataset. If you are interested, some details are available at https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the data to `torch.Tensor`, a PyTorch-specific representation of a tensor (a tensor is a generalization of a matrix to more than 2 dimensions). The `dtype` parameter forces conversion to floating-point numbers. The output of the cell is the size of the matrices. The output of the cell are the shapes of both tensors - `X` is 20640 rows by 8 columns and `y` is a vector of 20640 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(dataset['data'], dtype=torch.float)\n",
    "y = torch.tensor(dataset['target'], dtype=torch.float)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the data randomly into three subsets: a training set consisting of 70\\% of the learning examples, a validation set consisting of 10\\% and a test set consisting of the remaining 20\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = X.shape\n",
    "n_training = int(.7*n)\n",
    "n_validation = int(.1*n)\n",
    "indices = np.random.permutation(n)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "X_training = X[:n_training, :]\n",
    "y_training = y[:n_training]\n",
    "X_validation = X[n_training:n_training+n_validation, :]\n",
    "y_validation = y[n_training:n_training+n_validation]\n",
    "X_test = X[n_training+n_validation:, :]\n",
    "y_test = y[n_training+n_validation:]\n",
    "\n",
    "print(\"Training:\", X_training.shape, y_training.shape)\n",
    "print(\"Validation:\", X_validation.shape, y_validation.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple regressor\n",
    "\n",
    "PyTorch is based on modules (objects of the class `torch.nn.Module`), which are composed into a compution graph. Each module may use parameters (objects of the class `torch.nn.Parameter`), and for each such an object it is possible to automatically compute gradients and optimize them according to some cost function.\n",
    "\n",
    "We begin by constructing a single linear layer, that is, a layer implementing the operation $\\hat{y} = Xw + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = nn.Linear(p, 1) # p input features, 1 output feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance we use mean-squared error. While PyTorch offers a ready to use implementation as the class `torch.nn.MSELoss()`, this time we implement it manually as an example. The main point is to inherit from the class `torch.nn.Module` and then to override the method `forward`, which is responsible for performing computations in the forward direction of the computation graph. Computations in the backward direction (that is, gradient flow) are derrived automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSE, self).__init__()\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        return ((prediction-target)**2).mean()\n",
    "\n",
    "mse = MSE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to optimize the vector `w` and `b` automatically during the learning. To this end, we use the class `torch.optim.Adam`, which implements some extensions of the graident descent algorithm. We construct the object `opt`, which is responsible for optimizing the regressor's parameters, obtained by calling `regressor.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(regressor.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting computations\n",
    "\n",
    "We implement mini-batch learning over `n_epoch` epochs. Within each epoch:\n",
    "1. The indices of the training set are shuffled.\n",
    "2. We iterate over these indices, retrieving `batch_size` of them every time. We use these indices to select examples used in the current learning step.\n",
    "3. We zero the gradients stored by the optimizer (`opt.zero_grad()`)\n",
    "4. We execute the regressor and compute the mean-squared error (MSE).\n",
    "5. We compute the gradients (`mse_value.backward()`) and apply them (`opt.step()`) to update the parameters of the regressor\n",
    "6. We store the value of MSE. We call the `detach()` method to detach the MSE from its gradients. This enables passing the values to other libraries like matplotlib.\n",
    "\n",
    "After an epoch is completed, we average the collected MSEs and store the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "batch_size = 100\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    indices = np.random.permutation(n_training)\n",
    "    mse_epoch = []\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        end = min(start + batch_size, len(indices))\n",
    "        indices_batch = indices[start:end]\n",
    "        X_batch = X_training[indices_batch, :]\n",
    "        y_batch = y_training[indices_batch]\n",
    "        opt.zero_grad()\n",
    "        y_pred = regressor(X_batch).reshape((-1,))\n",
    "        mse_value = mse(y_pred, y_batch)\n",
    "        mse_value.backward()\n",
    "        opt.step()\n",
    "        mse_epoch.append(mse_value.detach())\n",
    "    mse_values.append(np.mean(mse_epoch))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the two preceeding cells of the code again. Because the regressor and the optimizer were not created anew, so the parameters are already optimized and the obtained charts are vastly different. In order to return to the initial state it is necessary to create new obiect `regressor` and `opt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement early stopping: after each epoch of learning compute the value of MSE using the sets `X_validation` and `y_validation`. Stop the learning process if there is no improvement over the last 10 epochs with regard to the value of MSE on the validation set. Store values of `mse` for training and for validation (respecitvely in `training_mses` and `validation_mses`), the second cell of code will plot them. Print out the number of the epoch when the learning process was stopped.\n",
    "\n",
    "Hints and remarks:\n",
    "* Observe that we must call the methods `regressor.train()` and `regressor.eval()` to switch the module between the training mode and the evaluation mode. \n",
    "* Remeber not to call `opt.step()` after performing the computation on the validation set, to not update the weights of the model. Similarly, you don't need to compute gradients during this step.\n",
    "* You don't need to shuffle the validation set, nor go over it using batches.\n",
    "* You may want to print out the epoch number and the value of the MSE on the validation set every time you find a better value. In the beginning you should expect a better value more or less every epoch and the decrease in MSE will be step, with time the decreases will diminishi and you'll hit a better value only every few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = nn.Linear(p, 1) # p input features, 1 output feature\n",
    "opt = optim.Adam(regressor.parameters())\n",
    "\n",
    "training_mses = []\n",
    "validation_mses = []\n",
    "batch_size = 100\n",
    "n_epoch = 10000\n",
    "for epoch in range(n_epoch):\n",
    "    regressor.train()\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        ...\n",
    "    regressor.eval()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_mses, 'b') # plot training errors in blue\n",
    "plt.plot(validation_mses, 'r') # plot validation errors in red\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part is concerned with the classification task. We begin by downloading the MNIST dataset, consisting of 70.000 examples of handwritten digits (0-9), represented as grayscale images of 28x28 pixels. We use the function `fetch_openml` that downloads the data from [https://www.openml.org/](https://www.openml.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(mnist.data, dtype=torch.float)\n",
    "y = torch.tensor([int(v) for v in mnist.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect some examples. The digit in the top-right corner of each image is not a part of the image itself, but rather the label of the image, displayed using `ax.text` in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4,4))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(X[i].reshape(28, 28),cmap=plt.cm.binary,interpolation='nearest')\n",
    "    ax.text(0, 7, str(y[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits the data into three subsets: training, validation and test. This time, instead of representing the sets as raw tensors, we wrap each of them into an object of the class `torch.utils.data.TensorDataset`, which then enables shuffling, grouping into mini-batches and iterating over using `torch.utils.data.DatasetLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = mnist.data.shape\n",
    "k = 10 # number of classes\n",
    "n_train = int(.7*n)\n",
    "n_validation = int(.1*n)\n",
    "indices = np.random.permutation(n)\n",
    "train_indices = indices[:n_train]\n",
    "validation_indices = indices[n_train:n_train+n_validation]\n",
    "test_indices = indices[n_train+n_validation:]\n",
    "\n",
    "ds_train = data.TensorDataset(X[train_indices,:], y[train_indices])\n",
    "ds_validation = data.TensorDataset(X[validation_indices,:], y[validation_indices])\n",
    "ds_test = data.TensorDataset(X[test_indices,:], y[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple logisitc regression model, learning on the raw pixels of the images. The matrix of the feature weights (here: pixels) is of the size $p \\times k$, i.e., $p$ weights for each of the $k$ classes. The model computes logits, i.e., the output of the logistic  regression **before** applying the softmax function. In the output, each row corresponds to a single example and each column to a single class. Should each row be normalized using the softmax function, each row would be a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(p, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the loss function we will use *cross entropy* and perform the optimization using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an auxiliary function to compute the classification accuracy. For each row of the `logits` matrix we select the number of the column with the highest value and compare it with the expected (true) value. This way we obtain a vector of 0s and 1s, which are then averaged to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(logits, expected):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == expected).type(torch.float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder is very similar to the linear regression: during each of the `n_epoch` epochs, we enable the training mode (`model.train()`), create a data loder ( `torch.utils.data.DataLoader`) to handle shuffling and creating batches for us and then iterate over the loader, computing loss and gradients. Then we evaluate by computing the accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "acc_values = []\n",
    "batch_size = 100\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    model.train()\n",
    "    loader = data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)    \n",
    "    epoch_loss = []\n",
    "    for X_batch, y_batch in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = cost(logits, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()        \n",
    "        epoch_loss.append(loss.detach())\n",
    "    model.eval()\n",
    "    loss_values.append(torch.tensor(epoch_loss).mean())\n",
    "    logits = model(ds_validation.tensors[0])\n",
    "    acc = compute_acc(logits, ds_validation.tensors[1])\n",
    "    acc_values.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss on the training set\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()\n",
    "plt.title(\"Accuracy on the validation set\")\n",
    "plt.plot(acc_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Early stopping in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and modify the code presented above so that it uses early stopping instead of training over a constant number of epochs, taking into account accuracy on the validation set. Include creating a new model, a new optimizer and a new loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, possibly in more cells than one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: A neural network with a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression is a neural network without hidden layers. Use the code presented above and extend it with a hidden layer of 500 neurons. In order to connect a few objects of the type `torch.nn.Module` into a sequence use the class `torch.nn.Sequential`. Use the leaky ReLU function (`torch.nn.LeakyReLU`) to implement non-linearity. \n",
    "\n",
    "Implement early stopping. During the training collect the loss and accuracy on the  training set and the accuracy on the validation set and plot them. If the training takes too much time, modify `batch_size` and/or the number of neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    ..., # replace ... with creating a hidden layer\n",
    "    nn.LeakyReLU(),  # non-linearity\n",
    "    ...) # replace ... with creating an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, possibly in more cells than one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
