{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMAND LINE FUNCTIONS WORTH REMEMBERING"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ sed\n",
    "$ grep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Versioning Control (DVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The main aim of this exercise is to familiarize students with the awsome `dvc` tool for data and model versioning in machine learning/data mining projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way is to install the library inside a virtual Python environment or using Conda, although direct installation from a repository is possible. All details regarding the installation of the library can be found at [project's website](https://dvc.org/doc/install/linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:58:50.212573Z",
     "start_time": "2021-02-03T11:58:11.868216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a directory and to initialize `git` inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:09:58.804766Z",
     "start_time": "2021-02-03T12:09:58.776830Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir dvc-tutorial\n",
    "\n",
    "cd dvc-tutorial\n",
    "\n",
    "git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:09:59.634439Z",
     "start_time": "2021-02-03T12:09:59.618967Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd dvc-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:10:02.271419Z",
     "start_time": "2021-02-03T12:10:00.413675Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:10:03.117289Z",
     "start_time": "2021-02-03T12:10:03.094350Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:10:04.565289Z",
     "start_time": "2021-02-03T12:10:04.527525Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .dvc/plots/*\n",
    "git add .dvc/config\n",
    "git add .dvc/.gitignore\n",
    "git add .dvcignore\n",
    "\n",
    "git commit -m \"Initialize DVC for the project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data versioning\n",
    "\n",
    "The main goal of `dvc` is to allow for large data files versioning. Using `git` for this purpose is [quite problematic](https://docs.github.com/en/github/managing-large-files/working-with-large-files). In this laboratory we will use `dvc` to work with different versions of the same data file.\n",
    "\n",
    "Before starting the laboratory you should download and locally store `adult.data` and `adult.names` files from [UCI ML Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir data\n",
    "cp /path/to/files/adult* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:31:09.898261Z",
     "start_time": "2021-02-03T12:31:06.794621Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc add data/adult.data\n",
    "dvc add data/adult.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the files which were automatically created as the result of adding data files to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:32:33.974755Z",
     "start_time": "2021-02-03T12:32:33.946910Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat data/adult.data.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:32:43.169365Z",
     "start_time": "2021-02-03T12:32:43.157533Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat data/adult.names.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow for change tracking in data files we need to add `*.dvc` and   `data/.gitignore` files to the Git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:34:23.595700Z",
     "start_time": "2021-02-03T12:34:23.568663Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add data/.gitignore data/adult.data.dvc data/adult.names.dvc\n",
    "git commit -m \"Added ADULT dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to create a remote data repository. DVC works with many external data sources, including Amazon S3, Google Cloud Storage, remote servers accessible via `ssh`, HDFS systems, and many more. We will use a local directory to simulate an external repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:38:03.702839Z",
     "start_time": "2021-02-03T12:38:02.088781Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ~/dvcrepo\n",
    "dvc remote add -d repozytorium ~/dvcrepo\n",
    "git commit .dvc/config -m \"Added local directory simulating remote data repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:38:57.434966Z",
     "start_time": "2021-02-03T12:38:55.822517Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:39:13.588479Z",
     "start_time": "2021-02-03T12:39:13.559465Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -al ~/dvcrepo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:40:22.490328Z",
     "start_time": "2021-02-03T12:40:22.477786Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "ls -al ~/dvcrepo/1a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:41:01.909662Z",
     "start_time": "2021-02-03T12:41:01.886105Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ~/dvcrepo/1a/7cdb3ff7a1b709968b1c7a11def63e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remote repo can be used to download original versions of data files when fixing the unnecessary changes, re-creating an experimental branch, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:42:46.905102Z",
     "start_time": "2021-02-03T12:42:46.871899Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf .dvc/cache/\n",
    "rm data/adult.data\n",
    "rm data/adult.names\n",
    "\n",
    "ls -al data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:43:01.818927Z",
     "start_time": "2021-02-03T12:42:58.944763Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc pull\n",
    "\n",
    "ls -al data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will change the data files by removing all information about federal employees. Let's check how many such records do we have, and then let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:48:16.271645Z",
     "start_time": "2021-02-03T12:48:16.255096Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat data/adult.data | wc -l\n",
    "grep 'Federal-gov' data/adult.data | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:49:20.039570Z",
     "start_time": "2021-02-03T12:49:19.995093Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sed -i \"/Federal-gov/d\" data/adult.data\n",
    "cat data/adult.data | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:50:27.004595Z",
     "start_time": "2021-02-03T12:50:23.953798Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "dvc add data/adult.data\n",
    "git commit data/adult.data.dvc -m \"Removed federal workers from the dataset\"\n",
    "\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to rollback this change, we need to revert to the correct version of the `adult.data.dvc` file and running `dvc checkout` command to synchronize repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:53:34.802275Z",
     "start_time": "2021-02-03T12:53:34.772408Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "git log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:57:52.086754Z",
     "start_time": "2021-02-03T12:57:49.474674Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "git checkout 34685237371f63dc2fa2f997ce9f2aa514c0ffe9 data/adult.data.dvc\n",
    "dvc checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:57:55.122612Z",
     "start_time": "2021-02-03T12:57:55.088726Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "grep 'Federal-gov' data/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T12:58:00.813685Z",
     "start_time": "2021-02-03T12:58:00.777309Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "git commit data/adult.data.dvc -m \"Reverting the deletion of federal employees\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to remote data repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having configured a `git` repo using `dvc` we can easily use `dvc` to quickly download data and models, share the data, etc. The results of the previous chapter were stored in the [https://github.com/megaduks/dvc-tutorial](https://github.com/megaduks/dvc-tutorial) repo and now we will see how we can use remote repo to work with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:18:01.264647Z",
     "start_time": "2021-02-03T13:17:57.271870Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "dvc list https://github.com/megaduks/dvc-tutorial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets can be downloaded using a single command, e.g. to initialize a new project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:21:58.704953Z",
     "start_time": "2021-02-03T13:21:55.821001Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir new_project\n",
    "cd new_project\n",
    "dvc get https://github.com/megaduks/dvc-tutorial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:22:37.977979Z",
     "start_time": "2021-02-03T13:22:37.958932Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -al nowy_projekt/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using the above command we have lost the information on the origin of the data and we can't re-connect the locally downloaded data with the remote repository. The `dvc get` command resembles `wget` in this regard. If we want to keep the connection between remote and local data, we must use `dvc import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:27:09.200288Z",
     "start_time": "2021-02-03T13:27:02.454767Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p newer_project/data\n",
    "dvc import https://github.com/megaduks/dvc-tutorial/ data/adult.data \\\n",
    "    -o newer_project/data/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:27:23.595980Z",
     "start_time": "2021-02-03T13:27:23.570453Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat newer_project/data/adult.data.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, metadata of the `adult.data` file contain information on the remote repository from which the data originates. Precise hashes identifying a particular version of the data file are stored as well. In addition, we can easily track changes of the origin data in the remote repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:31:12.428565Z",
     "start_time": "2021-02-03T13:31:08.978660Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc update newer_project/data/adult.data.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC offers also a programmatical API to access data in remote repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:35:11.818028Z",
     "start_time": "2021-02-03T13:35:10.906461Z"
    }
   },
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "\n",
    "with dvc.api.open('data/adult.data', repo='https://github.com/megaduks/dvc-tutorial') as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data flows\n",
    "\n",
    "The most interesting functionality offered by `dvc` is the ability to manage reproducible data workflows. We will use the following flow to illustrate this concept:\n",
    "\n",
    "- we will pre-process data by removing selected records\n",
    "- we will add a new feature\n",
    "- we will train a simple model\n",
    "- we will evaluate the quality of the model\n",
    "\n",
    "The code in the following examples is very simplified, but it's purpose is to illustrate the concept of reproducible data flows. First, we need to install some additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T13:52:03.935895Z",
     "start_time": "2021-02-03T13:52:02.122229Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install pandas sklearn pyaml scikit-learn scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the first step of the data flow. In this step we read in a text file and transform it to a serialized binary version (a pickle). \n",
    "\n",
    "Create a `params.yaml` file and put the following inside:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "prepare:\n",
    "  split: 0.75\n",
    "  seed: 42\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a `prepare.py` file with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import yaml\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "params = yaml.safe_load(open('params.yaml'))['prepare']\n",
    "\n",
    "split = params['split']\n",
    "random.seed(params['seed'])\n",
    "\n",
    "input_file = Path(sys.argv[1])\n",
    "train_output = Path('data') / 'prepared' / 'train.csv'\n",
    "test_output = Path('data') / 'prepared' / 'test.csv'\n",
    "\n",
    "Path('data/prepared').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(input_file, sep=',')\n",
    "train_df, test_df = train_test_split(df, train_size=split)\n",
    "\n",
    "train_df.to_csv(train_output, header=None)\n",
    "test_df.to_csv(test_output, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the first data flow in which we:\n",
    "- create a named step (`-n prepare`)\n",
    "- pass parameters (`-p prepare.seed,prepare.split`)\n",
    "- pass dependencies (`-d prepare.py -d data/adult.data`)\n",
    "- indicate the output (`-o data/prepared/`)\n",
    "- run the script and pass parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T14:28:20.384888Z",
     "start_time": "2021-02-03T14:28:16.949192Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n prepare \\\n",
    "    -p prepare.seed,prepare.split \\\n",
    "    -d prepare.py -d data/adult.data \\\n",
    "    -o data/prepared \\\n",
    "    python prepare.py data/adult.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result, we observe output files and a special `dvc.yaml` file with human-readable description of the data flow configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T14:29:14.948770Z",
     "start_time": "2021-02-03T14:29:14.913851Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T14:29:26.323449Z",
     "start_time": "2021-02-03T14:29:26.289676Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "ls -al data/prepared/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to add to the data flow data transformation. We will re-code all categorical attributes using [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) and we will compute feature interactions using [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures). This last class uses the `degree` parameter. Update the parameter file to account for the second step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "prepare:\n",
    "  split: 0.75\n",
    "  seed: 42\n",
    "featurize:\n",
    "  degree: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `featurize.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "\n",
    "params = yaml.safe_load(open('params.yaml'))['featurize']\n",
    "degree = params['degree']\n",
    "\n",
    "input_dir = sys.argv[1]\n",
    "output_dir = sys.argv[2]\n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "train_file = Path(input_dir) / 'train.csv'\n",
    "test_file = Path(input_dir) / 'test.csv'\n",
    "\n",
    "col_names = [\n",
    "        'age',\n",
    "        'workclass',\n",
    "        'weight',\n",
    "        'education',\n",
    "        'edu-num',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'sex',\n",
    "        'capital-gain',\n",
    "        'capital-loss',\n",
    "        'hours-per-week',\n",
    "        'native-country',\n",
    "        'class'\n",
    "]\n",
    "\n",
    "train_df = pd.read_csv(train_file, sep=',', names=col_names)\n",
    "test_df = pd.read_csv(test_file, sep=',', names=col_names)\n",
    "\n",
    "train_df = train_df.apply(LabelEncoder().fit_transform)\n",
    "test_df = test_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree, interaction_only=True)\n",
    "\n",
    "train_y = train_df['class']\n",
    "test_y = test_df['class']\n",
    "\n",
    "train_df = train_df.drop('class', axis=1)\n",
    "test_df = test_df.drop('class', axis=1)\n",
    "\n",
    "train_df = np.column_stack((poly.fit_transform(train_df), train_y))\n",
    "test_df = np.column_stack((poly.fit_transform(test_df), test_y))\n",
    "\n",
    "train_output = Path(output_dir) / 'train.p'\n",
    "test_output = Path(output_dir) / 'test.p'\n",
    "\n",
    "with open(train_output, 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n",
    "\n",
    "with open(test_output, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data flow can be executed by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T21:53:51.420099Z",
     "start_time": "2021-02-03T21:53:49.713567Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n featurize \\\n",
    "    -p featurize.degree \\\n",
    "    -d featurize.py -d data/prepared/ \\\n",
    "    -o data/features \\\n",
    "    python featurize.py data/prepared/ data/features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to loose the results of our work we should record data flow steps in the `git` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .gitignore dvc.lock dvc.yaml\n",
    "git commit -m 'Added preparation and featurization steps to data pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to run model training. We will use a simple script with Random Forest, and we will use two parameters: the number of trees in the forest and the maximum depth of each tree. Change the parameter file in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "prepare:\n",
    "  split: 0.75\n",
    "  seed: 42\n",
    "featurize:\n",
    "  degree: 2\n",
    "train:\n",
    "  max_depth: 2\n",
    "  n_estimators: 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `train.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = yaml.safe_load(open('params.yaml'))['train']\n",
    "max_depth = params['max_depth']\n",
    "n_estimators = params['n_estimators']\n",
    "\n",
    "input_dir = sys.argv[1]\n",
    "output_dir = sys.argv[2]\n",
    "\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "train_file = Path(input_dir) / 'train.p'\n",
    "model_file = Path(output_dir) / 'model.p'\n",
    "\n",
    "with open(train_file, 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "\n",
    "X = train_df[:, :-1]\n",
    "y = train_df[:, -1]\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth\n",
    ")\n",
    "clf.fit(X, y)\n",
    "\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(clf, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the script expects two parameters to be passed via the command line (the input directory with the data and the output directory to store the results of the script). To add the training step to the data flow execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n train \\\n",
    "    -p train.max_depth,train.n_estimators \\\n",
    "    -d train.py -d data/features/ \\\n",
    "    -o data/models/ \\\n",
    "    python train.py data/features/ data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we record the changes in the data flow in `git`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add .gitignore dvc.lock dvc.yaml\n",
    "git commit -m 'Added training step to data pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why have we created the `dvc.yaml` file? At the first glance it might seem overly complicated. But this is where `dvc` truly shines, the presence of the full definition of the data flow allows for full reproducibilty using a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc repro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change a single parameter in the `train` section (e.g., change the number of trees in the RandomForest) and re-run the experiment. Which steps have been executed? Change another parameter in the `prepare` section (e.g. the way train/test split is performed) and re-run the experiment once again. Has something changed?\n",
    "\n",
    "If you want to visualize the data flow, use the `dvc dag` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The last element of the `dvc` framework that we will examine is the way experiments are executed. Before we start experimenting, we need to create a `evaluate.py` file with the code to evaluate the results of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import sklearn.metrics as metrics\n",
    "from pathlib import Path\n",
    "\n",
    "model_file = Path(sys.argv[1]) / 'model.p'\n",
    "test_file = Path(sys.argv[2]) / 'test.p'\n",
    "\n",
    "scores_file = sys.argv[3]\n",
    "plots_file = sys.argv[4]\n",
    "\n",
    "with open(model_file, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(test_file, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "X = test_df[:,:-1]\n",
    "y = test_df[:,-1]\n",
    "\n",
    "predictions_by_class = model.predict_proba(X)\n",
    "y_pred = predictions_by_class[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y, y_pred)\n",
    "auc = metrics.auc(recall, precision)\n",
    "\n",
    "with open(scores_file, 'w') as f:\n",
    "    json.dump({'auc': auc}, f)\n",
    "\n",
    "with open(plots_file, 'w') as f:\n",
    "    json.dump({'prc': [{\n",
    "            'precision': p,\n",
    "            'recall': r,\n",
    "            'threshold': t\n",
    "        } for p, r, t in zip(precision, recall, thresholds)\n",
    "    ]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym razem dodanie kroku ewaluacji do potoku będzie bardziej skomplikowane, ponieważ musimy też uwzględnić specjalny plik do przechowywania wartości metryk oraz plik przechowywania danych na potrzeby wykresów. \n",
    "\n",
    "This time adding a step to the data flow is more complicated, because we have to include a special file to store the metrics associated with experiment runs, and an additional file to store the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc run -n evaluate \\\n",
    "    -d evaluate.py -d data/models/ -d data/features/ \\\n",
    "    -M scores.json \\\n",
    "    --plots-no-cache prc.json \\\n",
    "    python evaluate.py data/models/ data/features/ scores.json prc.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see at the final data flow configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T23:56:25.467419Z",
     "start_time": "2021-02-03T23:56:25.420110Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat dvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to record all the changes in `git`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T23:58:05.640014Z",
     "start_time": "2021-02-03T23:58:05.587838Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add dvc.lock dvc.yaml\n",
    "git commit -m 'Added evaluation step to data pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result of the data flow a new file `scores.json` has been added. This file contains the AUROC measure for the experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T23:59:06.139126Z",
     "start_time": "2021-02-03T23:59:06.105759Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat scores.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prc.json` file contains the information about the training (*precision-recall curve*). Let's add both files to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T00:01:16.579173Z",
     "start_time": "2021-02-04T00:01:16.550290Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git add scores.json prc.json\n",
    "git commit -m 'Added evaluation metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment with changed parameters and let's see if these changes affect the metric. Change the `degree` parameter to 3 and change the `n_estimators` parameter to 25. Re-run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T00:05:38.929902Z",
     "start_time": "2021-02-04T00:05:37.128051Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc params diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T00:06:12.264328Z",
     "start_time": "2021-02-04T00:06:10.306014Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "dvc metrics diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T00:06:55.683795Z",
     "start_time": "2021-02-04T00:06:53.979999Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc plots diff -x recall -y precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
