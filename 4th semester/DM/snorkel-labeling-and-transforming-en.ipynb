{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to introduce students to the [Snorkel](http://www.snorkel.org) tool and the possibilities of programmatic label generation using the weak-supervised learning paradigm.\n",
    "\n",
    "In order to use weakly supervised learning to generate labels, it is necessary to create three datasets:\n",
    "\n",
    "- **train set**: which does not have any labels\n",
    "- **validation set**: used for hyperparameter optimization, has labels\n",
    "- **test set**: used only for final model evaluation, has labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "The first step will be to load the dataset and split it into a train set and a test set. Since in our set all SMS have a label, we will simulate a weakly supervised learning problem by randomly removing 80% of the labels. Additionally, Snorkel requires numeric labels, so we need to recode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you'll have scikit-learn 0.24.2 which is incompatible.\n",
      "imbalanced-learn 0.9.1 requires scikit-learn>=1.1.0, but you'll have scikit-learn 0.24.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading snorkel-0.9.8-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from snorkel) (1.5.2)\n",
      "Requirement already satisfied: networkx<2.7,>=2.2 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from snorkel) (2.5)\n",
      "Collecting scikit-learn<0.25.0,>=0.20.2\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-win_amd64.whl (6.9 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.33.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from snorkel) (4.50.2)\n",
      "Collecting tensorboard<2.7.0,>=2.0.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from snorkel) (1.3.5)\n",
      "Collecting torch<2.0.0,>=1.2.0\n",
      "  Downloading torch-1.11.0-cp38-cp38-win_amd64.whl (158.0 MB)\n",
      "Collecting munkres>=1.0.6\n",
      "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: numpy<1.20.0,>=1.16.5 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from snorkel) (1.19.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from networkx<2.7,>=2.2->snorkel) (4.4.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from scikit-learn<0.25.0,>=0.20.2->snorkel) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from scikit-learn<0.25.0,>=0.20.2->snorkel) (2.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (1.0.1)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (50.3.1.post20201107)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-4.21.2-cp38-cp38-win_amd64.whl (524 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (0.35.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->snorkel) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from torch<2.0.0,>=1.2.0->snorkel) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (3.0.4)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from grpcio>=1.24.3->tensorboard<2.7.0,>=2.0.0->snorkel) (1.15.0)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\1625203\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.7.0,>=2.0.0->snorkel) (3.4.0)\n",
      "Installing collected packages: scikit-learn, absl-py, tensorboard-plugin-wit, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, protobuf, tensorboard-data-server, importlib-metadata, markdown, tensorboard, torch, munkres, snorkel\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.1.1\n",
      "    Uninstalling scikit-learn-1.1.1:\n",
      "      Successfully uninstalled scikit-learn-1.1.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 2.0.0\n",
      "    Uninstalling importlib-metadata-2.0.0:\n",
      "      Successfully uninstalled importlib-metadata-2.0.0\n",
      "Successfully installed absl-py-1.1.0 cachetools-4.2.4 google-auth-1.35.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 importlib-metadata-4.11.4 markdown-3.3.7 munkres-1.1.4 oauthlib-3.2.0 protobuf-4.21.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 scikit-learn-0.24.2 snorkel-0.9.8 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:04.995959Z",
     "start_time": "2021-05-19T19:35:04.879076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!head data/smsspamcollection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:05.564736Z",
     "start_time": "2021-05-19T19:35:05.335569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          text  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "   label  \n",
       "0     -1  \n",
       "1      0  \n",
       "2     -1  \n",
       "3     -1  \n",
       "4     -1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth', 600)\n",
    "\n",
    "SPAM = 1\n",
    "HAM = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "df = pd.read_csv('smsspamcollection.csv', sep='\\t', header=None, names=['old_label', 'text'])\n",
    "\n",
    "df['label'] = df.old_label.apply(lambda x: SPAM if x == 'spam' else HAM)\n",
    "\n",
    "df.loc[df.sample(frac=0.8).index, 'label'] = ABSTAIN\n",
    "df.drop(columns=['old_label'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:05.899853Z",
     "start_time": "2021-05-19T19:35:05.891864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>Why don't you wait 'til at least wednesday to see if you get your .</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>Huh y lei...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  text  \\\n",
       "0                                                      Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "2          Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                                    U dun say so early hor... U c already then say...   \n",
       "4                                                                                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "6                                                                                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "...                                                                                                                                                                ...   \n",
       "5563                                                                                                                                               Ard 6 like dat lor.   \n",
       "5564                                                                                               Why don't you wait 'til at least wednesday to see if you get your .   \n",
       "5565                                                                                                                                                      Huh y lei...   \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.   \n",
       "5570                                     The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free   \n",
       "\n",
       "      label  \n",
       "0        -1  \n",
       "2        -1  \n",
       "3        -1  \n",
       "4        -1  \n",
       "6        -1  \n",
       "...     ...  \n",
       "5563     -1  \n",
       "5564     -1  \n",
       "5565     -1  \n",
       "5567     -1  \n",
       "5570     -1  \n",
       "\n",
       "[4458 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstain_idx = df.label == ABSTAIN\n",
    "\n",
    "df_train = df[abstain_idx]\n",
    "df_test = df[~abstain_idx]\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple keyword search\n",
    "\n",
    "As a first example, we will use a search for the words \"check\" and \"free\" in SMS content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:07.160530Z",
     "start_time": "2021-05-19T19:35:06.743723Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def check(sms):\n",
    "    return SPAM if \"check\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def free(sms):\n",
    "    return SPAM if \"free\" in sms.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply the labeling functions to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:07.664922Z",
     "start_time": "2021-05-19T19:35:07.537784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                         | 0/4458 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_is_builtin_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-036a8954bfa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mapplier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPandasLFApplier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mL_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapplier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\snorkel\\labeling\\apply\\pandas.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, df, progress_bar, fault_tolerant, return_meta)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mlabels_with_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrows_to_triplets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_from_row_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_with_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m                     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_is_builtin_func'"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [check, free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of applying the set of labeling functions to the train set is a matrix of size $m \\times n$, where $m$ is the number of examples and $n$ is the number of labeling functions. The matrix contains the result of applying each function to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:08.394943Z",
     "start_time": "2021-05-19T19:35:08.383414Z"
    }
   },
   "outputs": [],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:08.802557Z",
     "start_time": "2021-05-19T19:35:08.794050Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to analyze this is to determine the coverage of labeling functions (i.e., the percentage of cases for which the function returned a result other than `ABSTAIN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:09.658152Z",
     "start_time": "2021-05-19T19:35:09.652780Z"
    }
   },
   "outputs": [],
   "source": [
    "coverage_check, coverage_free = (L_train != ABSTAIN).mean(axis=0)\n",
    "\n",
    "print(f\"Coverage for check(): {coverage_check * 100:.1f}%\")\n",
    "print(f\"Coverage for free(): {coverage_free * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, Snorkel offers additional tools that allow for deeper analysis of the result of labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:10.422401Z",
     "start_time": "2021-05-19T19:35:10.392383Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of each column is as follows:\n",
    "- `Polarity`: the set of labels returned by the function\n",
    "- `Coverage`: the percentage of examples for which the function returns a value other than `ABSTAIN`\n",
    "- Overlaps: the percentage of examples for which at least one other labeling function returned a value\n",
    "- Conflicts: the percentage of examples for which at least one other labeling function returned a different value\n",
    "\n",
    "If the train set contained labels, the method would also return:\n",
    "- `Correct`: the number of correct labels\n",
    "- `Incorrect`: number of incorrect labels\n",
    "- `Empirical Accuracy`: the percentage of correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the examples labeled by the `free()` function as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:11.544719Z",
     "start_time": "2021-05-19T19:35:11.527030Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:,1] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the phrase \"call now\" is also a good indicator for spam. So let's add one more labeling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:12.390882Z",
     "start_time": "2021-05-19T19:35:12.232917Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def call_now(sms):\n",
    "    return SPAM if \"call now\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:12.520535Z",
     "start_time": "2021-05-19T19:35:12.493371Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which examples were labeled as spam by the `call_now()` function but omitted by `free()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:13.266968Z",
     "start_time": "2021-05-19T19:35:13.238111Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 2])\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:13.561565Z",
     "start_time": "2021-05-19T19:35:13.544427Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.iloc[buckets[(ABSTAIN, SPAM)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:14.070016Z",
     "start_time": "2021-05-19T19:35:14.050884Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as spam all messages containing the word \"HOT\" written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:15.054371Z",
     "start_time": "2021-05-19T19:35:15.048930Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def hot(sms):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching based on a regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of labeling function is one that uses regexp to find specific expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:16.266072Z",
     "start_time": "2021-05-19T19:35:15.979235Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def regex_I_am_free(sms):\n",
    "    if re.search(r\"I\\s.*free\", sms.text, flags=re.I):\n",
    "        return HAM\n",
    "    elif re.search(r\"free\", sms.text, flags=re.I):\n",
    "        return SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare examples that the `free()` function labels as spam and the `regex_I_am_free()` function considers valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:17.020431Z",
     "start_time": "2021-05-19T19:35:16.996219Z"
    }
   },
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 3])\n",
    "df_train.iloc[buckets[(SPAM, HAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that will mark as spam all messages containing any amounts specified with a currency symbol ($99, £1.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:18.671866Z",
     "start_time": "2021-05-19T19:35:18.669094Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def contains_money(sms):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching based on heuristics\n",
    "\n",
    "A simple heuristic to find spam is to assume that if more than 10% of the message text is written in capitals, there is a good chance it is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:20.563651Z",
     "start_time": "2021-05-19T19:35:20.207843Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def has_many_uppercase_words(sms):\n",
    "    percentage_uppercase = sum([word.isupper() for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return SPAM if percentage_uppercase > 0.1 else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as valid those messages that are shorter than 10 words and do not contain any word written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:21.341099Z",
     "start_time": "2021-05-19T19:35:21.333785Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_and_no_uppercase(sms):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an external statistical model\n",
    "\n",
    "When labeling data, you can use external models whose response can be important information for deciding how to label an example. Snorkel has several built-in integrations in the form of the `Preprocessor` interface, in the example below we will use the `SpaCy` library to perform additional grammatical analysis of the text. However, you will need to download the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:31:22.242830Z",
     "start_time": "2021-05-19T19:31:03.524220Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:31:34.018309Z",
     "start_time": "2021-05-19T19:31:33.068241Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:26.090920Z",
     "start_time": "2021-05-19T19:35:25.099419Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:26.870094Z",
     "start_time": "2021-05-19T19:35:26.810785Z"
    }
   },
   "outputs": [],
   "source": [
    "_text = \"\"\"I don't England is a country that is part of the United Kingdom. \n",
    "It shares land borders with Wales to its west and Scotland to its north. \n",
    "The Irish Sea lies northwest of England and the Celtic Sea to the southwest. \n",
    "England is separated from continental Europe by the North Sea to the east and the \n",
    "English Channel to the south. The country covers five-eighths of the island of \n",
    "Great Britain, which lies in the North Atlantic, and includes over 100 smaller islands, \n",
    "such as the Isles of Scilly and the Isle of Wight.\"\"\"\n",
    "\n",
    "doc = nlp(_text)\n",
    "\n",
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:28.056838Z",
     "start_time": "2021-05-19T19:35:27.497633Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that short text messages in which a reference to a specific person appears are not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:28.564396Z",
     "start_time": "2021-05-19T19:35:28.552608Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:29.352633Z",
     "start_time": "2021-05-19T19:35:29.348630Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[spacy])\n",
    "def has_person(sms):\n",
    "    if len(sms.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in sms.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:09.731132Z",
     "start_time": "2021-05-19T19:35:29.949300Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of pre-processing data for labeling would be determining the average word frequency of a document. Below we define a function that determines the average word frequency and we decorate it as an example of a pre-processor. When a text message is sent to the next labeling function, the pre-processor will populate the text message with the average word frequency and, based on that, the labeling function will make a decision (we assume that if the text message contains many rare words then it is spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:32.970893Z",
     "start_time": "2021-05-19T19:37:32.784823Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def avg_word_freq(sms):\n",
    "    sms.avg_word_freq = sum([zipf_frequency(word, 'en') for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:34.795326Z",
     "start_time": "2021-05-19T19:37:34.790325Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[avg_word_freq])\n",
    "def many_rare_words(sms):\n",
    "    return ABSTAIN if sms.avg_word_freq >= 4 else SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:39.197040Z",
     "start_time": "2021-05-19T19:37:35.984206Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:39.849049Z",
     "start_time": "2021-05-19T19:37:39.827079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:,6] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks messages containing more than 3 adjectives as spam. Use the SpaCy library for pre-processing. \n",
    "\n",
    "__Hint__: the following example shows how to read the part-of-speech label for each token from the message being analyzed. For information on all token properties recognized by SpaCy, see [API documentation](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:41.894274Z",
     "start_time": "2021-05-19T19:37:41.111265Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sms = \"Yetunde, i'm sorry but moji and i seem too busy to be able to go shopping.\"\n",
    "\n",
    "for token in nlp(sms):\n",
    "    print(f\"{token.text:<10} {token.pos_:<10} {token.tag_:<10} {token.lemma_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining labeling functions into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of labeling functions is not to achieve individually large coverage. Labeling functions are inherently noisy and can make many individual errors. The true utility of labeling functions becomes apparent when multiple functions are combined to form a single model.\n",
    "\n",
    "We will first build a simple model based on majority voting, and then build a more complex model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:10.800140Z",
     "start_time": "2021-05-19T19:37:42.829275Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:11.685278Z",
     "start_time": "2021-05-19T19:38:11.659189Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_test, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:13.165490Z",
     "start_time": "2021-05-19T19:38:12.488067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:14.046160Z",
     "start_time": "2021-05-19T19:38:14.041527Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:14.971642Z",
     "start_time": "2021-05-19T19:38:14.958011Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels, counts = np.unique(preds_train, return_counts=True)\n",
    "\n",
    "for l, c in zip(labels, counts):\n",
    "    print(f\"LABEL: {l}, count: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:16.095821Z",
     "start_time": "2021-05-19T19:38:15.735200Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:16.707091Z",
     "start_time": "2021-05-19T19:38:16.660020Z"
    }
   },
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority voting accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Probabilistic model accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, some data points will not receive any label. It is necessary to filter out these points before sending the labeling result for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:18.328146Z",
     "start_time": "2021-05-19T19:38:18.257149Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import preds_to_probs, probs_to_preds\n",
    "\n",
    "preds_train, probs_train = label_model.predict(L=L_train, return_probs=True)\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "df_train.shape, df_train_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we were able to quickly prepare labels for about 650 examples (recall that initially no example in the `df_train` set had labels).\n",
    "\n",
    "The next step will use prepared labels as training data for the actual classifier. We will use simple [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), first pre-processing the input data. Since we are working with text, we will use the [word vector representation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) created based on 5-grams by `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:20.290628Z",
     "start_time": "2021-05-19T19:38:20.140958Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:21.403376Z",
     "start_time": "2021-05-19T19:38:21.027861Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver='lbfgs')\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:22.426987Z",
     "start_time": "2021-05-19T19:38:22.421221Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Logistic regression accuracy: {sklearn_model.score(X=X_test, y=df_test.label) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the final model improved the score over the majority vote and the `LabelModel` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Complete the above calls with functions that you wrote yourself and check whether your functions improve the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming functions\n",
    "\n",
    "The idea of a transforming function is to perform an atomic transformation of an instance. For data that is an image, typical transformations include cropping, rotating, and changing the color palette. For text data, you can replace words with synonyms, substitute named entities, cut random pieces of text, etc. In the following example we will find types of named entities occurring in the text, and then prepare a simple transformer that will randomly replace occurrences of the `PERSON` entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:31.560232Z",
     "start_time": "2021-05-19T19:38:26.866544Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for doc in nlp.pipe(df_train.text.sample(frac=0.05)):\n",
    "    print(f\"Entities: {[(e.text, e.label_) for e in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:40:36.945027Z",
     "start_time": "2021-05-19T19:39:12.256235Z"
    }
   },
   "outputs": [],
   "source": [
    "person_entities = []\n",
    "\n",
    "for doc in nlp.pipe(df_train.text):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'PERSON':\n",
    "            person_entities.append(e.text)\n",
    "        \n",
    "person_entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:03.662025Z",
     "start_time": "2021-05-19T19:42:02.919683Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def random_person_ner(sms):\n",
    "    person_ners = [e.text for e in sms.doc.ents]\n",
    "    \n",
    "    if person_ners:\n",
    "        person_to_replace = np.random.choice(person_ners)\n",
    "        person_to_add = np.random.choice(person_entities)\n",
    "        sms.text = sms.text.replace(person_to_replace, person_to_add)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of transformation could be using WordNet to find synonyms for words. However, this requires downloading a corpus of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:20.633409Z",
     "start_time": "2021-05-19T19:42:20.093443Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:25.058494Z",
     "start_time": "2021-05-19T19:42:25.054333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_synonym(word):\n",
    "    \n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        \n",
    "        return np.random.choice([w.replace(\"_\", \" \") for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:27.153320Z",
     "start_time": "2021-05-19T19:42:27.148396Z"
    }
   },
   "outputs": [],
   "source": [
    "@transformation_function()\n",
    "def replace_words_with_synonym(sms, num_replacements=5):\n",
    "\n",
    "    words = sms.text.split()\n",
    "    \n",
    "    for _ in range(num_replacements):\n",
    "        word_idx = np.random.choice(range(len(words)))\n",
    "        synonym = get_synonym(words[word_idx])\n",
    "        if synonym:\n",
    "            words[word_idx] = synonym\n",
    "        \n",
    "    sms.text = ' '.join(words)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare the original text message content with the transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:31.366022Z",
     "start_time": "2021-05-19T19:42:31.361290Z"
    }
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/utils.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def preview_tfs(df, tfs):\n",
    "    transformed_examples = []\n",
    "    for f in tfs:\n",
    "        for i, row in df.iterrows():\n",
    "            transformed_or_none = f(row)\n",
    "            # If TF returned a transformed example, record it in dict and move to next TF.\n",
    "            if transformed_or_none is not None:\n",
    "                transformed_examples.append(\n",
    "                    OrderedDict(\n",
    "                        {\n",
    "                            \"TF Name\": f.name,\n",
    "                            \"Original Text\": row.text,\n",
    "                            \"Transformed Text\": transformed_or_none.text,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    return pd.DataFrame(transformed_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:46:55.348701Z",
     "start_time": "2021-05-19T19:46:54.714091Z"
    }
   },
   "outputs": [],
   "source": [
    "tfs = [random_person_ner, replace_words_with_synonym]\n",
    "\n",
    "preview_tfs(df_train.sample(frac=0.1), tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying transforming functions requires some policy defining the order and number of transformations. In the example below, two transformation functions are drawn at random and this sequence of two functions is applied twice to each data point. As a result, we triple the size of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:48:27.795346Z",
     "start_time": "2021-05-19T19:48:18.367019Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy, PandasTFApplier\n",
    "\n",
    "random_policy = RandomPolicy(len(tfs), sequence_length=2, n_per_original=2, keep_original=True)\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, random_policy)\n",
    "\n",
    "df_train_sample = df_train.sample(frac=0.1)\n",
    "df_train_augmented = tf_applier.apply(df_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:48:30.188103Z",
     "start_time": "2021-05-19T19:48:30.181349Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_sample.shape, df_train_augmented.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Modify the transforming function ``replace_words_with_synonym()`` so that you can restrict the replacement of words with synonyms only for specific parts of speech (e.g., replace only nouns or verbs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
