{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World of Catan Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Fandom Wiki domain name\n",
    "\n",
    "domain_address = 'https://catan.fandom.com'\n",
    "\n",
    "# Page with links to all \"basic\" pages\n",
    "# NOTE - for large Fandom Wikis, there is a pagination so it is necessary to go through several subpages\n",
    "\n",
    "all_pages_list_subpage = '/wiki/Special:AllPages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap all \"basic\" pages links\n",
    "\n",
    "response = requests.get(domain_address + all_pages_list_subpage)\n",
    "parsed = bs4.BeautifulSoup(response.text)\n",
    "list_of_pages = list()\n",
    "\n",
    "# Collect all links\n",
    "\n",
    "div_with_list_of_links = parsed.find_all('div', {'class': 'mw-allpages-body'})[0]\n",
    "for a_element in div_with_list_of_links.find_all('a', href=True):\n",
    "    list_of_pages.append(a_element['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrap the outgoing links from each web page to create a dictionary representing the graph\n",
    "#\n",
    "# Dictionary structure:\n",
    "# - keys - website addresses\n",
    "# - values - list of website addresses to which the given page links\n",
    "# \n",
    "# {k_1: [v_1_1, v_1_2, ...], k_2: [v_2_1, v_2_2, ...], ...}\n",
    "#\n",
    "# Graph interpretation:\n",
    "# For each pair k_i: v_i_j, there is an edge in the graph k_i -> v_i_j\n",
    "#\n",
    "# Scraping constraints:\n",
    "# - view only the main content of the site - <div class=\"mw-parser-output\">...</div>\n",
    "# - focus only on the link elements - <a href=\"...\"></a>\n",
    "# - collect only those links, which are contained in list_of_pages\n",
    "\n",
    "graph = dict()\n",
    "for idx, link in enumerate(list_of_pages):\n",
    "    graph[link] = list()\n",
    "    response = requests.get(domain_address + link)\n",
    "    parsed = bs4.BeautifulSoup(response.text)\n",
    "    content = parsed.find('div', {'class': 'mw-parser-output'})\n",
    "    print(link, response.status_code)\n",
    "    for a_element in content.find_all('a', href=True):\n",
    "        href = a_element['href'].split('#')[0].split('?')[0]\n",
    "        if href in list_of_pages:\n",
    "            if href not in graph[link]:\n",
    "                graph[link].append(href)\n",
    "\n",
    "# Save the graph structure\n",
    "                \n",
    "with open('catan_links.pickle', 'wb') as file:\n",
    "    pickle.dump(graph, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tram network - ZTM Pozna≈Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import pickle\n",
    "\n",
    "# Tram numbers list\n",
    "\n",
    "address_prefix = 'https://www.ztm.poznan.pl/pl/rozklad-jazdy/'\n",
    "tram_list = [1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 'T12', 13, 14, 15, 18, 24, 98]\n",
    "\n",
    "# Structure to collect the data\n",
    "\n",
    "stops_connections = dict()\n",
    "\n",
    "# Collect all schedules\n",
    "\n",
    "for t in tram_list:\n",
    "    address = address_prefix + str(t)\n",
    "    response = requests.get(address)\n",
    "    print(t, address, response.status_code)\n",
    "    parsed = bs4.BeautifulSoup(response.text)\n",
    "    for stop_list in parsed.find_all('ul', {'class': 'line-direction__stops'}):\n",
    "        prev_stop = None\n",
    "        for stop_li in stop_list.find_all('li', {'class': 'show'}):\n",
    "            for stop_el in stop_li.find_all('span', {'class': 'line-stop__name'}):\n",
    "                stop_name = stop_el.get_text()\n",
    "                if stop_name != prev_stop and prev_stop is not None:\n",
    "                    if prev_stop not in stops_connections.keys():\n",
    "                        stops_connections[prev_stop] = list()\n",
    "                    if stop_name not in stops_connections[prev_stop]:\n",
    "                        stops_connections[prev_stop].append(stop_name)\n",
    "                prev_stop = stop_name\n",
    "\n",
    "# Save the graph structure\n",
    "\n",
    "with open('tram_stops.pickle', 'wb') as file:\n",
    "    pickle.dump(stops_connections, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_connections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
